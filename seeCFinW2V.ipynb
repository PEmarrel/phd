{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6176006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import math, random\n",
    "\n",
    "import os\n",
    "\n",
    "from typing import Sequence, Optional, Callable, List, Dict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from modelSGNS import SkipGramModel\n",
    "from dataSet import SGNS_store_DataSet\n",
    "from visuEmbedding import interactive_embedding_plot_3D, components_to_fig_3D, components_to_fig_3D_animation\n",
    "\n",
    "# torch.manual_seed(1)systemd\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(embeddings:nn.Embedding) -> torch.Tensor:\n",
    "    emb = embeddings.weight.detach()\n",
    "    emb_norm = F.normalize(emb, p=2, dim=1)\n",
    "    similarity_matrix = emb_norm @ emb_norm.t()\n",
    "    return similarity_matrix\n",
    "\n",
    "def update_sim_history(words: list[str], idx: List[int], cos_sim_history:Dict, similarity_matrix):\n",
    "    num_words = len(words)\n",
    "\n",
    "    for i in range(num_words):\n",
    "        for j in range(num_words):\n",
    "            similarity = ((similarity_matrix[idx[i], idx[j]] + 1) / 2) * 100\n",
    "            cos_sim_history[words[i]][words[j]].append(round(float(similarity), 2))\n",
    "\n",
    "def heat_map(words:List[str], similarity_matrix, figsize=(10, 8), save_file='tmp.png'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt=\".2f\", cmap=\"magma\",\n",
    "                xticklabels=words, yticklabels=words, cbar=True, robust=False,\n",
    "                vmin=0, vmax=100,\n",
    "                square=False, linewidths=0.)\n",
    "\n",
    "    plt.title(\"Matrix de Similarité Cosinus\")\n",
    "    plt.xlabel(\"Mots\", fontstyle=\"italic\")\n",
    "    plt.ylabel(\"Mots\", fontstyle=\"italic\")\n",
    "    plt.savefig(save_file)\n",
    "    plt.close('all')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b719b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots central, similaire et différent\n",
    "data1 = ['peluche', 'train', ['chat', 'chien', 'souris']]\n",
    "data5 = ['train', 'peluche', ['chat', 'chien', 'souris']]\n",
    "\n",
    "data2 = ['chat', 'chien', ['peluche', 'train']]\n",
    "data3 = ['chat', 'souris', ['peluche', 'train']]\n",
    "data4 = ['chien', 'souris', ['peluche', 'train']]\n",
    "\n",
    "encoder = {\n",
    "    'peluche' : 0,\n",
    "    'train' : 1,\n",
    "    'chat' : 2,\n",
    "    'chien' : 3,\n",
    "    'souris': 4\n",
    "}\n",
    "\n",
    "nb_word = 5\n",
    "list_data = [data1, data2, data5, data3, data4] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ea463",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V:SkipGramModel = SkipGramModel(nb_word, embedding_dimension=3, sparse=False, init_range=1)\n",
    "# optimizer = torch.optim.SparseAdam(modelW2V.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(modelW2V.parameters(), lr=0.02)\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "epoch_loss = 0\n",
    "\n",
    "all_cos_sim = {}\n",
    "\n",
    "for w1 in encoder.keys():\n",
    "    all_cos_sim[w1] = {}\n",
    "    for w2 in encoder.keys():\n",
    "        all_cos_sim[w1][w2] = []\n",
    "\n",
    "\n",
    "modelW2V.train()\n",
    "poids_souris = []\n",
    "rm_me = []\n",
    "emb_history = []\n",
    "for _ in range(10):\n",
    "    for centers, pos, negs in list_data:\n",
    "        centers = torch.tensor(encoder[centers]).unsqueeze(dim=-1)\n",
    "        pos = torch.tensor(encoder[pos]).unsqueeze(dim=-1)\n",
    "        negs = torch.tensor([encoder[w] for w in negs]).unsqueeze(dim=0)\n",
    "\n",
    "        pos = pos\n",
    "        negs = negs\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = modelW2V(centers, pos, negs)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        loss_history.append(batch_loss)\n",
    "\n",
    "        k = list(encoder.keys())\n",
    "        v = list(encoder.values())\n",
    "        \n",
    "        matrix_sim = cosine_similarity_matrix(modelW2V.word_emb)\n",
    "        update_sim_history(words=k, idx=v, cos_sim_history=all_cos_sim, similarity_matrix=matrix_sim)\n",
    "    \n",
    "    w = deepcopy(modelW2V.word_emb.weight.detach().cpu().numpy())\n",
    "    emb_history.append(w)\n",
    "    w = deepcopy(modelW2V.con_emb.weight.detach().cpu().numpy())\n",
    "    rm_me.append(w)\n",
    "    \n",
    "\n",
    "k = list(encoder.keys())\n",
    "idx = list(encoder.values())\n",
    "m_to_h = matrix_sim.detach().numpy()\n",
    "m_to_h_2 = m_to_h[idx,:]\n",
    "m_to_h_2 = m_to_h_2[:, idx]\n",
    "\n",
    "m_to_h_2 = (m_to_h_2 + 1) / 2 * 100\n",
    "heat_map(k, m_to_h_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d302fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_colors = {\n",
    "    'chat': (\"blue\",  \"cyan\"),\n",
    "    'peluche': (\"green\", \"lightgreen\"),\n",
    "}\n",
    "\n",
    "\n",
    "fig = components_to_fig_3D_animation(\n",
    "    history_components=emb_history,\n",
    "    encoder=encoder,\n",
    "    words_display=list(encoder.keys()),\n",
    "    highlight_words=[\"chat\", \"peluche\"],\n",
    "    nb_neighbors=2, _min=-5, _max=5, base_color=base_colors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_colors = {\n",
    "    'chat': (\"blue\",  \"cyan\"),\n",
    "    'peluche': (\"green\", \"lightgreen\"),\n",
    "}\n",
    "\n",
    "\n",
    "fig = components_to_fig_3D_animation(\n",
    "    history_components=rm_me,\n",
    "    encoder=encoder,\n",
    "    words_display=list(encoder.keys()),\n",
    "    highlight_words=[\"chat\", \"peluche\"],\n",
    "    nb_neighbors=2, _min=-5, _max=5, base_color=base_colors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98537bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_cos_sim['train']['peluche'])\n",
    "print(all_cos_sim['chat']['chien'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_data(\n",
    "    language: str,\n",
    "    dataseteur: Callable[..., object],\n",
    "    window_size: int = 3,\n",
    "    nb_neg: int = 3,\n",
    "    subsample_thresh: float = 1.0,\n",
    "    vocab_size_limit: Optional[int] = None,\n",
    "    power:float =0.75,\n",
    "    file: Optional[str] = None,\n",
    "    files: Optional[list[str]] = None,\n",
    "    sentences: Optional[List[List[str]]] = None,\n",
    "    remove_accent: bool = True,\n",
    "    remove_ponct: bool = True,\n",
    "    keep_accent: bool = True,\n",
    "    contraction_map: Optional[dict] = None,\n",
    "    stop_words:List[str] = []\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if contraction_map is None:\n",
    "        contraction_map = {\n",
    "            \"n't\": \" n't\", \"'re\": \" 're\", \"'ve\": \" 've\", \"'ll\": \" 'll\",\n",
    "            \"'d\": \" 'd\", \"'s\": \" 's\", \"'m\": \" 'm\"\n",
    "        }\n",
    "\n",
    "    if all([(files is None), (file is None), (sentences is None)],):\n",
    "        raise AssertionError(\"One of files, file or sentence must not be None\")\n",
    "\n",
    "    if language not in {\"english\", \"french\", None}:\n",
    "        raise ValueError(\"language must be 'english' or 'french' or None\")\n",
    "\n",
    "    def remove_accents(text: str) -> str:\n",
    "        nk = unicodedata.normalize(\"NFKD\", text)\n",
    "        return \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "\n",
    "    keep = {\"'\", \"’\"} if keep_accent else set()\n",
    "    base_punct = set(string.punctuation)\n",
    "    extra_punct = set('“”‘’—–…«»')\n",
    "    punct_to_remove = (base_punct | extra_punct) - keep\n",
    "    TRANSL_TABLE = str.maketrans('', '', ''.join(sorted(punct_to_remove)))\n",
    "\n",
    "    tokens_by_sentence: List[List[str]] = []\n",
    "\n",
    "    if files is not None:\n",
    "        for name_file in files:\n",
    "            with open(name_file, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    s = line.strip().lower()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    if remove_accent:\n",
    "                        s = remove_accents(s)\n",
    "                    for k, v in contraction_map.items():\n",
    "                        s = s.replace(k, v)\n",
    "                    if remove_ponct:\n",
    "                        s = s.translate(TRANSL_TABLE)\n",
    "                    s2 = [word for word in s.split() if word not in stop_words]\n",
    "                    s = \" \".join(s2)\n",
    "                    toks = word_tokenize(s, language=language)\n",
    "                    if toks:\n",
    "                        tokens_by_sentence.append(toks)\n",
    "\n",
    "    elif file is not None:\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip().lower()\n",
    "                if not s:\n",
    "                    continue\n",
    "                if remove_accent:\n",
    "                    s = remove_accents(s)\n",
    "                for k, v in contraction_map.items():\n",
    "                    s = s.replace(k, v)\n",
    "                if remove_ponct:\n",
    "                    s = s.translate(TRANSL_TABLE)\n",
    "                s2 = [word for word in s.split() if word not in stop_words]\n",
    "                s = \" \".join(s2)                \n",
    "                if not s:\n",
    "                    continue\n",
    "                toks = word_tokenize(s, language=language)\n",
    "                if toks:\n",
    "                    tokens_by_sentence.append(toks)\n",
    "    else:\n",
    "        tokens_by_sentence = [list(s) for s in sentences if s]\n",
    "\n",
    "    return dataseteur(\n",
    "        sentences=tokens_by_sentence,\n",
    "        window_size=window_size,\n",
    "        nb_neg=nb_neg,\n",
    "        subsample_thresh=subsample_thresh,\n",
    "        power=power,\n",
    "        vocab_size_limit=vocab_size_limit\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset:SGNS_store_DataSet = pipe_data(\n",
    "    language=\"french\",\n",
    "    dataseteur=SGNS_store_DataSet,\n",
    "    window_size = 3,\n",
    "    nb_neg=5,\n",
    "    subsample_thresh= 1,\n",
    "    vocab_size_limit=None,\n",
    "    file=\"data/GPT5v2.txt\",\n",
    "    # files=[\"data/chat_chien.txt\", \"data/toy.txt\"],\n",
    "    remove_accent=True,\n",
    "    remove_ponct=True,\n",
    "    keep_accent= False,\n",
    "    contraction_map=None,\n",
    "    stop_words=[\"le\", \"les\", \"sur\", \"jouer\", \"fait\", \"de\", \"et\", \"la\", \"des\", \"sont\"] + \\\n",
    "    [\"the\", \"your\", \"a\", \"rubber\"]\n",
    "\n",
    ")\n",
    "\n",
    "print(dataset[0])\n",
    "data = DataLoader(dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V_emb2:SkipGramModel = SkipGramModel(emb_size=dataset.vocab_size, embedding_dimension=3, init_range=1, sparse=True)\n",
    "# optimizer = torch.optim.Adam(modelW2V.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.AdamW(modelW2V.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SparseAdam(modelW2V_emb2.parameters(), lr=0.02)\n",
    "# optimizer = torch.optim.SGD(modelW2V.parameters(), lr=0.01, weight_decay=None)\n",
    "\n",
    "\n",
    "all_cos_sim = {}\n",
    "\n",
    "for w1 in dataset.encoder.keys():\n",
    "    all_cos_sim[w1] = {}\n",
    "    for w2 in dataset.encoder.keys():\n",
    "        all_cos_sim[w1][w2] = []\n",
    "\n",
    "modelW2V_emb2.train()\n",
    "\n",
    "k = ['chat', 'chien', 'balle', 'train']\n",
    "v = dataset.encode(k)\n",
    "\n",
    "emb_history = []\n",
    "contexte_hist = []\n",
    "for _ in range(60):\n",
    "    for sentence_nb, (centers, pos, negs) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        loss = modelW2V_emb2(centers, pos, negs)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        epoch_loss += batch_loss\n",
    "        loss_history.append(batch_loss)\n",
    "       \n",
    "        matrix_sim = cosine_similarity_matrix(modelW2V_emb2.word_emb)\n",
    "        update_sim_history(words=k, idx=v, cos_sim_history=all_cos_sim, similarity_matrix=matrix_sim)\n",
    "    w = deepcopy(modelW2V_emb2.word_emb.weight.detach().cpu().numpy())\n",
    "    emb_history.append(w)\n",
    "    w = deepcopy(modelW2V_emb2.con_emb.weight.detach().cpu().numpy())\n",
    "    contexte_hist.append(w)\n",
    "\n",
    "m_to_h = matrix_sim.detach().numpy()\n",
    "m_to_h_2 = m_to_h[dataset.encode(k),:]\n",
    "m_to_h_2 = m_to_h_2[:, dataset.encode(k)]\n",
    "\n",
    "m_to_h_2 = (m_to_h_2 + 1) / 2 * 100\n",
    "heat_map(k, m_to_h_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_colors = {\n",
    "    'le': (\"red\",   \"orange\"),\n",
    "    'chien': (\"blue\",  \"cyan\"),\n",
    "    'balle': (\"green\", \"lightgreen\"),\n",
    "    'train': (\"purple\",\"violet\"),\n",
    "    'chat': (\"brown\", \"peru\"),\n",
    "    'et': (\"magenta\",\"pink\"),\n",
    "    'animals': (\"blue\",  \"cyan\"),\n",
    "    'toy': (\"green\", \"lightgreen\"),\n",
    "    'tail': (\"purple\",\"violet\"),\n",
    "}\n",
    "\n",
    "fig = components_to_fig_3D_animation(\n",
    "    history_components=emb_history,\n",
    "    encoder=dataset.encoder,\n",
    "    words_display=list(dataset.encoder.keys()),\n",
    "    highlight_words=k, base_color=base_colors,\n",
    "    nb_neighbors=6, _min=-5, _max=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04edfb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = components_to_fig_3D_animation(\n",
    "    history_components=contexte_hist,\n",
    "    encoder=dataset.encoder,\n",
    "    words_display=list(dataset.encoder.keys()),\n",
    "    highlight_words=k, base_color=base_colors,\n",
    "    nb_neighbors=6, _min=-5, _max=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contraction_map = {\n",
    "#     \"n't\": \" n't\", \"'re\": \" 're\", \"'ve\": \" 've\", \"'ll\": \" 'll\",\n",
    "#     \"'d\": \" 'd\", \"'s\": \" 's\", \"'m\": \" 'm\"\n",
    "# }\n",
    "\n",
    "# def remove_accents(text: str) -> str:\n",
    "#     nk = unicodedata.normalize(\"NFKD\", text)\n",
    "#     return \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "\n",
    "# keep = {\"'\", \"’\"}\n",
    "# base_punct = set(string.punctuation)\n",
    "# extra_punct = set('“”‘’—–…«»')\n",
    "# punct_to_remove = (base_punct | extra_punct) - keep\n",
    "# TRANSL_TABLE = str.maketrans('', '', ''.join(sorted(punct_to_remove)))\n",
    "\n",
    "# tokens_by_sentence: List[List[str]] = []\n",
    "\n",
    "# stop_words = [\"le\", \"les\", \"sur\", \"jouer\", \"fait\", \"de\", \"et\", \"la\", \"des\", \"sont\"] + \\\n",
    "#     [\"the\", \"your\", \"a\", \"rubber\"]\n",
    "\n",
    "# with open(\"data/dataSeparate.txt\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         s = line.strip().lower()\n",
    "#         if not s:\n",
    "#             continue\n",
    "#         s = remove_accents(s)\n",
    "#         s = s.translate(TRANSL_TABLE)\n",
    "#         s2 = [word for word in s.split() if word not in stop_words]\n",
    "#         s = \" \".join(s2)\n",
    "\n",
    "#         toks = word_tokenize(s, language=\"english\")\n",
    "\n",
    "#         if toks:\n",
    "#             tokens_by_sentence.append(toks)\n",
    "\n",
    "# dataset:SGNS_store_DataSet=SGNS_store_DataSet(tokens_by_sentence, 3, 10, power=1, subsample_thresh=1)\n",
    "# data = DataLoader(dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V:SkipGramModel = SkipGramModel(dataset.vocab_size, embedding_dimension=3, init_range=1, sparse=False)\n",
    "optimizer = torch.optim.Adam(modelW2V.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.AdamW(modelW2V.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.SparseAdam(modelW2V.parameters(), lr=0.1)\n",
    "# optimizer = torch.optim.SGD(modelW2V.parameters(), lr=0.01, weight_decay=None)\n",
    "\n",
    "\n",
    "all_cos_sim = {}\n",
    "\n",
    "for w1 in dataset.encoder.keys():\n",
    "    all_cos_sim[w1] = {}\n",
    "    for w2 in dataset.encoder.keys():\n",
    "        all_cos_sim[w1][w2] = []\n",
    "\n",
    "modelW2V.train()\n",
    "\n",
    "k = ['tail', 'animals', 'toy', 'spherical']\n",
    "v = dataset.encode(k)\n",
    "\n",
    "emb_history = []\n",
    "emb_con_history = []\n",
    "\n",
    "nb_epoch = 30\n",
    "for _ in range(nb_epoch):\n",
    "    w = deepcopy(modelW2V.word_emb.weight.detach().cpu().numpy())\n",
    "    emb_history.append(w)\n",
    "\n",
    "    w = deepcopy(modelW2V.con_emb.weight.detach().cpu().numpy())\n",
    "    emb_con_history.append(w)\n",
    "\n",
    "    for sentence_nb, (centers, pos, negs) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        loss = modelW2V(centers, pos, negs)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "       \n",
    "        matrix_sim = cosine_similarity_matrix(modelW2V.word_emb)\n",
    "        update_sim_history(words=k, idx=v, cos_sim_history=all_cos_sim, similarity_matrix=matrix_sim)\n",
    "\n",
    "m_to_h = matrix_sim.detach().numpy()\n",
    "m_to_h_2 = m_to_h[dataset.encode(k),:]\n",
    "m_to_h_2 = m_to_h_2[:, dataset.encode(k)]\n",
    "\n",
    "m_to_h_2 = (m_to_h_2 + 1) / 2 * 100\n",
    "heat_map(k, m_to_h_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3039b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyser_anisotropie(embeddings):\n",
    "    \"\"\"\n",
    "    Analyse la distribution des vecteurs d'embedding.\n",
    "    Args:\n",
    "        embeddings: np.array de forme (n_samples, n_features)\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we are interesting by the direction of vector, so we can (and must) normalize each vector to have same distance\n",
    "    normes = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    embeddings_norm = embeddings / (normes + 1e-10)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim_matrix = cosine_similarity(embeddings_norm)\n",
    "\n",
    "    # Keep only one result for each pair of cosine\n",
    "    indices = np.triu_indices_from(sim_matrix, k=1)\n",
    "    mean_cosine = np.mean(sim_matrix[indices])\n",
    "    \n",
    "    print(f\"Cosine similarity (mean) : {mean_cosine:.4f}\")\n",
    "\n",
    "    pca = PCA(n_components=min(embeddings.shape[0], embeddings.shape[1]))\n",
    "    pca.fit(embeddings_norm)\n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    print(f\"Ratio de variance expliquée () : {explained_var}\")\n",
    "    \n",
    "\n",
    "    mean_vector = np.mean(embeddings_norm, axis=0)\n",
    "    norm_mean_vector = np.linalg.norm(mean_vector)\n",
    "    \n",
    "    print(f\"Norme du vecteur moyen (0=isotrope, 1=aligné) : {norm_mean_vector:.4f}\")\n",
    "\n",
    "\n",
    "# analyser_anisotropie(emb_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122be79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05503ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelW2V:SkipGramModel = SkipGramModel(dataset.vocab_size, embedding_dimension=15, init_range=1, sparse=False)\n",
    "# optimizer = torch.optim.Adam(modelW2V.parameters(), lr=0.01)\n",
    "\n",
    "# all_cos_sim = {}\n",
    "\n",
    "# for w1 in dataset.encoder.keys():\n",
    "#     all_cos_sim[w1] = {}\n",
    "#     for w2 in dataset.encoder.keys():\n",
    "#         all_cos_sim[w1][w2] = []\n",
    "\n",
    "# modelW2V.train()\n",
    "\n",
    "# k = ['chat', 'chien', 'balle', 'train']\n",
    "# v = dataset.encode(k)\n",
    "\n",
    "# nb_epoch = 30\n",
    "# for _ in range(nb_epoch):\n",
    "#     for sentence_nb, (centers, pos, negs) in enumerate(data):\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = modelW2V(centers, pos, negs)\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "       \n",
    "#         matrix_sim = cosine_similarity_matrix(modelW2V.word_emb)\n",
    "#         update_sim_history(words=k, idx=v, cos_sim_history=all_cos_sim, similarity_matrix=matrix_sim)\n",
    "\n",
    "# k = ['chat', 'chien', 'balle', 'train']\n",
    "# m_to_h = matrix_sim.detach().numpy()\n",
    "# m_to_h_2 = m_to_h[dataset.encode(k),:]\n",
    "# m_to_h_2 = m_to_h_2[:, dataset.encode(k)]\n",
    "\n",
    "# m_to_h_2 = (m_to_h_2 + 1) / 2 * 100\n",
    "# heat_map(k, m_to_h_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9859f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser_anisotropie(modelW2V.word_emb.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30517fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import skew\n",
    "\n",
    "def analyser_anisotropie_advanced(embeddings, max_samples=10000):\n",
    "    \"\"\"\n",
    "    Advanced anisotropy analysis with mathematical fixes and PC alignment.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: np.array (n_samples, n_features)\n",
    "        max_samples: int, limit for pairwise calculation to avoid RAM OOM.\n",
    "    \"\"\"\n",
    "    n, d = embeddings.shape\n",
    "    \n",
    "    # ---- 0. Sampling for Pairwise Ops (Speed/RAM Safety) ----\n",
    "    if n > max_samples:\n",
    "        indices = np.random.choice(n, max_samples, replace=False)\n",
    "        sample_emb = embeddings[indices]\n",
    "    else:\n",
    "        sample_emb = embeddings\n",
    "\n",
    "    # ---- 1. Normalization ----\n",
    "    # We use the sample for expensive stats, full for global stats\n",
    "    norms = np.linalg.norm(sample_emb, axis=1, keepdims=True)\n",
    "    emb_norm = sample_emb / (norms + 1e-10)\n",
    "    \n",
    "    # Full dataset normalization for PCA\n",
    "    full_norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    full_emb_norm = embeddings / (full_norms + 1e-10)\n",
    "\n",
    "    print(f\"--- Geometry Analysis (N={n}, D={d}) ---\")\n",
    "\n",
    "    # ---- 2. Pairwise Cosine Statistics ----\n",
    "    # High mean + Low Std = The \"Narrow Cone\" Effect\n",
    "    sim_matrix = cosine_similarity(emb_norm)\n",
    "    # Extract upper triangle to avoid self-similarity (1.0) and duplicates\n",
    "    i, j = np.triu_indices_from(sim_matrix, k=1)\n",
    "    sims = sim_matrix[i, j]\n",
    "\n",
    "    print(f\"Mean Cosine Sim:      {sims.mean():.4f}  (Target: ~0 for isotropic)\")\n",
    "    print(f\"Std Cosine Sim:       {sims.std():.4f}\")\n",
    "    print(f\"Distribution Skew:    {skew(sims):.4f}  (>0 means skewed toward alignment)\")\n",
    "\n",
    "    # ---- 3. The Ethayarajh Metric (Vector Space Drift) ----\n",
    "    # Does the space define a specific direction?\n",
    "    global_mean_vec = full_emb_norm.mean(axis=0)\n",
    "    mean_vec_norm = np.linalg.norm(global_mean_vec)\n",
    "    \n",
    "    print(f\"Norm of Mean Vector:  {mean_vec_norm:.4f}  (0=Isotropic, 1=Collapsed)\")\n",
    "\n",
    "    # ---- 4. PCA & Effective Rank (Dimensionality) ----\n",
    "    pca = PCA(n_components=min(d, n))\n",
    "    pca.fit(full_emb_norm)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    \n",
    "    # Effective Rank (Evans et al., 2022)\n",
    "    # Measures the \"true\" number of dimensions being used\n",
    "    entropy = -np.sum(evr * np.log(evr + 1e-12))\n",
    "    eff_rank = np.exp(entropy)\n",
    "    \n",
    "    print(f\"Top-1 Variance:       {evr[0]:.4f}\")\n",
    "    print(f\"Effective Rank:       {eff_rank:.1f} / {d}\")\n",
    "\n",
    "    # ---- 5. Alignment to Principal Components (New) ----\n",
    "    # Are vectors just pointing at the top eigenvector?\n",
    "    # We project unit vectors onto the first Principal Component\n",
    "    pc1 = pca.components_[0]\n",
    "    # Dot product (cosine since vectors are normalized)\n",
    "    alignment_pc1 = np.abs(full_emb_norm @ pc1)\n",
    "    print(f\"Avg Alignment to PC1: {alignment_pc1.mean():.4f}  (1.0 = perfectly aligned on PC1)\")\n",
    "\n",
    "    # ---- 6. Whitening Impact (Corrected Math) ----\n",
    "    # We calculate how much 'centering' alone fixes the problem.\n",
    "    # If Centering fixes it -> The anisotropy is just a shift (easy fix).\n",
    "    # If Whitening is needed -> The anisotropy is scaling/correlation (harder).\n",
    "    \n",
    "    centered = emb_norm - emb_norm.mean(axis=0)\n",
    "    \n",
    "    # Calculate mean cosine of centered data\n",
    "    sim_centered = cosine_similarity(centered)\n",
    "    sims_c = sim_centered[np.triu_indices_from(sim_centered, k=1)]\n",
    "    \n",
    "    print(f\"\\n--- Remediation Check ---\")\n",
    "    print(f\"Original Mean Cosine: {sims.mean():.4f}\")\n",
    "    print(f\"Centered Mean Cosine: {sims_c.mean():.4f}  (Lower is better)\")\n",
    "    \n",
    "    # Quick check: If Centered Mean Cosine is close to 0, you don't strictly need Whitening.\n",
    "    if sims_c.mean() < 0.05:\n",
    "        print(\">> Result: Centering alone restores isotropy.\")\n",
    "    else:\n",
    "        print(\">> Result: Deep anisotropy detected (requires whitening/flow-based correction).\")\n",
    "\n",
    "    return {\n",
    "        \"mean_cosine\": sims.mean(),\n",
    "        \"effective_rank\": eff_rank,\n",
    "        \"pc1_alignment\": alignment_pc1.mean()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b513431",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser_anisotropie_advanced(modelW2V.word_emb.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ee503",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset:SGNS_store_DataSet = pipe_data(\n",
    "    language=\"french\",\n",
    "    dataseteur=SGNS_store_DataSet,\n",
    "    window_size = 3,\n",
    "    nb_neg=5,\n",
    "    subsample_thresh= 1,\n",
    "    vocab_size_limit=None,\n",
    "    file=\"data/GPT5v2.txt\",\n",
    "    remove_accent=True,\n",
    "    remove_ponct=True,\n",
    "    keep_accent= False,\n",
    "    contraction_map=None,\n",
    "    stop_words=[\"le\", \"les\", \"sur\", \"jouer\", \"fait\", \"de\", \"et\", \"la\", \"des\", \"sont\"] + \\\n",
    "    [\"the\", \"your\", \"a\", \"rubber\"]\n",
    "\n",
    ")\n",
    "data = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "nb_epoch = 30\n",
    "similarity = {}\n",
    "k = [\"chat\", \"chien\", \"train\", \"balle\"]\n",
    "for i in range(2, 40, 2):\n",
    "    print(f\"{\"=\"*20} embedding {i}\")\n",
    "    similarity[i] = {}\n",
    "    modelW2V:SkipGramModel = SkipGramModel(dataset.vocab_size, embedding_dimension=i, init_range=None, sparse=True)\n",
    "    optimizer = torch.optim.SparseAdam(modelW2V.parameters(), lr=0.009)\n",
    "\n",
    "    for _ in range(nb_epoch):\n",
    "        for sentence_nb, (centers, pos, negs) in enumerate(data):\n",
    "            optimizer.zero_grad()\n",
    "            loss = modelW2V(centers, pos, negs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    similarity[i][\"sim\"] = cosine_similarity_matrix(modelW2V.word_emb)\n",
    "    m_to_h = similarity[i][\"sim\"]\n",
    "    m_to_h = ((m_to_h + 1) / 2) * 100\n",
    "    m_to_h_2 = m_to_h[dataset.encode(k),:]\n",
    "    m_to_h_2 = m_to_h_2[:, dataset.encode(k)]\n",
    "    heat_map(words=k, similarity_matrix=m_to_h_2, save_file=f\"W2V_{i}\")\n",
    "    similarity[i][\"anistropie\"] = analyser_anisotropie_advanced(modelW2V.word_emb.weight.detach().cpu().numpy())\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in similarity.items():\n",
    "    print(key, value[\"anistropie\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
