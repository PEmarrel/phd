{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Word2Vec Model\n",
        "==============\n",
        "\n",
        "Introduces Gensim's Word2Vec model and demonstrates its use on the `Lee Evaluation Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you missed the buzz, Word2Vec is a widely used algorithm based on neural\n",
        "networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow).\n",
        "Using large amounts of unannotated plain text, word2vec learns relationships\n",
        "between words automatically. The output are vectors, one vector per word,\n",
        "with remarkable linear relationships that allow us to do things like:\n",
        "\n",
        "* vec(\"king\") - vec(\"man\") + vec(\"woman\") =~ vec(\"queen\")\n",
        "* vec(\"Montreal Canadiens\") – vec(\"Montreal\") + vec(\"Toronto\") =~ vec(\"Toronto Maple Leafs\").\n",
        "\n",
        "Word2vec is very useful in `automatic text tagging\n",
        "<https://github.com/RaRe-Technologies/movie-plots-by-genre>`_\\ , recommender\n",
        "systems and machine translation.\n",
        "\n",
        "This tutorial:\n",
        "\n",
        "#. Introduces ``Word2Vec`` as an improvement over traditional bag-of-words\n",
        "#. Shows off a demo of ``Word2Vec`` using a pre-trained model\n",
        "#. Demonstrates training a new model from your own data\n",
        "#. Demonstrates loading and saving models\n",
        "#. Introduces several training parameters and demonstrates their effect\n",
        "#. Discusses memory requirements\n",
        "#. Visualizes Word2Vec embeddings by applying dimensionality reduction\n",
        "\n",
        "Review: Bag-of-words\n",
        "--------------------\n",
        "\n",
        ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
        "\n",
        "You may be familiar with the `bag-of-words model\n",
        "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
        "`core_concepts_vector` section.\n",
        "This model transforms each document to a fixed-length vector of integers.\n",
        "For example, given the sentences:\n",
        "\n",
        "- ``John likes to watch movies. Mary likes movies too.``\n",
        "- ``John also likes to watch football games. Mary hates football.``\n",
        "\n",
        "The model outputs the vectors:\n",
        "\n",
        "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
        "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
        "\n",
        "Each vector has 10 elements, where each element counts the number of times a\n",
        "particular word occurred in the document.\n",
        "The order of elements is arbitrary.\n",
        "In the example above, the order of the elements corresponds to the words:\n",
        "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
        "\n",
        "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
        "\n",
        "First, they lose all information about word order: \"John likes Mary\" and\n",
        "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
        "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
        "models consider word phrases of length n to represent documents as\n",
        "fixed-length vectors to capture local word order but suffer from data\n",
        "sparsity and high dimensionality.\n",
        "\n",
        "Second, the model does not attempt to learn the meaning of the underlying\n",
        "words, and as a consequence, the distance between vectors doesn't always\n",
        "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
        "second problem.\n",
        "\n",
        "Introducing: the ``Word2Vec`` Model\n",
        "-----------------------------------\n",
        "\n",
        "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
        "vector space using a shallow neural network. The result is a set of\n",
        "word-vectors where vectors close together in vector space have similar\n",
        "meanings based on context, and word-vectors distant to each other have\n",
        "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
        "together and ``strong`` and ``Paris`` would be relatively far.\n",
        "\n",
        "The are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`\n",
        "class implements them both:\n",
        "\n",
        "1. Skip-grams (SG)\n",
        "2. Continuous-bag-of-words (CBOW)\n",
        "\n",
        ".. Important::\n",
        "  Don't let the implementation details below scare you.\n",
        "  They're advanced material: if it's too much, then move on to the next section.\n",
        "\n",
        "The `Word2Vec Skip-gram <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model>`__\n",
        "model, for example, takes in pairs (word1, word2) generated by moving a\n",
        "window across text data, and trains a 1-hidden-layer neural network based on\n",
        "the synthetic task of given an input word, giving us a predicted probability\n",
        "distribution of nearby words to the input. A virtual `one-hot\n",
        "<https://en.wikipedia.org/wiki/One-hot>`__ encoding of words\n",
        "goes through a 'projection layer' to the hidden layer; these projection\n",
        "weights are later interpreted as the word embeddings. So if the hidden layer\n",
        "has 300 neurons, this network will give us 300-dimensional word embeddings.\n",
        "\n",
        "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It\n",
        "is also a 1-hidden-layer neural network. The synthetic training task now uses\n",
        "the average of multiple input context words, rather than a single word as in\n",
        "skip-gram, to predict the center word. Again, the projection weights that\n",
        "turn one-hot words into averageable vectors, of the same width as the hidden\n",
        "layer, are interpreted as the word embeddings.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Word2Vec Demo\n",
        "-------------\n",
        "\n",
        "To see what ``Word2Vec`` can do, let's download a pre-trained model and play\n",
        "around with it. We will fetch the Word2Vec model trained on part of the\n",
        "Google News dataset, covering approximately 3 million words and phrases. Such\n",
        "a model can take hours to train, but since it's already available,\n",
        "downloading and loading it with Gensim takes minutes.\n",
        "\n",
        ".. Important::\n",
        "  The model is approximately 2GB, so you'll need a decent network connection\n",
        "  to proceed.  Otherwise, skip ahead to the \"Training Your Own Model\" section\n",
        "  below.\n",
        "\n",
        "You may also check out an `online word2vec demo\n",
        "<http://radimrehurek.com/2014/02/word2vec-tutorial/#app>`_ where you can try\n",
        "this vector algebra for yourself. That demo runs ``word2vec`` on the\n",
        "**entire** Google News dataset, of **about 100 billion words**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:04:50,008 : INFO : loading projection weights from /home/PE/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
            "2025-10-17 14:05:35,350 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/PE/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-10-17T14:05:35.350876', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'load_word2vec_format'}\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A common operation is to retrieve the vocabulary of a model. That is trivial:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word #0/3000000 is </s>\n",
            "word #1/3000000 is in\n",
            "word #2/3000000 is for\n",
            "word #3/3000000 is that\n",
            "word #4/3000000 is is\n",
            "word #5/3000000 is on\n",
            "word #6/3000000 is ##\n",
            "word #7/3000000 is The\n",
            "word #8/3000000 is with\n",
            "word #9/3000000 is said\n"
          ]
        }
      ],
      "source": [
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 10:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily obtain vectors for terms the model is familiar with:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vec_king = wv['king']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, the model is unable to infer vectors for unfamiliar words.\n",
        "This is one limitation of Word2Vec: if this limitation matters to you, check\n",
        "out the FastText model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The word 'cameroon' does not appear in this model\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    vec_cameroon = wv['cameroon']\n",
        "except KeyError:\n",
        "    print(\"The word 'cameroon' does not appear in this model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving on, ``Word2Vec`` supports several word similarity tasks out of the\n",
        "box.  You can see how the similarity intuitively decreases as the words get\n",
        "less and less similar.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'car'\t'minivan'\t0.69\n",
            "'car'\t'bicycle'\t0.54\n",
            "'car'\t'airplane'\t0.42\n",
            "'car'\t'cereal'\t0.14\n",
            "'car'\t'communism'\t0.06\n"
          ]
        }
      ],
      "source": [
        "pairs = [\n",
        "    ('car', 'minivan'),   # a minivan is a kind of car\n",
        "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
        "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
        "    ('car', 'cereal'),    # ... and so on\n",
        "    ('car', 'communism'),\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the 5 most similar words to \"car\" or \"minivan\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which of the below does not belong in the sequence?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "car\n"
          ]
        }
      ],
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Your Own Model\n",
        "-----------------------\n",
        "\n",
        "To start, you'll need some data for training the model. For the following\n",
        "examples, we'll use the `Lee Evaluation Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
        "(which you `already have\n",
        "<https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor>`_\n",
        "if you've installed Gensim).\n",
        "\n",
        "This corpus is small enough to fit entirely in memory, but we'll implement a\n",
        "memory-friendly iterator that reads it line-by-line to demonstrate how you\n",
        "would handle a larger corpus.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:05:58,249 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
            "2025-10-17 14:05:58,249 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
            "2025-10-17 14:05:58,250 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2025-10-17T14:05:58.250227', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = datapath('lee_background.cor')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we wanted to do any custom preprocessing, e.g. decode a non-standard\n",
        "encoding, lowercase, remove numbers, extract named entities... All of this can\n",
        "be done inside the ``MyCorpus`` iterator and ``word2vec`` doesn’t need to\n",
        "know. All that is required is that the input yields one sentence (list of\n",
        "utf8 words) after another.\n",
        "\n",
        "Let's go ahead and train a model on our corpus.  Don't worry about the\n",
        "training parameters much for now, we'll revisit them later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:05:58,699 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:05:58,700 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:05:58,762 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
            "2025-10-17 14:05:58,763 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:05:58,768 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.07% of original 6981, drops 5231)', 'datetime': '2025-10-17T14:05:58.768180', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:05:58,769 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.84% of original 58152, drops 8817)', 'datetime': '2025-10-17T14:05:58.769085', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:05:58,777 : INFO : deleting the raw counts dictionary of 6981 items\n",
            "2025-10-17 14:05:58,777 : INFO : sample=0.001 downsamples 51 most-common words\n",
            "2025-10-17 14:05:58,778 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2025-10-17T14:05:58.778483', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:05:58,790 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
            "2025-10-17 14:05:58,791 : INFO : resetting layer weights\n",
            "2025-10-17 14:05:58,793 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:05:58.793304', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:05:58,793 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:05:58.793840', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:05:58,879 : INFO : EPOCH 0: training on 58152 raw words (36013 effective words) took 0.1s, 432289 effective words/s\n",
            "2025-10-17 14:05:58,966 : INFO : EPOCH 1: training on 58152 raw words (35923 effective words) took 0.1s, 424908 effective words/s\n",
            "2025-10-17 14:05:59,045 : INFO : EPOCH 2: training on 58152 raw words (35928 effective words) took 0.1s, 461773 effective words/s\n",
            "2025-10-17 14:05:59,119 : INFO : EPOCH 3: training on 58152 raw words (35901 effective words) took 0.1s, 491771 effective words/s\n",
            "2025-10-17 14:05:59,200 : INFO : EPOCH 4: training on 58152 raw words (35858 effective words) took 0.1s, 453315 effective words/s\n",
            "2025-10-17 14:05:59,200 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179623 effective words) took 0.4s, 442252 effective words/s', 'datetime': '2025-10-17T14:05:59.200948', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:05:59,201 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1750, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:05:59.201454', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "import gensim.models\n",
        "\n",
        "sentences = MyCorpus()\n",
        "model = gensim.models.Word2Vec(sentences=sentences)\n",
        "\n",
        "# for s in sentences:\n",
        "#     print(s)\n",
        "#     raise SystemExit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have our model, we can use it in the same way as in the demo above.\n",
        "\n",
        "The main part of the model is ``model.wv``\\ , where \"wv\" stands for \"word vectors\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vec_king = model.wv['king']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieving the vocabulary works the same way:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word #0/3000000 is </s>\n",
            "word #1/3000000 is in\n",
            "word #2/3000000 is for\n",
            "word #3/3000000 is that\n",
            "word #4/3000000 is is\n",
            "word #5/3000000 is on\n",
            "word #6/3000000 is ##\n",
            "word #7/3000000 is The\n",
            "word #8/3000000 is with\n",
            "word #9/3000000 is said\n"
          ]
        }
      ],
      "source": [
        "for index, word in enumerate(wv.index_to_key):\n",
        "    if index == 10:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storing and loading models\n",
        "--------------------------\n",
        "\n",
        "You'll notice that training non-trivial models can take time.  Once you've\n",
        "trained your model and it works as expected, you can save it to disk.  That\n",
        "way, you don't have to spend time training it all over again later.\n",
        "\n",
        "You can store/load models using the standard gensim methods:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:00,272 : INFO : Word2Vec lifecycle event {'fname_or_handle': '/tmp/gensim-model-jfcd632k', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-10-17T14:06:00.272168', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'saving'}\n",
            "2025-10-17 14:06:00,272 : INFO : not storing attribute cum_table\n",
            "2025-10-17 14:06:00,274 : INFO : saved /tmp/gensim-model-jfcd632k\n",
            "2025-10-17 14:06:00,275 : INFO : loading Word2Vec object from /tmp/gensim-model-jfcd632k\n",
            "2025-10-17 14:06:00,453 : INFO : loading wv recursively from /tmp/gensim-model-jfcd632k.wv.* with mmap=None\n",
            "2025-10-17 14:06:00,454 : INFO : setting ignored attribute cum_table to None\n",
            "2025-10-17 14:06:00,464 : INFO : Word2Vec lifecycle event {'fname': '/tmp/gensim-model-jfcd632k', 'datetime': '2025-10-17T14:06:00.464941', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'loaded'}\n"
          ]
        }
      ],
      "source": [
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
        "    temporary_filepath = tmp.name\n",
        "    model.save(temporary_filepath)\n",
        "    #\n",
        "    # The model is now safely stored in the filepath.\n",
        "    # You can copy it to other machines, share it with others, etc.\n",
        "    #\n",
        "    # To load a saved model:\n",
        "    #\n",
        "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "which uses pickle internally, optionally ``mmap``\\ ‘ing the model’s internal\n",
        "large NumPy matrices into virtual memory directly from disk files, for\n",
        "inter-process memory sharing.\n",
        "\n",
        "In addition, you can load models created by the original C tool, both using\n",
        "its text and binary formats::\n",
        "\n",
        "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
        "  # using gzipped/bz2 input works too, no need to unzip\n",
        "  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Parameters\n",
        "-------------------\n",
        "\n",
        "``Word2Vec`` accepts several parameters that affect both training speed and quality.\n",
        "\n",
        "min_count\n",
        "---------\n",
        "\n",
        "``min_count`` is for pruning the internal dictionary. Words that appear only\n",
        "once or twice in a billion-word corpus are probably uninteresting typos and\n",
        "garbage. In addition, there’s not enough data to make any meaningful training\n",
        "on those words, so it’s best to ignore them:\n",
        "\n",
        "default value of min_count=5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:00,837 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:00,838 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:00,916 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
            "2025-10-17 14:06:00,917 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:00,920 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 889 unique words (12.73% of original 6981, drops 6092)', 'datetime': '2025-10-17T14:06:00.920731', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:00,921 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 43776 word corpus (75.28% of original 58152, drops 14376)', 'datetime': '2025-10-17T14:06:00.921224', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:00,925 : INFO : deleting the raw counts dictionary of 6981 items\n",
            "2025-10-17 14:06:00,925 : INFO : sample=0.001 downsamples 55 most-common words\n",
            "2025-10-17 14:06:00,926 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 29691.39528319831 word corpus (67.8%% of prior 43776)', 'datetime': '2025-10-17T14:06:00.926397', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:00,933 : INFO : estimated required memory for 889 words and 100 dimensions: 1155700 bytes\n",
            "2025-10-17 14:06:00,934 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:00,935 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:00.935042', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:00,935 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 889 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:00.935400', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:01,012 : INFO : EPOCH 0: training on 58152 raw words (29685 effective words) took 0.1s, 393073 effective words/s\n",
            "2025-10-17 14:06:01,106 : INFO : EPOCH 1: training on 58152 raw words (29741 effective words) took 0.1s, 319486 effective words/s\n",
            "2025-10-17 14:06:01,179 : INFO : EPOCH 2: training on 58152 raw words (29613 effective words) took 0.1s, 413384 effective words/s\n",
            "2025-10-17 14:06:01,257 : INFO : EPOCH 3: training on 58152 raw words (29657 effective words) took 0.1s, 385850 effective words/s\n",
            "2025-10-17 14:06:01,338 : INFO : EPOCH 4: training on 58152 raw words (29652 effective words) took 0.1s, 375862 effective words/s\n",
            "2025-10-17 14:06:01,338 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (148348 effective words) took 0.4s, 368219 effective words/s', 'datetime': '2025-10-17T14:06:01.338582', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:01,339 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=889, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:01.339195', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "model = gensim.models.Word2Vec(sentences, min_count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "vector_size\n",
        "-----------\n",
        "\n",
        "``vector_size`` is the number of dimensions (N) of the N-dimensional space that\n",
        "gensim Word2Vec maps the words onto.\n",
        "\n",
        "Bigger size values require more training data, but can lead to better (more\n",
        "accurate) models. Reasonable values are in the tens to hundreds.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:01,357 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:01,360 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:01,469 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
            "2025-10-17 14:06:01,469 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:01,478 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.07% of original 6981, drops 5231)', 'datetime': '2025-10-17T14:06:01.478168', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:01,478 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.84% of original 58152, drops 8817)', 'datetime': '2025-10-17T14:06:01.478852', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:01,487 : INFO : deleting the raw counts dictionary of 6981 items\n",
            "2025-10-17 14:06:01,487 : INFO : sample=0.001 downsamples 51 most-common words\n",
            "2025-10-17 14:06:01,488 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2025-10-17T14:06:01.488944', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:01,500 : INFO : estimated required memory for 1750 words and 200 dimensions: 3675000 bytes\n",
            "2025-10-17 14:06:01,501 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:01,504 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:01.504793', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:01,505 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1750 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:01.505415', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:01,585 : INFO : EPOCH 0: training on 58152 raw words (35936 effective words) took 0.1s, 458840 effective words/s\n",
            "2025-10-17 14:06:01,674 : INFO : EPOCH 1: training on 58152 raw words (35991 effective words) took 0.1s, 411026 effective words/s\n",
            "2025-10-17 14:06:01,769 : INFO : EPOCH 2: training on 58152 raw words (35888 effective words) took 0.1s, 383204 effective words/s\n",
            "2025-10-17 14:06:01,860 : INFO : EPOCH 3: training on 58152 raw words (35915 effective words) took 0.1s, 399489 effective words/s\n",
            "2025-10-17 14:06:01,942 : INFO : EPOCH 4: training on 58152 raw words (36037 effective words) took 0.1s, 445504 effective words/s\n",
            "2025-10-17 14:06:01,943 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179767 effective words) took 0.4s, 410422 effective words/s', 'datetime': '2025-10-17T14:06:01.943906', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:01,944 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1750, vector_size=200, alpha=0.025>', 'datetime': '2025-10-17T14:06:01.944575', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "# The default value of vector_size is 100.\n",
        "model = gensim.models.Word2Vec(sentences, vector_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "workers\n",
        "-------\n",
        "\n",
        "``workers`` , the last of the major parameters (full list `here\n",
        "<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>`_)\n",
        "is for training parallelization, to speed up training:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:01,988 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:01,990 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:02,092 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
            "2025-10-17 14:06:02,093 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:02,101 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.07% of original 6981, drops 5231)', 'datetime': '2025-10-17T14:06:02.101511', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:02,102 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.84% of original 58152, drops 8817)', 'datetime': '2025-10-17T14:06:02.102183', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:02,118 : INFO : deleting the raw counts dictionary of 6981 items\n",
            "2025-10-17 14:06:02,119 : INFO : sample=0.001 downsamples 51 most-common words\n",
            "2025-10-17 14:06:02,119 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2025-10-17T14:06:02.119671', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:02,136 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
            "2025-10-17 14:06:02,137 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:02,140 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:02.140195', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:02,140 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:02.140808', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:02,218 : INFO : EPOCH 0: training on 58152 raw words (35914 effective words) took 0.1s, 473632 effective words/s\n",
            "2025-10-17 14:06:02,300 : INFO : EPOCH 1: training on 58152 raw words (35918 effective words) took 0.1s, 449381 effective words/s\n",
            "2025-10-17 14:06:02,401 : INFO : EPOCH 2: training on 58152 raw words (35964 effective words) took 0.1s, 363902 effective words/s\n",
            "2025-10-17 14:06:02,486 : INFO : EPOCH 3: training on 58152 raw words (35946 effective words) took 0.1s, 428406 effective words/s\n",
            "2025-10-17 14:06:02,562 : INFO : EPOCH 4: training on 58152 raw words (35833 effective words) took 0.1s, 484485 effective words/s\n",
            "2025-10-17 14:06:02,563 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179575 effective words) took 0.4s, 425286 effective words/s', 'datetime': '2025-10-17T14:06:02.563510', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:02,563 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1750, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:02.563915', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n"
          ]
        }
      ],
      "source": [
        "# default value of workers=3 (tutorial says 1...)\n",
        "model = gensim.models.Word2Vec(sentences, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``workers`` parameter only has an effect if you have `Cython\n",
        "<http://cython.org/>`_ installed. Without Cython, you’ll only be able to use\n",
        "one core because of the `GIL\n",
        "<https://wiki.python.org/moin/GlobalInterpreterLock>`_ (and ``word2vec``\n",
        "training will be `miserably slow\n",
        "<http://rare-technologies.com/word2vec-in-python-part-two-optimizing/>`_\\ ).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Memory\n",
        "------\n",
        "\n",
        "At its core, ``word2vec`` model parameters are stored as matrices (NumPy\n",
        "arrays). Each array is **#vocabulary** (controlled by the ``min_count`` parameter)\n",
        "times **vector size** (the ``vector_size`` parameter) of floats (single precision aka 4 bytes).\n",
        "\n",
        "Three such matrices are held in RAM (work is underway to reduce that number\n",
        "to two, or even one). So if your input contains 100,000 unique words, and you\n",
        "asked for layer ``vector_size=200``\\ , the model will require approx.\n",
        "``100,000*200*4*3 bytes = ~229MB``.\n",
        "\n",
        "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would\n",
        "take a few megabytes), but unless your words are extremely loooong strings, memory\n",
        "footprint will be dominated by the three matrices above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating\n",
        "----------\n",
        "\n",
        "``Word2Vec`` training is an unsupervised task, there’s no good way to\n",
        "objectively evaluate the result. Evaluation depends on your end application.\n",
        "\n",
        "Google has released their testing set of about 20,000 syntactic and semantic\n",
        "test examples, following the “A is to B as C is to D” task. It is provided in\n",
        "the 'datasets' folder.\n",
        "\n",
        "For example a syntactic analogy of comparative type is ``bad:worse;good:?``.\n",
        "There are total of 9 types of syntactic comparisons in the dataset like\n",
        "plural nouns and nouns of opposite meaning.\n",
        "\n",
        "The semantic questions contain five types of semantic analogies, such as\n",
        "capital cities (``Paris:France;Tokyo:?``) or family members\n",
        "(``brother:sister;dad:?``).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gensim supports the same evaluation set, in exactly the same format:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:02,807 : INFO : Evaluating word analogies for top 300000 words in the model on /home/PE/Documents/small_Word2Vec/.venv3.11/lib/python3.11/site-packages/gensim/test/test_data/questions-words.txt\n",
            "2025-10-17 14:06:02,826 : INFO : capital-common-countries: 0.0% (0/6)\n",
            "2025-10-17 14:06:02,969 : INFO : capital-world: 0.0% (0/2)\n",
            "2025-10-17 14:06:03,077 : INFO : family: 0.0% (0/6)\n",
            "2025-10-17 14:06:03,107 : INFO : gram3-comparative: 0.0% (0/20)\n",
            "2025-10-17 14:06:03,121 : INFO : gram4-superlative: 0.0% (0/12)\n",
            "2025-10-17 14:06:03,139 : INFO : gram5-present-participle: 0.0% (0/20)\n",
            "2025-10-17 14:06:03,170 : INFO : gram6-nationality-adjective: 0.0% (0/30)\n",
            "2025-10-17 14:06:03,231 : INFO : gram7-past-tense: 0.0% (0/20)\n",
            "2025-10-17 14:06:03,256 : INFO : gram8-plural: 0.0% (0/30)\n",
            "2025-10-17 14:06:03,263 : INFO : Quadruplets with out-of-vocabulary words: 99.3%\n",
            "2025-10-17 14:06:03,266 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
            "2025-10-17 14:06:03,267 : INFO : Total accuracy: 0.0% (0/146)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0,\n",
              " [{'section': 'capital-common-countries',\n",
              "   'correct': [],\n",
              "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
              "    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
              "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
              "    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
              "    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
              "    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]},\n",
              "  {'section': 'capital-world',\n",
              "   'correct': [],\n",
              "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
              "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]},\n",
              "  {'section': 'currency', 'correct': [], 'incorrect': []},\n",
              "  {'section': 'city-in-state', 'correct': [], 'incorrect': []},\n",
              "  {'section': 'family',\n",
              "   'correct': [],\n",
              "   'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
              "    ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
              "    ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
              "    ('HIS', 'HER', 'HE', 'SHE'),\n",
              "    ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
              "    ('MAN', 'WOMAN', 'HIS', 'HER')]},\n",
              "  {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []},\n",
              "  {'section': 'gram2-opposite', 'correct': [], 'incorrect': []},\n",
              "  {'section': 'gram3-comparative',\n",
              "   'correct': [],\n",
              "   'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
              "    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
              "    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
              "    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
              "    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
              "    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
              "    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
              "    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
              "    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
              "    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
              "    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
              "    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
              "    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
              "    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
              "    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
              "    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
              "    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
              "    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
              "    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
              "    ('SMALL', 'SMALLER', 'LOW', 'LOWER')]},\n",
              "  {'section': 'gram4-superlative',\n",
              "   'correct': [],\n",
              "   'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
              "    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
              "    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
              "    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
              "    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
              "    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
              "    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
              "    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
              "    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
              "    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
              "    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
              "    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]},\n",
              "  {'section': 'gram5-present-participle',\n",
              "   'correct': [],\n",
              "   'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
              "    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
              "    ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
              "    ('GO', 'GOING', 'SAY', 'SAYING'),\n",
              "    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
              "    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
              "    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
              "    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
              "    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
              "    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
              "    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
              "    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
              "    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
              "    ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
              "    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
              "    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
              "    ('SAY', 'SAYING', 'GO', 'GOING'),\n",
              "    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
              "    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
              "    ('SAY', 'SAYING', 'RUN', 'RUNNING')]},\n",
              "  {'section': 'gram6-nationality-adjective',\n",
              "   'correct': [],\n",
              "   'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
              "    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
              "    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
              "    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
              "    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
              "    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
              "    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
              "    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
              "    ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
              "    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
              "    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
              "    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
              "    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
              "    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
              "    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
              "    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
              "    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
              "    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
              "    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
              "    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
              "    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]},\n",
              "  {'section': 'gram7-past-tense',\n",
              "   'correct': [],\n",
              "   'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
              "    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
              "    ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
              "    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
              "    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
              "    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
              "    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
              "    ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
              "    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
              "    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
              "    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
              "    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
              "    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
              "    ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
              "    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
              "    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
              "    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
              "    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
              "    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
              "    ('TAKING', 'TOOK', 'SAYING', 'SAID')]},\n",
              "  {'section': 'gram8-plural',\n",
              "   'correct': [],\n",
              "   'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
              "    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
              "    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
              "    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
              "    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
              "    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
              "    ('CAR', 'CARS', 'MAN', 'MEN'),\n",
              "    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
              "    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
              "    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
              "    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
              "    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
              "    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
              "    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
              "    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
              "    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
              "    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
              "    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
              "    ('MAN', 'MEN', 'CAR', 'CARS'),\n",
              "    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
              "    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
              "    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
              "    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
              "    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
              "    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
              "    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
              "    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
              "    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
              "    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
              "    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]},\n",
              "  {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []},\n",
              "  {'section': 'Total accuracy',\n",
              "   'correct': [],\n",
              "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
              "    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
              "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
              "    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
              "    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
              "    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'),\n",
              "    ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
              "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
              "    ('HE', 'SHE', 'HIS', 'HER'),\n",
              "    ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
              "    ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
              "    ('HIS', 'HER', 'HE', 'SHE'),\n",
              "    ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
              "    ('MAN', 'WOMAN', 'HIS', 'HER'),\n",
              "    ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
              "    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
              "    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
              "    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
              "    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
              "    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
              "    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
              "    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
              "    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
              "    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
              "    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
              "    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
              "    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
              "    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
              "    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
              "    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
              "    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
              "    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
              "    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
              "    ('SMALL', 'SMALLER', 'LOW', 'LOWER'),\n",
              "    ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
              "    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
              "    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
              "    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
              "    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
              "    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
              "    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
              "    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
              "    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
              "    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
              "    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
              "    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
              "    ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
              "    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
              "    ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
              "    ('GO', 'GOING', 'SAY', 'SAYING'),\n",
              "    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
              "    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
              "    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
              "    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
              "    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
              "    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
              "    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
              "    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
              "    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
              "    ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
              "    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
              "    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
              "    ('SAY', 'SAYING', 'GO', 'GOING'),\n",
              "    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
              "    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
              "    ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
              "    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
              "    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
              "    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
              "    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
              "    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
              "    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
              "    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
              "    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
              "    ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
              "    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
              "    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
              "    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
              "    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
              "    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
              "    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
              "    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
              "    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
              "    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
              "    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
              "    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
              "    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
              "    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'),\n",
              "    ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
              "    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
              "    ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
              "    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
              "    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
              "    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
              "    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
              "    ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
              "    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
              "    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
              "    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
              "    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
              "    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
              "    ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
              "    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
              "    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
              "    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
              "    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
              "    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
              "    ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
              "    ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
              "    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
              "    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
              "    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
              "    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
              "    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
              "    ('CAR', 'CARS', 'MAN', 'MEN'),\n",
              "    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
              "    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
              "    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
              "    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
              "    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
              "    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
              "    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
              "    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
              "    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
              "    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
              "    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
              "    ('MAN', 'MEN', 'CAR', 'CARS'),\n",
              "    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
              "    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
              "    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
              "    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
              "    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
              "    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
              "    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
              "    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
              "    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
              "    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
              "    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ``evaluate_word_analogies`` method takes an `optional parameter\n",
        "<http://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies>`_\n",
        "``restrict_vocab`` which limits which test examples are to be considered.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
        "\n",
        "By default it uses an academic dataset WS-353 but one can create a dataset\n",
        "specific to your business based on it. It contains word pairs together with\n",
        "human-assigned similarity judgments. It measures the relatedness or\n",
        "co-occurrence of two words. For example, 'coast' and 'shore' are very similar\n",
        "as they appear in the same context. At the same time 'clothes' and 'closet'\n",
        "are less similar because they are related but not interchangeable.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:03,512 : INFO : Skipping line #2 with OOV words: love\tsex\t6.77\n",
            "2025-10-17 14:06:03,513 : INFO : Skipping line #3 with OOV words: tiger\tcat\t7.35\n",
            "2025-10-17 14:06:03,513 : INFO : Skipping line #4 with OOV words: tiger\ttiger\t10.00\n",
            "2025-10-17 14:06:03,514 : INFO : Skipping line #5 with OOV words: book\tpaper\t7.46\n",
            "2025-10-17 14:06:03,514 : INFO : Skipping line #6 with OOV words: computer\tkeyboard\t7.62\n",
            "2025-10-17 14:06:03,514 : INFO : Skipping line #7 with OOV words: computer\tinternet\t7.58\n",
            "2025-10-17 14:06:03,515 : INFO : Skipping line #9 with OOV words: train\tcar\t6.31\n",
            "2025-10-17 14:06:03,515 : INFO : Skipping line #10 with OOV words: telephone\tcommunication\t7.50\n",
            "2025-10-17 14:06:03,516 : INFO : Skipping line #14 with OOV words: bread\tbutter\t6.19\n",
            "2025-10-17 14:06:03,516 : INFO : Skipping line #15 with OOV words: cucumber\tpotato\t5.92\n",
            "2025-10-17 14:06:03,517 : INFO : Skipping line #16 with OOV words: doctor\tnurse\t7.00\n",
            "2025-10-17 14:06:03,517 : INFO : Skipping line #18 with OOV words: student\tprofessor\t6.81\n",
            "2025-10-17 14:06:03,518 : INFO : Skipping line #19 with OOV words: smart\tstudent\t4.62\n",
            "2025-10-17 14:06:03,518 : INFO : Skipping line #20 with OOV words: smart\tstupid\t5.81\n",
            "2025-10-17 14:06:03,518 : INFO : Skipping line #21 with OOV words: company\tstock\t7.08\n",
            "2025-10-17 14:06:03,519 : INFO : Skipping line #22 with OOV words: stock\tmarket\t8.08\n",
            "2025-10-17 14:06:03,519 : INFO : Skipping line #23 with OOV words: stock\tphone\t1.62\n",
            "2025-10-17 14:06:03,519 : INFO : Skipping line #24 with OOV words: stock\tCD\t1.31\n",
            "2025-10-17 14:06:03,520 : INFO : Skipping line #25 with OOV words: stock\tjaguar\t0.92\n",
            "2025-10-17 14:06:03,520 : INFO : Skipping line #26 with OOV words: stock\tegg\t1.81\n",
            "2025-10-17 14:06:03,520 : INFO : Skipping line #27 with OOV words: fertility\tegg\t6.69\n",
            "2025-10-17 14:06:03,521 : INFO : Skipping line #28 with OOV words: stock\tlive\t3.73\n",
            "2025-10-17 14:06:03,521 : INFO : Skipping line #29 with OOV words: stock\tlife\t0.92\n",
            "2025-10-17 14:06:03,522 : INFO : Skipping line #30 with OOV words: book\tlibrary\t7.46\n",
            "2025-10-17 14:06:03,522 : INFO : Skipping line #32 with OOV words: wood\tforest\t7.73\n",
            "2025-10-17 14:06:03,525 : INFO : Skipping line #33 with OOV words: money\tcash\t9.15\n",
            "2025-10-17 14:06:03,525 : INFO : Skipping line #34 with OOV words: professor\tcucumber\t0.31\n",
            "2025-10-17 14:06:03,525 : INFO : Skipping line #35 with OOV words: king\tcabbage\t0.23\n",
            "2025-10-17 14:06:03,526 : INFO : Skipping line #36 with OOV words: king\tqueen\t8.58\n",
            "2025-10-17 14:06:03,526 : INFO : Skipping line #37 with OOV words: king\trook\t5.92\n",
            "2025-10-17 14:06:03,526 : INFO : Skipping line #38 with OOV words: bishop\trabbi\t6.69\n",
            "2025-10-17 14:06:03,527 : INFO : Skipping line #41 with OOV words: holy\tsex\t1.62\n",
            "2025-10-17 14:06:03,528 : INFO : Skipping line #42 with OOV words: fuck\tsex\t9.44\n",
            "2025-10-17 14:06:03,528 : INFO : Skipping line #43 with OOV words: Maradona\tfootball\t8.62\n",
            "2025-10-17 14:06:03,529 : INFO : Skipping line #44 with OOV words: football\tsoccer\t9.03\n",
            "2025-10-17 14:06:03,530 : INFO : Skipping line #45 with OOV words: football\tbasketball\t6.81\n",
            "2025-10-17 14:06:03,530 : INFO : Skipping line #46 with OOV words: football\ttennis\t6.63\n",
            "2025-10-17 14:06:03,531 : INFO : Skipping line #47 with OOV words: tennis\tracket\t7.56\n",
            "2025-10-17 14:06:03,531 : INFO : Skipping line #50 with OOV words: Arafat\tJackson\t2.50\n",
            "2025-10-17 14:06:03,532 : INFO : Skipping line #51 with OOV words: law\tlawyer\t8.38\n",
            "2025-10-17 14:06:03,532 : INFO : Skipping line #52 with OOV words: movie\tstar\t7.38\n",
            "2025-10-17 14:06:03,533 : INFO : Skipping line #53 with OOV words: movie\tpopcorn\t6.19\n",
            "2025-10-17 14:06:03,533 : INFO : Skipping line #54 with OOV words: movie\tcritic\t6.73\n",
            "2025-10-17 14:06:03,533 : INFO : Skipping line #55 with OOV words: movie\ttheater\t7.92\n",
            "2025-10-17 14:06:03,534 : INFO : Skipping line #56 with OOV words: physics\tproton\t8.12\n",
            "2025-10-17 14:06:03,534 : INFO : Skipping line #57 with OOV words: physics\tchemistry\t7.35\n",
            "2025-10-17 14:06:03,534 : INFO : Skipping line #58 with OOV words: space\tchemistry\t4.88\n",
            "2025-10-17 14:06:03,535 : INFO : Skipping line #59 with OOV words: alcohol\tchemistry\t5.54\n",
            "2025-10-17 14:06:03,535 : INFO : Skipping line #60 with OOV words: vodka\tgin\t8.46\n",
            "2025-10-17 14:06:03,535 : INFO : Skipping line #61 with OOV words: vodka\tbrandy\t8.13\n",
            "2025-10-17 14:06:03,536 : INFO : Skipping line #62 with OOV words: drink\tcar\t3.04\n",
            "2025-10-17 14:06:03,537 : INFO : Skipping line #63 with OOV words: drink\tear\t1.31\n",
            "2025-10-17 14:06:03,537 : INFO : Skipping line #64 with OOV words: drink\tmouth\t5.96\n",
            "2025-10-17 14:06:03,538 : INFO : Skipping line #65 with OOV words: drink\teat\t6.87\n",
            "2025-10-17 14:06:03,538 : INFO : Skipping line #66 with OOV words: baby\tmother\t7.85\n",
            "2025-10-17 14:06:03,538 : INFO : Skipping line #67 with OOV words: drink\tmother\t2.65\n",
            "2025-10-17 14:06:03,539 : INFO : Skipping line #68 with OOV words: car\tautomobile\t8.94\n",
            "2025-10-17 14:06:03,539 : INFO : Skipping line #69 with OOV words: gem\tjewel\t8.96\n",
            "2025-10-17 14:06:03,539 : INFO : Skipping line #70 with OOV words: journey\tvoyage\t9.29\n",
            "2025-10-17 14:06:03,540 : INFO : Skipping line #71 with OOV words: boy\tlad\t8.83\n",
            "2025-10-17 14:06:03,540 : INFO : Skipping line #72 with OOV words: coast\tshore\t9.10\n",
            "2025-10-17 14:06:03,541 : INFO : Skipping line #73 with OOV words: asylum\tmadhouse\t8.87\n",
            "2025-10-17 14:06:03,541 : INFO : Skipping line #74 with OOV words: magician\twizard\t9.02\n",
            "2025-10-17 14:06:03,541 : INFO : Skipping line #75 with OOV words: midday\tnoon\t9.29\n",
            "2025-10-17 14:06:03,541 : INFO : Skipping line #76 with OOV words: furnace\tstove\t8.79\n",
            "2025-10-17 14:06:03,542 : INFO : Skipping line #77 with OOV words: food\tfruit\t7.52\n",
            "2025-10-17 14:06:03,542 : INFO : Skipping line #78 with OOV words: bird\tcock\t7.10\n",
            "2025-10-17 14:06:03,542 : INFO : Skipping line #79 with OOV words: bird\tcrane\t7.38\n",
            "2025-10-17 14:06:03,543 : INFO : Skipping line #80 with OOV words: tool\timplement\t6.46\n",
            "2025-10-17 14:06:03,543 : INFO : Skipping line #81 with OOV words: brother\tmonk\t6.27\n",
            "2025-10-17 14:06:03,543 : INFO : Skipping line #82 with OOV words: crane\timplement\t2.69\n",
            "2025-10-17 14:06:03,544 : INFO : Skipping line #83 with OOV words: lad\tbrother\t4.46\n",
            "2025-10-17 14:06:03,544 : INFO : Skipping line #84 with OOV words: journey\tcar\t5.85\n",
            "2025-10-17 14:06:03,545 : INFO : Skipping line #85 with OOV words: monk\toracle\t5.00\n",
            "2025-10-17 14:06:03,545 : INFO : Skipping line #86 with OOV words: cemetery\twoodland\t2.08\n",
            "2025-10-17 14:06:03,546 : INFO : Skipping line #87 with OOV words: food\trooster\t4.42\n",
            "2025-10-17 14:06:03,549 : INFO : Skipping line #89 with OOV words: forest\tgraveyard\t1.85\n",
            "2025-10-17 14:06:03,549 : INFO : Skipping line #90 with OOV words: shore\twoodland\t3.08\n",
            "2025-10-17 14:06:03,549 : INFO : Skipping line #91 with OOV words: monk\tslave\t0.92\n",
            "2025-10-17 14:06:03,550 : INFO : Skipping line #92 with OOV words: coast\tforest\t3.15\n",
            "2025-10-17 14:06:03,550 : INFO : Skipping line #93 with OOV words: lad\twizard\t0.92\n",
            "2025-10-17 14:06:03,550 : INFO : Skipping line #94 with OOV words: chord\tsmile\t0.54\n",
            "2025-10-17 14:06:03,551 : INFO : Skipping line #95 with OOV words: glass\tmagician\t2.08\n",
            "2025-10-17 14:06:03,551 : INFO : Skipping line #96 with OOV words: noon\tstring\t0.54\n",
            "2025-10-17 14:06:03,552 : INFO : Skipping line #97 with OOV words: rooster\tvoyage\t0.62\n",
            "2025-10-17 14:06:03,552 : INFO : Skipping line #98 with OOV words: money\tdollar\t8.42\n",
            "2025-10-17 14:06:03,553 : INFO : Skipping line #99 with OOV words: money\tcash\t9.08\n",
            "2025-10-17 14:06:03,555 : INFO : Skipping line #100 with OOV words: money\tcurrency\t9.04\n",
            "2025-10-17 14:06:03,555 : INFO : Skipping line #101 with OOV words: money\twealth\t8.27\n",
            "2025-10-17 14:06:03,556 : INFO : Skipping line #103 with OOV words: money\tpossession\t7.29\n",
            "2025-10-17 14:06:03,557 : INFO : Skipping line #105 with OOV words: money\tdeposit\t7.73\n",
            "2025-10-17 14:06:03,558 : INFO : Skipping line #106 with OOV words: money\twithdrawal\t6.88\n",
            "2025-10-17 14:06:03,559 : INFO : Skipping line #107 with OOV words: money\tlaundering\t5.65\n",
            "2025-10-17 14:06:03,560 : INFO : Skipping line #109 with OOV words: tiger\tjaguar\t8.00\n",
            "2025-10-17 14:06:03,560 : INFO : Skipping line #110 with OOV words: tiger\tfeline\t8.00\n",
            "2025-10-17 14:06:03,560 : INFO : Skipping line #111 with OOV words: tiger\tcarnivore\t7.08\n",
            "2025-10-17 14:06:03,561 : INFO : Skipping line #112 with OOV words: tiger\tmammal\t6.85\n",
            "2025-10-17 14:06:03,561 : INFO : Skipping line #113 with OOV words: tiger\tanimal\t7.00\n",
            "2025-10-17 14:06:03,562 : INFO : Skipping line #114 with OOV words: tiger\torganism\t4.77\n",
            "2025-10-17 14:06:03,563 : INFO : Skipping line #115 with OOV words: tiger\tfauna\t5.62\n",
            "2025-10-17 14:06:03,564 : INFO : Skipping line #116 with OOV words: tiger\tzoo\t5.87\n",
            "2025-10-17 14:06:03,564 : INFO : Skipping line #117 with OOV words: psychology\tpsychiatry\t8.08\n",
            "2025-10-17 14:06:03,565 : INFO : Skipping line #118 with OOV words: psychology\tanxiety\t7.00\n",
            "2025-10-17 14:06:03,565 : INFO : Skipping line #119 with OOV words: psychology\tfear\t6.85\n",
            "2025-10-17 14:06:03,565 : INFO : Skipping line #120 with OOV words: psychology\tdepression\t7.42\n",
            "2025-10-17 14:06:03,566 : INFO : Skipping line #121 with OOV words: psychology\tclinic\t6.58\n",
            "2025-10-17 14:06:03,566 : INFO : Skipping line #122 with OOV words: psychology\tdoctor\t6.42\n",
            "2025-10-17 14:06:03,567 : INFO : Skipping line #123 with OOV words: psychology\tFreud\t8.21\n",
            "2025-10-17 14:06:03,568 : INFO : Skipping line #124 with OOV words: psychology\tmind\t7.69\n",
            "2025-10-17 14:06:03,568 : INFO : Skipping line #125 with OOV words: psychology\thealth\t7.23\n",
            "2025-10-17 14:06:03,569 : INFO : Skipping line #126 with OOV words: psychology\tscience\t6.71\n",
            "2025-10-17 14:06:03,569 : INFO : Skipping line #127 with OOV words: psychology\tdiscipline\t5.58\n",
            "2025-10-17 14:06:03,570 : INFO : Skipping line #128 with OOV words: psychology\tcognition\t7.48\n",
            "2025-10-17 14:06:03,570 : INFO : Skipping line #129 with OOV words: planet\tstar\t8.45\n",
            "2025-10-17 14:06:03,571 : INFO : Skipping line #130 with OOV words: planet\tconstellation\t8.06\n",
            "2025-10-17 14:06:03,571 : INFO : Skipping line #131 with OOV words: planet\tmoon\t8.08\n",
            "2025-10-17 14:06:03,572 : INFO : Skipping line #132 with OOV words: planet\tsun\t8.02\n",
            "2025-10-17 14:06:03,572 : INFO : Skipping line #133 with OOV words: planet\tgalaxy\t8.11\n",
            "2025-10-17 14:06:03,573 : INFO : Skipping line #134 with OOV words: planet\tspace\t7.92\n",
            "2025-10-17 14:06:03,573 : INFO : Skipping line #135 with OOV words: planet\tastronomer\t7.94\n",
            "2025-10-17 14:06:03,574 : INFO : Skipping line #136 with OOV words: precedent\texample\t5.85\n",
            "2025-10-17 14:06:03,574 : INFO : Skipping line #137 with OOV words: precedent\tinformation\t3.85\n",
            "2025-10-17 14:06:03,575 : INFO : Skipping line #138 with OOV words: precedent\tcognition\t2.81\n",
            "2025-10-17 14:06:03,575 : INFO : Skipping line #139 with OOV words: precedent\tlaw\t6.65\n",
            "2025-10-17 14:06:03,575 : INFO : Skipping line #140 with OOV words: precedent\tcollection\t2.50\n",
            "2025-10-17 14:06:03,576 : INFO : Skipping line #141 with OOV words: precedent\tgroup\t1.77\n",
            "2025-10-17 14:06:03,576 : INFO : Skipping line #142 with OOV words: precedent\tantecedent\t6.04\n",
            "2025-10-17 14:06:03,577 : INFO : Skipping line #143 with OOV words: cup\tcoffee\t6.58\n",
            "2025-10-17 14:06:03,577 : INFO : Skipping line #144 with OOV words: cup\ttableware\t6.85\n",
            "2025-10-17 14:06:03,578 : INFO : Skipping line #145 with OOV words: cup\tarticle\t2.40\n",
            "2025-10-17 14:06:03,578 : INFO : Skipping line #146 with OOV words: cup\tartifact\t2.92\n",
            "2025-10-17 14:06:03,579 : INFO : Skipping line #147 with OOV words: cup\tobject\t3.69\n",
            "2025-10-17 14:06:03,579 : INFO : Skipping line #148 with OOV words: cup\tentity\t2.15\n",
            "2025-10-17 14:06:03,580 : INFO : Skipping line #149 with OOV words: cup\tdrink\t7.25\n",
            "2025-10-17 14:06:03,580 : INFO : Skipping line #151 with OOV words: cup\tsubstance\t1.92\n",
            "2025-10-17 14:06:03,581 : INFO : Skipping line #152 with OOV words: cup\tliquid\t5.90\n",
            "2025-10-17 14:06:03,582 : INFO : Skipping line #153 with OOV words: jaguar\tcat\t7.42\n",
            "2025-10-17 14:06:03,584 : INFO : Skipping line #154 with OOV words: jaguar\tcar\t7.27\n",
            "2025-10-17 14:06:03,584 : INFO : Skipping line #157 with OOV words: energy\tlaboratory\t5.09\n",
            "2025-10-17 14:06:03,585 : INFO : Skipping line #158 with OOV words: computer\tlaboratory\t6.78\n",
            "2025-10-17 14:06:03,586 : INFO : Skipping line #159 with OOV words: weapon\tsecret\t6.06\n",
            "2025-10-17 14:06:03,586 : INFO : Skipping line #160 with OOV words: FBI\tfingerprint\t6.94\n",
            "2025-10-17 14:06:03,587 : INFO : Skipping line #161 with OOV words: FBI\tinvestigation\t8.31\n",
            "2025-10-17 14:06:03,588 : INFO : Skipping line #163 with OOV words: Mars\twater\t2.94\n",
            "2025-10-17 14:06:03,588 : INFO : Skipping line #164 with OOV words: Mars\tscientist\t5.63\n",
            "2025-10-17 14:06:03,590 : INFO : Skipping line #166 with OOV words: canyon\tlandscape\t7.53\n",
            "2025-10-17 14:06:03,591 : INFO : Skipping line #167 with OOV words: image\tsurface\t4.56\n",
            "2025-10-17 14:06:03,592 : INFO : Skipping line #168 with OOV words: discovery\tspace\t6.34\n",
            "2025-10-17 14:06:03,592 : INFO : Skipping line #169 with OOV words: water\tseepage\t6.56\n",
            "2025-10-17 14:06:03,593 : INFO : Skipping line #170 with OOV words: sign\trecess\t2.38\n",
            "2025-10-17 14:06:03,593 : INFO : Skipping line #172 with OOV words: mile\tkilometer\t8.66\n",
            "2025-10-17 14:06:03,594 : INFO : Skipping line #173 with OOV words: computer\tnews\t4.47\n",
            "2025-10-17 14:06:03,595 : INFO : Skipping line #174 with OOV words: territory\tsurface\t5.34\n",
            "2025-10-17 14:06:03,595 : INFO : Skipping line #175 with OOV words: atmosphere\tlandscape\t3.69\n",
            "2025-10-17 14:06:03,596 : INFO : Skipping line #176 with OOV words: president\tmedal\t3.00\n",
            "2025-10-17 14:06:03,596 : INFO : Skipping line #179 with OOV words: skin\teye\t6.22\n",
            "2025-10-17 14:06:03,597 : INFO : Skipping line #181 with OOV words: theater\thistory\t3.91\n",
            "2025-10-17 14:06:03,598 : INFO : Skipping line #182 with OOV words: volunteer\tmotto\t2.56\n",
            "2025-10-17 14:06:03,599 : INFO : Skipping line #183 with OOV words: prejudice\trecognition\t3.00\n",
            "2025-10-17 14:06:03,599 : INFO : Skipping line #184 with OOV words: decoration\tvalor\t5.63\n",
            "2025-10-17 14:06:03,600 : INFO : Skipping line #185 with OOV words: century\tyear\t7.59\n",
            "2025-10-17 14:06:03,600 : INFO : Skipping line #186 with OOV words: century\tnation\t3.16\n",
            "2025-10-17 14:06:03,602 : INFO : Skipping line #187 with OOV words: delay\tracism\t1.19\n",
            "2025-10-17 14:06:03,605 : INFO : Skipping line #191 with OOV words: minority\tpeace\t3.69\n",
            "2025-10-17 14:06:03,606 : INFO : Skipping line #194 with OOV words: deployment\tdeparture\t4.25\n",
            "2025-10-17 14:06:03,607 : INFO : Skipping line #195 with OOV words: deployment\twithdrawal\t5.88\n",
            "2025-10-17 14:06:03,607 : INFO : Skipping line #197 with OOV words: announcement\tnews\t7.56\n",
            "2025-10-17 14:06:03,608 : INFO : Skipping line #198 with OOV words: announcement\teffort\t2.75\n",
            "2025-10-17 14:06:03,609 : INFO : Skipping line #199 with OOV words: stroke\thospital\t7.03\n",
            "2025-10-17 14:06:03,609 : INFO : Skipping line #200 with OOV words: disability\tdeath\t5.47\n",
            "2025-10-17 14:06:03,610 : INFO : Skipping line #201 with OOV words: victim\temergency\t6.47\n",
            "2025-10-17 14:06:03,611 : INFO : Skipping line #203 with OOV words: journal\tassociation\t4.97\n",
            "2025-10-17 14:06:03,611 : INFO : Skipping line #205 with OOV words: doctor\tliability\t5.19\n",
            "2025-10-17 14:06:03,612 : INFO : Skipping line #206 with OOV words: liability\tinsurance\t7.03\n",
            "2025-10-17 14:06:03,612 : INFO : Skipping line #207 with OOV words: school\tcenter\t3.44\n",
            "2025-10-17 14:06:03,613 : INFO : Skipping line #208 with OOV words: reason\thypertension\t2.31\n",
            "2025-10-17 14:06:03,614 : INFO : Skipping line #209 with OOV words: reason\tcriterion\t5.91\n",
            "2025-10-17 14:06:03,614 : INFO : Skipping line #210 with OOV words: hundred\tpercent\t7.38\n",
            "2025-10-17 14:06:03,614 : INFO : Skipping line #211 with OOV words: Harvard\tYale\t8.13\n",
            "2025-10-17 14:06:03,615 : INFO : Skipping line #212 with OOV words: hospital\tinfrastructure\t4.63\n",
            "2025-10-17 14:06:03,615 : INFO : Skipping line #213 with OOV words: death\trow\t5.25\n",
            "2025-10-17 14:06:03,615 : INFO : Skipping line #214 with OOV words: death\tinmate\t5.03\n",
            "2025-10-17 14:06:03,616 : INFO : Skipping line #215 with OOV words: lawyer\tevidence\t6.69\n",
            "2025-10-17 14:06:03,619 : INFO : Skipping line #218 with OOV words: word\tsimilarity\t4.75\n",
            "2025-10-17 14:06:03,620 : INFO : Skipping line #219 with OOV words: board\trecommendation\t4.47\n",
            "2025-10-17 14:06:03,621 : INFO : Skipping line #221 with OOV words: OPEC\tcountry\t5.63\n",
            "2025-10-17 14:06:03,622 : INFO : Skipping line #222 with OOV words: peace\tatmosphere\t3.69\n",
            "2025-10-17 14:06:03,623 : INFO : Skipping line #224 with OOV words: territory\tkilometer\t5.28\n",
            "2025-10-17 14:06:03,624 : INFO : Skipping line #226 with OOV words: competition\tprice\t6.44\n",
            "2025-10-17 14:06:03,625 : INFO : Skipping line #227 with OOV words: consumer\tconfidence\t4.13\n",
            "2025-10-17 14:06:03,625 : INFO : Skipping line #228 with OOV words: consumer\tenergy\t4.75\n",
            "2025-10-17 14:06:03,625 : INFO : Skipping line #231 with OOV words: credit\tcard\t8.06\n",
            "2025-10-17 14:06:03,626 : INFO : Skipping line #233 with OOV words: hotel\treservation\t8.03\n",
            "2025-10-17 14:06:03,627 : INFO : Skipping line #234 with OOV words: grocery\tmoney\t5.94\n",
            "2025-10-17 14:06:03,628 : INFO : Skipping line #235 with OOV words: registration\tarrangement\t6.00\n",
            "2025-10-17 14:06:03,628 : INFO : Skipping line #236 with OOV words: arrangement\taccommodation\t5.41\n",
            "2025-10-17 14:06:03,629 : INFO : Skipping line #238 with OOV words: type\tkind\t8.97\n",
            "2025-10-17 14:06:03,629 : INFO : Skipping line #239 with OOV words: arrival\thotel\t6.00\n",
            "2025-10-17 14:06:03,630 : INFO : Skipping line #240 with OOV words: bed\tcloset\t6.72\n",
            "2025-10-17 14:06:03,630 : INFO : Skipping line #241 with OOV words: closet\tclothes\t8.00\n",
            "2025-10-17 14:06:03,631 : INFO : Skipping line #242 with OOV words: situation\tconclusion\t4.81\n",
            "2025-10-17 14:06:03,631 : INFO : Skipping line #243 with OOV words: situation\tisolation\t3.88\n",
            "2025-10-17 14:06:03,632 : INFO : Skipping line #244 with OOV words: impartiality\tinterest\t5.16\n",
            "2025-10-17 14:06:03,633 : INFO : Skipping line #245 with OOV words: direction\tcombination\t2.25\n",
            "2025-10-17 14:06:03,633 : INFO : Skipping line #246 with OOV words: street\tplace\t6.44\n",
            "2025-10-17 14:06:03,634 : INFO : Skipping line #247 with OOV words: street\tavenue\t8.88\n",
            "2025-10-17 14:06:03,634 : INFO : Skipping line #248 with OOV words: street\tblock\t6.88\n",
            "2025-10-17 14:06:03,635 : INFO : Skipping line #249 with OOV words: street\tchildren\t4.94\n",
            "2025-10-17 14:06:03,635 : INFO : Skipping line #250 with OOV words: listing\tproximity\t2.56\n",
            "2025-10-17 14:06:03,636 : INFO : Skipping line #251 with OOV words: listing\tcategory\t6.38\n",
            "2025-10-17 14:06:03,637 : INFO : Skipping line #252 with OOV words: cell\tphone\t7.81\n",
            "2025-10-17 14:06:03,637 : INFO : Skipping line #253 with OOV words: production\thike\t1.75\n",
            "2025-10-17 14:06:03,638 : INFO : Skipping line #254 with OOV words: benchmark\tindex\t4.25\n",
            "2025-10-17 14:06:03,639 : INFO : Skipping line #256 with OOV words: media\tgain\t2.88\n",
            "2025-10-17 14:06:03,641 : INFO : Skipping line #257 with OOV words: dividend\tpayment\t7.63\n",
            "2025-10-17 14:06:03,641 : INFO : Skipping line #258 with OOV words: dividend\tcalculation\t6.48\n",
            "2025-10-17 14:06:03,642 : INFO : Skipping line #259 with OOV words: calculation\tcomputation\t8.44\n",
            "2025-10-17 14:06:03,642 : INFO : Skipping line #260 with OOV words: currency\tmarket\t7.50\n",
            "2025-10-17 14:06:03,643 : INFO : Skipping line #261 with OOV words: OPEC\toil\t8.59\n",
            "2025-10-17 14:06:03,643 : INFO : Skipping line #262 with OOV words: oil\tstock\t6.34\n",
            "2025-10-17 14:06:03,644 : INFO : Skipping line #263 with OOV words: announcement\tproduction\t3.38\n",
            "2025-10-17 14:06:03,644 : INFO : Skipping line #264 with OOV words: announcement\twarning\t6.00\n",
            "2025-10-17 14:06:03,645 : INFO : Skipping line #265 with OOV words: profit\twarning\t3.88\n",
            "2025-10-17 14:06:03,645 : INFO : Skipping line #266 with OOV words: profit\tloss\t7.63\n",
            "2025-10-17 14:06:03,646 : INFO : Skipping line #267 with OOV words: dollar\tyen\t7.78\n",
            "2025-10-17 14:06:03,647 : INFO : Skipping line #268 with OOV words: dollar\tbuck\t9.22\n",
            "2025-10-17 14:06:03,647 : INFO : Skipping line #269 with OOV words: dollar\tprofit\t7.38\n",
            "2025-10-17 14:06:03,648 : INFO : Skipping line #270 with OOV words: dollar\tloss\t6.09\n",
            "2025-10-17 14:06:03,648 : INFO : Skipping line #271 with OOV words: computer\tsoftware\t8.50\n",
            "2025-10-17 14:06:03,649 : INFO : Skipping line #272 with OOV words: network\thardware\t8.31\n",
            "2025-10-17 14:06:03,649 : INFO : Skipping line #273 with OOV words: phone\tequipment\t7.13\n",
            "2025-10-17 14:06:03,649 : INFO : Skipping line #274 with OOV words: equipment\tmaker\t5.91\n",
            "2025-10-17 14:06:03,650 : INFO : Skipping line #275 with OOV words: luxury\tcar\t6.47\n",
            "2025-10-17 14:06:03,650 : INFO : Skipping line #277 with OOV words: report\tgain\t3.63\n",
            "2025-10-17 14:06:03,651 : INFO : Skipping line #278 with OOV words: investor\tearning\t7.13\n",
            "2025-10-17 14:06:03,651 : INFO : Skipping line #279 with OOV words: liquid\twater\t7.89\n",
            "2025-10-17 14:06:03,652 : INFO : Skipping line #280 with OOV words: baseball\tseason\t5.97\n",
            "2025-10-17 14:06:03,654 : INFO : Skipping line #283 with OOV words: marathon\tsprint\t7.47\n",
            "2025-10-17 14:06:03,654 : INFO : Skipping line #285 with OOV words: game\tdefeat\t6.97\n",
            "2025-10-17 14:06:03,655 : INFO : Skipping line #287 with OOV words: seafood\tsea\t7.47\n",
            "2025-10-17 14:06:03,655 : INFO : Skipping line #288 with OOV words: seafood\tfood\t8.34\n",
            "2025-10-17 14:06:03,656 : INFO : Skipping line #289 with OOV words: seafood\tlobster\t8.70\n",
            "2025-10-17 14:06:03,656 : INFO : Skipping line #290 with OOV words: lobster\tfood\t7.81\n",
            "2025-10-17 14:06:03,656 : INFO : Skipping line #291 with OOV words: lobster\twine\t5.70\n",
            "2025-10-17 14:06:03,657 : INFO : Skipping line #292 with OOV words: food\tpreparation\t6.22\n",
            "2025-10-17 14:06:03,658 : INFO : Skipping line #293 with OOV words: video\tarchive\t6.34\n",
            "2025-10-17 14:06:03,658 : INFO : Skipping line #298 with OOV words: championship\ttournament\t8.36\n",
            "2025-10-17 14:06:03,659 : INFO : Skipping line #299 with OOV words: fighting\tdefeating\t7.41\n",
            "2025-10-17 14:06:03,659 : INFO : Skipping line #301 with OOV words: day\tsummer\t3.94\n",
            "2025-10-17 14:06:03,659 : INFO : Skipping line #302 with OOV words: summer\tdrought\t7.16\n",
            "2025-10-17 14:06:03,660 : INFO : Skipping line #303 with OOV words: summer\tnature\t5.63\n",
            "2025-10-17 14:06:03,660 : INFO : Skipping line #304 with OOV words: day\tdawn\t7.53\n",
            "2025-10-17 14:06:03,660 : INFO : Skipping line #305 with OOV words: nature\tenvironment\t8.31\n",
            "2025-10-17 14:06:03,661 : INFO : Skipping line #306 with OOV words: environment\tecology\t8.81\n",
            "2025-10-17 14:06:03,661 : INFO : Skipping line #307 with OOV words: nature\tman\t6.25\n",
            "2025-10-17 14:06:03,663 : INFO : Skipping line #311 with OOV words: soap\topera\t7.94\n",
            "2025-10-17 14:06:03,664 : INFO : Skipping line #312 with OOV words: opera\tperformance\t6.88\n",
            "2025-10-17 14:06:03,664 : INFO : Skipping line #313 with OOV words: life\tlesson\t5.94\n",
            "2025-10-17 14:06:03,665 : INFO : Skipping line #315 with OOV words: production\tcrew\t6.25\n",
            "2025-10-17 14:06:03,665 : INFO : Skipping line #316 with OOV words: television\tfilm\t7.72\n",
            "2025-10-17 14:06:03,666 : INFO : Skipping line #317 with OOV words: lover\tquarrel\t6.19\n",
            "2025-10-17 14:06:03,666 : INFO : Skipping line #318 with OOV words: viewer\tserial\t2.97\n",
            "2025-10-17 14:06:03,667 : INFO : Skipping line #319 with OOV words: possibility\tgirl\t1.94\n",
            "2025-10-17 14:06:03,667 : INFO : Skipping line #320 with OOV words: population\tdevelopment\t3.75\n",
            "2025-10-17 14:06:03,668 : INFO : Skipping line #321 with OOV words: morality\timportance\t3.31\n",
            "2025-10-17 14:06:03,669 : INFO : Skipping line #322 with OOV words: morality\tmarriage\t3.69\n",
            "2025-10-17 14:06:03,670 : INFO : Skipping line #323 with OOV words: Mexico\tBrazil\t7.44\n",
            "2025-10-17 14:06:03,671 : INFO : Skipping line #324 with OOV words: gender\tequality\t6.41\n",
            "2025-10-17 14:06:03,671 : INFO : Skipping line #325 with OOV words: change\tattitude\t5.44\n",
            "2025-10-17 14:06:03,672 : INFO : Skipping line #327 with OOV words: opera\tindustry\t2.63\n",
            "2025-10-17 14:06:03,675 : INFO : Skipping line #328 with OOV words: sugar\tapproach\t0.88\n",
            "2025-10-17 14:06:03,675 : INFO : Skipping line #329 with OOV words: practice\tinstitution\t3.19\n",
            "2025-10-17 14:06:03,676 : INFO : Skipping line #330 with OOV words: ministry\tculture\t4.69\n",
            "2025-10-17 14:06:03,676 : INFO : Skipping line #331 with OOV words: problem\tchallenge\t6.75\n",
            "2025-10-17 14:06:03,677 : INFO : Skipping line #332 with OOV words: size\tprominence\t5.31\n",
            "2025-10-17 14:06:03,678 : INFO : Skipping line #333 with OOV words: country\tcitizen\t7.31\n",
            "2025-10-17 14:06:03,678 : INFO : Skipping line #334 with OOV words: planet\tpeople\t5.75\n",
            "2025-10-17 14:06:03,679 : INFO : Skipping line #335 with OOV words: development\tissue\t3.97\n",
            "2025-10-17 14:06:03,679 : INFO : Skipping line #336 with OOV words: experience\tmusic\t3.47\n",
            "2025-10-17 14:06:03,680 : INFO : Skipping line #337 with OOV words: music\tproject\t3.63\n",
            "2025-10-17 14:06:03,680 : INFO : Skipping line #338 with OOV words: glass\tmetal\t5.56\n",
            "2025-10-17 14:06:03,681 : INFO : Skipping line #339 with OOV words: aluminum\tmetal\t7.83\n",
            "2025-10-17 14:06:03,683 : INFO : Skipping line #340 with OOV words: chance\tcredibility\t3.88\n",
            "2025-10-17 14:06:03,684 : INFO : Skipping line #341 with OOV words: exhibit\tmemorabilia\t5.31\n",
            "2025-10-17 14:06:03,685 : INFO : Skipping line #342 with OOV words: concert\tvirtuoso\t6.81\n",
            "2025-10-17 14:06:03,685 : INFO : Skipping line #343 with OOV words: rock\tjazz\t7.59\n",
            "2025-10-17 14:06:03,686 : INFO : Skipping line #344 with OOV words: museum\ttheater\t7.19\n",
            "2025-10-17 14:06:03,686 : INFO : Skipping line #345 with OOV words: observation\tarchitecture\t4.38\n",
            "2025-10-17 14:06:03,687 : INFO : Skipping line #347 with OOV words: preservation\tworld\t6.19\n",
            "2025-10-17 14:06:03,687 : INFO : Skipping line #348 with OOV words: admission\tticket\t7.69\n",
            "2025-10-17 14:06:03,687 : INFO : Skipping line #349 with OOV words: shower\tthunderstorm\t6.31\n",
            "2025-10-17 14:06:03,688 : INFO : Skipping line #350 with OOV words: shower\tflood\t6.03\n",
            "2025-10-17 14:06:03,690 : INFO : Skipping line #354 with OOV words: architecture\tcentury\t3.78\n",
            "2025-10-17 14:06:04,385 : INFO : Pearson correlation coefficient against /home/PE/Documents/small_Word2Vec/.venv3.11/lib/python3.11/site-packages/gensim/test/test_data/wordsim353.tsv: 0.2122\n",
            "2025-10-17 14:06:04,386 : INFO : Spearman rank-order correlation coefficient against /home/PE/Documents/small_Word2Vec/.venv3.11/lib/python3.11/site-packages/gensim/test/test_data/wordsim353.tsv: 0.1482\n",
            "2025-10-17 14:06:04,386 : INFO : Pairs with unknown words ratio: 83.0%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(PearsonRResult(statistic=0.2121643451542885, pvalue=0.10364502314397497),\n",
              " SignificanceResult(statistic=0.14820269908504785, pvalue=0.2584431957742359),\n",
              " 83.0028328611898)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Important::\n",
        "  Good performance on Google's or WS-353 test set doesn’t mean word2vec will\n",
        "  work well in your application, or vice versa. It’s always best to evaluate\n",
        "  directly on your intended task. For an example of how to use word2vec in a\n",
        "  classifier pipeline, see this `tutorial\n",
        "  <https://github.com/RaRe-Technologies/movie-plots-by-genre>`_.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Online training / Resuming training\n",
        "-----------------------------------\n",
        "\n",
        "Advanced users can load a model and continue training it with more sentences\n",
        "and `new vocabulary words <online_w2v_tutorial.ipynb>`_:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:04,396 : INFO : loading Word2Vec object from /tmp/gensim-model-jfcd632k\n",
            "2025-10-17 14:06:04,400 : INFO : loading wv recursively from /tmp/gensim-model-jfcd632k.wv.* with mmap=None\n",
            "2025-10-17 14:06:04,401 : INFO : setting ignored attribute cum_table to None\n",
            "2025-10-17 14:06:04,427 : INFO : Word2Vec lifecycle event {'fname': '/tmp/gensim-model-jfcd632k', 'datetime': '2025-10-17T14:06:04.427864', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'loaded'}\n",
            "2025-10-17 14:06:04,429 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:04,429 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:04,430 : INFO : collected 13 word types from a corpus of 13 raw words and 1 sentences\n",
            "2025-10-17 14:06:04,430 : INFO : Updating model with new vocabulary\n",
            "2025-10-17 14:06:04,443 : INFO : Word2Vec lifecycle event {'msg': 'added 0 new unique words (0.00% of original 13) and increased the count of 0 pre-existing words (0.00% of original 13)', 'datetime': '2025-10-17T14:06:04.443204', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:04,444 : INFO : deleting the raw counts dictionary of 13 items\n",
            "2025-10-17 14:06:04,444 : INFO : sample=0.001 downsamples 0 most-common words\n",
            "2025-10-17 14:06:04,446 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 0 word corpus (0.0%% of prior 0)', 'datetime': '2025-10-17T14:06:04.446438', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:04,470 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
            "2025-10-17 14:06:04,471 : INFO : updating layer weights\n",
            "2025-10-17 14:06:04,472 : INFO : Word2Vec lifecycle event {'update': True, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:04.472301', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:04,473 : WARNING : Effective 'alpha' higher than previous training cycles\n",
            "2025-10-17 14:06:04,474 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:04.474130', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:04,476 : INFO : EPOCH 0: training on 13 raw words (7 effective words) took 0.0s, 24150 effective words/s\n",
            "2025-10-17 14:06:04,478 : INFO : EPOCH 1: training on 13 raw words (6 effective words) took 0.0s, 22639 effective words/s\n",
            "2025-10-17 14:06:04,480 : INFO : EPOCH 2: training on 13 raw words (4 effective words) took 0.0s, 12028 effective words/s\n",
            "2025-10-17 14:06:04,485 : INFO : EPOCH 3: training on 13 raw words (4 effective words) took 0.0s, 10203 effective words/s\n",
            "2025-10-17 14:06:04,488 : INFO : EPOCH 4: training on 13 raw words (5 effective words) took 0.0s, 12234 effective words/s\n",
            "2025-10-17 14:06:04,488 : INFO : Word2Vec lifecycle event {'msg': 'training on 65 raw words (26 effective words) took 0.0s, 1870 effective words/s', 'datetime': '2025-10-17T14:06:04.488800', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        }
      ],
      "source": [
        "model = gensim.models.Word2Vec.load(temporary_filepath)\n",
        "more_sentences = [\n",
        "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
        "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n",
        "]\n",
        "model.build_vocab(more_sentences, update=True)\n",
        "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# cleaning up temporary file\n",
        "import os\n",
        "os.remove(temporary_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may need to tweak the ``total_words`` parameter to ``train()``,\n",
        "depending on what learning rate decay you want to simulate.\n",
        "\n",
        "Note that it’s not possible to resume training with models generated by the C\n",
        "tool, ``KeyedVectors.load_word2vec_format()``. You can still use them for\n",
        "querying/similarity, but information vital for training (the vocab tree) is\n",
        "missing there.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Loss Computation\n",
        "-------------------------\n",
        "\n",
        "The parameter ``compute_loss`` can be used to toggle computation of loss\n",
        "while training the Word2Vec model. The computed loss is stored in the model\n",
        "attribute ``running_training_loss`` and can be retrieved using the function\n",
        "``get_latest_training_loss`` as follows :\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:04,956 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:04,957 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:05,019 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
            "2025-10-17 14:06:05,020 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:05,034 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 6981 unique words (100.00% of original 6981, drops 0)', 'datetime': '2025-10-17T14:06:05.034742', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:05,035 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 58152 word corpus (100.00% of original 58152, drops 0)', 'datetime': '2025-10-17T14:06:05.035325', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:05,062 : INFO : deleting the raw counts dictionary of 6981 items\n",
            "2025-10-17 14:06:05,063 : INFO : sample=0.001 downsamples 43 most-common words\n",
            "2025-10-17 14:06:05,063 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 45723.4541622429 word corpus (78.6%% of prior 58152)', 'datetime': '2025-10-17T14:06:05.063877', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:05,111 : INFO : estimated required memory for 6981 words and 100 dimensions: 9075300 bytes\n",
            "2025-10-17 14:06:05,112 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:05,115 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:05.115287', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:05,116 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 6981 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:05.116028', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:05,288 : INFO : EPOCH 0: training on 58152 raw words (45692 effective words) took 0.2s, 266862 effective words/s\n",
            "2025-10-17 14:06:05,455 : INFO : EPOCH 1: training on 58152 raw words (45692 effective words) took 0.2s, 277203 effective words/s\n",
            "2025-10-17 14:06:05,602 : INFO : EPOCH 2: training on 58152 raw words (45791 effective words) took 0.1s, 315694 effective words/s\n",
            "2025-10-17 14:06:05,744 : INFO : EPOCH 3: training on 58152 raw words (45751 effective words) took 0.1s, 325004 effective words/s\n",
            "2025-10-17 14:06:05,913 : INFO : EPOCH 4: training on 58152 raw words (45658 effective words) took 0.2s, 273504 effective words/s\n",
            "2025-10-17 14:06:05,913 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (228584 effective words) took 0.8s, 286830 effective words/s', 'datetime': '2025-10-17T14:06:05.913568', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:05,914 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=6981, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:05.914173', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1356487.375\n"
          ]
        }
      ],
      "source": [
        "# instantiating and training the Word2Vec model\n",
        "model_with_loss = gensim.models.Word2Vec(\n",
        "    sentences,\n",
        "    min_count=1,\n",
        "    compute_loss=True,\n",
        "    hs=0,\n",
        "    sg=1,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# getting the training loss value\n",
        "training_loss = model_with_loss.get_latest_training_loss()\n",
        "print(training_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks\n",
        "----------\n",
        "\n",
        "Let's run some benchmarks to see effect of the training loss computation code\n",
        "on training time.\n",
        "\n",
        "We'll use the following data for the benchmarks:\n",
        "\n",
        "#. Lee Background corpus: included in gensim's test data\n",
        "#. Text8 corpus.  To demonstrate the effect of corpus size, we'll look at the\n",
        "   first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "\n",
        "import gensim.models.word2vec\n",
        "import gensim.downloader as api\n",
        "import smart_open\n",
        "\n",
        "\n",
        "def head(path, size):\n",
        "    with smart_open.open(path) as fin:\n",
        "        return io.StringIO(fin.read(size))\n",
        "\n",
        "\n",
        "def generate_input_data():\n",
        "    lee_path = datapath('lee_background.cor')\n",
        "    ls = gensim.models.word2vec.LineSentence(lee_path)\n",
        "    ls.name = '25kB'\n",
        "    yield ls\n",
        "\n",
        "    text8_path = api.load('text8').fn\n",
        "    labels = ('1MB', '10MB', '50MB', '100MB')\n",
        "    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n",
        "    for l, s in zip(labels, sizes):\n",
        "        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n",
        "        ls.name = l\n",
        "        yield ls\n",
        "\n",
        "\n",
        "input_data = list(generate_input_data())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compare the training time taken for different combinations of input\n",
        "data and model training parameters like ``hs`` and ``sg``.\n",
        "\n",
        "For each combination, we repeat the test several times to obtain the mean and\n",
        "standard deviation of the test duration.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:15,283 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:15,284 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:15,304 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:15,305 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:15,312 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:15.312941', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,313 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:15.313687', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,322 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:15,323 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:15,324 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:15.324343', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,345 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:15,345 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:15,348 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:15.348636', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:15,349 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:15.349201', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:15,379 : INFO : EPOCH 0: training on 59890 raw words (32630 effective words) took 0.0s, 1125796 effective words/s\n",
            "2025-10-17 14:06:15,411 : INFO : EPOCH 1: training on 59890 raw words (32603 effective words) took 0.0s, 1294942 effective words/s\n",
            "2025-10-17 14:06:15,443 : INFO : EPOCH 2: training on 59890 raw words (32683 effective words) took 0.0s, 1089881 effective words/s\n",
            "2025-10-17 14:06:15,477 : INFO : EPOCH 3: training on 59890 raw words (32537 effective words) took 0.0s, 1005881 effective words/s\n",
            "2025-10-17 14:06:15,501 : INFO : EPOCH 4: training on 59890 raw words (32651 effective words) took 0.0s, 1437241 effective words/s\n",
            "2025-10-17 14:06:15,502 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163104 effective words) took 0.2s, 1068863 effective words/s', 'datetime': '2025-10-17T14:06:15.502565', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:15,503 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:15.503220', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:15,504 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:15,505 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:15,519 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:15,520 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:15,528 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:15.528129', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,528 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:15.528711', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,536 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:15,537 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:15,538 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:15.538080', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,549 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:15,549 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:15,551 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:15.551487', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:15,551 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:15.551897', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:15,582 : INFO : EPOCH 0: training on 59890 raw words (32668 effective words) took 0.0s, 1112294 effective words/s\n",
            "2025-10-17 14:06:15,608 : INFO : EPOCH 1: training on 59890 raw words (32652 effective words) took 0.0s, 1377987 effective words/s\n",
            "2025-10-17 14:06:15,640 : INFO : EPOCH 2: training on 59890 raw words (32583 effective words) took 0.0s, 1053230 effective words/s\n",
            "2025-10-17 14:06:15,669 : INFO : EPOCH 3: training on 59890 raw words (32610 effective words) took 0.0s, 1229708 effective words/s\n",
            "2025-10-17 14:06:15,700 : INFO : EPOCH 4: training on 59890 raw words (32678 effective words) took 0.0s, 1116278 effective words/s\n",
            "2025-10-17 14:06:15,700 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163191 effective words) took 0.1s, 1099177 effective words/s', 'datetime': '2025-10-17T14:06:15.700814', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:15,701 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:15.701412', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:15,702 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:15,702 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:15,717 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:15,717 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:15,723 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:15.723011', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,724 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:15.724415', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,732 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:15,733 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:15,733 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:15.733607', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,745 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:15,746 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:15,747 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:15.747679', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:15,748 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:15.748167', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:15,782 : INFO : EPOCH 0: training on 59890 raw words (32668 effective words) took 0.0s, 1144458 effective words/s\n",
            "2025-10-17 14:06:15,811 : INFO : EPOCH 1: training on 59890 raw words (32676 effective words) took 0.0s, 1210772 effective words/s\n",
            "2025-10-17 14:06:15,839 : INFO : EPOCH 2: training on 59890 raw words (32617 effective words) took 0.0s, 1235895 effective words/s\n",
            "2025-10-17 14:06:15,877 : INFO : EPOCH 3: training on 59890 raw words (32598 effective words) took 0.0s, 875787 effective words/s\n",
            "2025-10-17 14:06:15,908 : INFO : EPOCH 4: training on 59890 raw words (32565 effective words) took 0.0s, 1150143 effective words/s\n",
            "2025-10-17 14:06:15,908 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163124 effective words) took 0.2s, 1018728 effective words/s', 'datetime': '2025-10-17T14:06:15.908762', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:15,909 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:15.909256', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:15,910 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:15,911 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:15,926 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:15,927 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:15,932 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:15.932234', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,933 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:15.933083', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,943 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:15,944 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:15,945 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:15.945246', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:15,958 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:15,958 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:15,960 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:15.960166', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:15,960 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:15.960637', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:15,989 : INFO : EPOCH 0: training on 59890 raw words (32596 effective words) took 0.0s, 1205315 effective words/s\n",
            "2025-10-17 14:06:16,015 : INFO : EPOCH 1: training on 59890 raw words (32650 effective words) took 0.0s, 1296920 effective words/s\n",
            "2025-10-17 14:06:16,046 : INFO : EPOCH 2: training on 59890 raw words (32591 effective words) took 0.0s, 1111569 effective words/s\n",
            "2025-10-17 14:06:16,073 : INFO : EPOCH 3: training on 59890 raw words (32630 effective words) took 0.0s, 1292656 effective words/s\n",
            "2025-10-17 14:06:16,100 : INFO : EPOCH 4: training on 59890 raw words (32511 effective words) took 0.0s, 1258855 effective words/s\n",
            "2025-10-17 14:06:16,101 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162978 effective words) took 0.1s, 1161542 effective words/s', 'datetime': '2025-10-17T14:06:16.101341', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,101 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:16.101717', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:16,102 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:16,103 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #0: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.20878203709920248, 'train_time_std': 0.009089137925210865}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:16,117 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:16,117 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:16,124 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:16.124235', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,125 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:16.125764', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,133 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:16,134 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:16,135 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:16.135267', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,146 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:16,147 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:16,148 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:16.148785', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:16,149 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:16.149350', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,181 : INFO : EPOCH 0: training on 59890 raw words (32630 effective words) took 0.0s, 1089462 effective words/s\n",
            "2025-10-17 14:06:16,211 : INFO : EPOCH 1: training on 59890 raw words (32659 effective words) took 0.0s, 1123824 effective words/s\n",
            "2025-10-17 14:06:16,244 : INFO : EPOCH 2: training on 59890 raw words (32557 effective words) took 0.0s, 1044287 effective words/s\n",
            "2025-10-17 14:06:16,278 : INFO : EPOCH 3: training on 59890 raw words (32724 effective words) took 0.0s, 1036460 effective words/s\n",
            "2025-10-17 14:06:16,308 : INFO : EPOCH 4: training on 59890 raw words (32619 effective words) took 0.0s, 1133194 effective words/s\n",
            "2025-10-17 14:06:16,309 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163189 effective words) took 0.2s, 1021309 effective words/s', 'datetime': '2025-10-17T14:06:16.309526', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,309 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:16.309929', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:16,310 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:16,311 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:16,325 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:16,325 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:16,330 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:16.330439', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,330 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:16.330974', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,338 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:16,339 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:16,340 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:16.340186', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,352 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:16,352 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:16,354 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:16.354169', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:16,354 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:16.354795', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,379 : INFO : EPOCH 0: training on 59890 raw words (32648 effective words) took 0.0s, 1416357 effective words/s\n",
            "2025-10-17 14:06:16,417 : INFO : EPOCH 1: training on 59890 raw words (32554 effective words) took 0.0s, 890286 effective words/s\n",
            "2025-10-17 14:06:16,443 : INFO : EPOCH 2: training on 59890 raw words (32607 effective words) took 0.0s, 1305325 effective words/s\n",
            "2025-10-17 14:06:16,469 : INFO : EPOCH 3: training on 59890 raw words (32637 effective words) took 0.0s, 1389131 effective words/s\n",
            "2025-10-17 14:06:16,494 : INFO : EPOCH 4: training on 59890 raw words (32664 effective words) took 0.0s, 1349491 effective words/s\n",
            "2025-10-17 14:06:16,495 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163110 effective words) took 0.1s, 1165023 effective words/s', 'datetime': '2025-10-17T14:06:16.495412', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,496 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:16.496010', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:16,496 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:16,497 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:16,510 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:16,511 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:16,517 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:16.516993', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,517 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:16.517484', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,525 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:16,526 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:16,527 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:16.527082', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,527 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:16,570 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:16,581 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:16,582 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:16,583 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:16.583701', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:16,584 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:16,584 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:16.584603', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,633 : INFO : EPOCH 0: training on 59890 raw words (32517 effective words) took 0.0s, 683604 effective words/s\n",
            "2025-10-17 14:06:16,677 : INFO : EPOCH 1: training on 59890 raw words (32567 effective words) took 0.0s, 762995 effective words/s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #1: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.19549846649169922, 'train_time_std': 0.00937267917370574}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:16,726 : INFO : EPOCH 2: training on 59890 raw words (32654 effective words) took 0.0s, 696542 effective words/s\n",
            "2025-10-17 14:06:16,772 : INFO : EPOCH 3: training on 59890 raw words (32582 effective words) took 0.0s, 742094 effective words/s\n",
            "2025-10-17 14:06:16,812 : INFO : EPOCH 4: training on 59890 raw words (32561 effective words) took 0.0s, 831652 effective words/s\n",
            "2025-10-17 14:06:16,813 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162881 effective words) took 0.2s, 712576 effective words/s', 'datetime': '2025-10-17T14:06:16.813515', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,813 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:16.813933', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:16,814 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:16,815 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:16,833 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:16,834 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:16,839 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:16.839558', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,840 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:16.840061', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,847 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:16,848 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:16,849 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:16.849178', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:16,850 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:16,893 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:16,904 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:16,905 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:16,906 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:16.906293', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:16,906 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:16,906 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:16.906973', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:16,957 : INFO : EPOCH 0: training on 59890 raw words (32648 effective words) took 0.0s, 662960 effective words/s\n",
            "2025-10-17 14:06:17,018 : INFO : EPOCH 1: training on 59890 raw words (32582 effective words) took 0.1s, 545944 effective words/s\n",
            "2025-10-17 14:06:17,064 : INFO : EPOCH 2: training on 59890 raw words (32595 effective words) took 0.0s, 730837 effective words/s\n",
            "2025-10-17 14:06:17,110 : INFO : EPOCH 3: training on 59890 raw words (32644 effective words) took 0.0s, 737352 effective words/s\n",
            "2025-10-17 14:06:17,161 : INFO : EPOCH 4: training on 59890 raw words (32580 effective words) took 0.0s, 667570 effective words/s\n",
            "2025-10-17 14:06:17,162 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163049 effective words) took 0.3s, 639711 effective words/s', 'datetime': '2025-10-17T14:06:17.162164', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:17,162 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:17.162885', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:17,164 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:17,164 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:17,177 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:17,178 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:17,184 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:17.184005', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,184 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:17.184662', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,192 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:17,192 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:17,193 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:17.193454', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,194 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:17,237 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:17,250 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:17,250 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:17,252 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:17.252413', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:17,252 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:17,253 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:17.253385', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:17,306 : INFO : EPOCH 0: training on 59890 raw words (32668 effective words) took 0.1s, 641042 effective words/s\n",
            "2025-10-17 14:06:17,361 : INFO : EPOCH 1: training on 59890 raw words (32664 effective words) took 0.1s, 613247 effective words/s\n",
            "2025-10-17 14:06:17,407 : INFO : EPOCH 2: training on 59890 raw words (32605 effective words) took 0.0s, 735534 effective words/s\n",
            "2025-10-17 14:06:17,451 : INFO : EPOCH 3: training on 59890 raw words (32579 effective words) took 0.0s, 761451 effective words/s\n",
            "2025-10-17 14:06:17,499 : INFO : EPOCH 4: training on 59890 raw words (32529 effective words) took 0.0s, 701363 effective words/s\n",
            "2025-10-17 14:06:17,499 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163045 effective words) took 0.2s, 663281 effective words/s', 'datetime': '2025-10-17T14:06:17.499685', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:17,500 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:17.500137', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:17,501 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:17,502 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:17,520 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:17,521 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:17,528 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:17.528697', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,529 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:17.529462', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,549 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:17,550 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:17,550 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:17.550651', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,552 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:17,653 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:17,670 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:17,671 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:17,673 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:17.673897', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:17,674 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:17,675 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:17.674999', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #2: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 0.3348703384399414, 'train_time_std': 0.013008925941800175}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:17,735 : INFO : EPOCH 0: training on 59890 raw words (32543 effective words) took 0.1s, 550342 effective words/s\n",
            "2025-10-17 14:06:17,789 : INFO : EPOCH 1: training on 59890 raw words (32552 effective words) took 0.1s, 626928 effective words/s\n",
            "2025-10-17 14:06:17,840 : INFO : EPOCH 2: training on 59890 raw words (32630 effective words) took 0.0s, 660436 effective words/s\n",
            "2025-10-17 14:06:17,884 : INFO : EPOCH 3: training on 59890 raw words (32560 effective words) took 0.0s, 772602 effective words/s\n",
            "2025-10-17 14:06:17,931 : INFO : EPOCH 4: training on 59890 raw words (32583 effective words) took 0.0s, 714649 effective words/s\n",
            "2025-10-17 14:06:17,931 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162868 effective words) took 0.3s, 635673 effective words/s', 'datetime': '2025-10-17T14:06:17.931786', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:17,932 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:17.932181', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:17,933 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:17,934 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:17,947 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:17,948 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:17,953 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:17.953341', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,953 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:17.953904', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,961 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:17,962 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:17,962 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:17.962597', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:17,963 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:18,007 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:18,035 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:18,035 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:18,037 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:18.037896', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:18,038 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:18,038 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:18.038906', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:18,106 : INFO : EPOCH 0: training on 59890 raw words (32668 effective words) took 0.1s, 499466 effective words/s\n",
            "2025-10-17 14:06:18,156 : INFO : EPOCH 1: training on 59890 raw words (32566 effective words) took 0.0s, 675111 effective words/s\n",
            "2025-10-17 14:06:18,207 : INFO : EPOCH 2: training on 59890 raw words (32623 effective words) took 0.0s, 659007 effective words/s\n",
            "2025-10-17 14:06:18,250 : INFO : EPOCH 3: training on 59890 raw words (32636 effective words) took 0.0s, 792991 effective words/s\n",
            "2025-10-17 14:06:18,297 : INFO : EPOCH 4: training on 59890 raw words (32648 effective words) took 0.0s, 713410 effective words/s\n",
            "2025-10-17 14:06:18,298 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163141 effective words) took 0.3s, 629902 effective words/s', 'datetime': '2025-10-17T14:06:18.298322', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:18,299 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:18.299123', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:18,301 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:18,302 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:18,324 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:18,325 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:18,335 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:18.335358', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:18,336 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:18.336560', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:18,353 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:18,355 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:18,356 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:18.356645', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:18,359 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:18,410 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:18,423 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:18,423 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:18,425 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:18.425029', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:18,425 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:18,426 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:18.426200', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:18,479 : INFO : EPOCH 0: training on 59890 raw words (32648 effective words) took 0.1s, 636882 effective words/s\n",
            "2025-10-17 14:06:18,529 : INFO : EPOCH 1: training on 59890 raw words (32688 effective words) took 0.0s, 678456 effective words/s\n",
            "2025-10-17 14:06:18,577 : INFO : EPOCH 2: training on 59890 raw words (32692 effective words) took 0.0s, 706209 effective words/s\n",
            "2025-10-17 14:06:18,622 : INFO : EPOCH 3: training on 59890 raw words (32556 effective words) took 0.0s, 756704 effective words/s\n",
            "2025-10-17 14:06:18,664 : INFO : EPOCH 4: training on 59890 raw words (32592 effective words) took 0.0s, 802470 effective words/s\n",
            "2025-10-17 14:06:18,665 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163176 effective words) took 0.2s, 685686 effective words/s', 'datetime': '2025-10-17T14:06:18.665183', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:18,665 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:18.665924', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:18,667 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:18,668 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:18,692 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:18,693 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:18,703 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:18.703359', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:18,703 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:18.703982', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:18,714 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:18,715 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:18,716 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:18.716644', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:18,727 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:18,728 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:18,730 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:18.730899', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:18,731 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:18.731442', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:18,799 : INFO : EPOCH 0: training on 59890 raw words (32668 effective words) took 0.1s, 493737 effective words/s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #3: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 0.3885192076365153, 'train_time_std': 0.030623544473732296}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:18,871 : INFO : EPOCH 1: training on 59890 raw words (32676 effective words) took 0.1s, 461568 effective words/s\n",
            "2025-10-17 14:06:18,938 : INFO : EPOCH 2: training on 59890 raw words (32621 effective words) took 0.1s, 501928 effective words/s\n",
            "2025-10-17 14:06:19,013 : INFO : EPOCH 3: training on 59890 raw words (32698 effective words) took 0.1s, 447644 effective words/s\n",
            "2025-10-17 14:06:19,088 : INFO : EPOCH 4: training on 59890 raw words (32561 effective words) took 0.1s, 447913 effective words/s\n",
            "2025-10-17 14:06:19,089 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163224 effective words) took 0.4s, 457245 effective words/s', 'datetime': '2025-10-17T14:06:19.089022', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:19,089 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:19.089760', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:19,091 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:19,093 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:19,105 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:19,106 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:19,119 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:19.119845', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,120 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:19.120389', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,127 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:19,128 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:19,128 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:19.128961', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,139 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:19,140 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:19,141 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:19.141774', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:19,142 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:19.142215', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:19,208 : INFO : EPOCH 0: training on 59890 raw words (32543 effective words) took 0.1s, 502464 effective words/s\n",
            "2025-10-17 14:06:19,275 : INFO : EPOCH 1: training on 59890 raw words (32692 effective words) took 0.1s, 498578 effective words/s\n",
            "2025-10-17 14:06:19,345 : INFO : EPOCH 2: training on 59890 raw words (32638 effective words) took 0.1s, 476625 effective words/s\n",
            "2025-10-17 14:06:19,415 : INFO : EPOCH 3: training on 59890 raw words (32678 effective words) took 0.1s, 478399 effective words/s\n",
            "2025-10-17 14:06:19,486 : INFO : EPOCH 4: training on 59890 raw words (32469 effective words) took 0.1s, 471678 effective words/s\n",
            "2025-10-17 14:06:19,487 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163020 effective words) took 0.3s, 473096 effective words/s', 'datetime': '2025-10-17T14:06:19.487331', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:19,488 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:19.488263', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:19,489 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:19,490 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:19,506 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:19,506 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:19,514 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:19.514907', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,515 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:19.515661', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,525 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:19,525 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:19,526 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:19.526287', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,539 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:19,540 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:19,542 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:19.542903', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:19,543 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:19.543402', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:19,622 : INFO : EPOCH 0: training on 59890 raw words (32543 effective words) took 0.1s, 421386 effective words/s\n",
            "2025-10-17 14:06:19,689 : INFO : EPOCH 1: training on 59890 raw words (32692 effective words) took 0.1s, 494499 effective words/s\n",
            "2025-10-17 14:06:19,756 : INFO : EPOCH 2: training on 59890 raw words (32638 effective words) took 0.1s, 497992 effective words/s\n",
            "2025-10-17 14:06:19,819 : INFO : EPOCH 3: training on 59890 raw words (32678 effective words) took 0.1s, 534530 effective words/s\n",
            "2025-10-17 14:06:19,888 : INFO : EPOCH 4: training on 59890 raw words (32469 effective words) took 0.1s, 481769 effective words/s\n",
            "2025-10-17 14:06:19,889 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163020 effective words) took 0.3s, 471575 effective words/s', 'datetime': '2025-10-17T14:06:19.889512', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:19,890 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:19.890308', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:19,891 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:19,893 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:19,909 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:19,910 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:19,915 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:19.915602', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,916 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:19.916086', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,924 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:19,925 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:19,925 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:19.925764', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:19,938 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:19,938 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:19,939 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:19.939650', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:19,940 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:19.940159', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:20,007 : INFO : EPOCH 0: training on 59890 raw words (32596 effective words) took 0.1s, 497531 effective words/s\n",
            "2025-10-17 14:06:20,080 : INFO : EPOCH 1: training on 59890 raw words (32652 effective words) took 0.1s, 454224 effective words/s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #4: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 0.4079549312591553, 'train_time_std': 0.011481673547809684}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:20,148 : INFO : EPOCH 2: training on 59890 raw words (32591 effective words) took 0.1s, 489712 effective words/s\n",
            "2025-10-17 14:06:20,222 : INFO : EPOCH 3: training on 59890 raw words (32763 effective words) took 0.1s, 455409 effective words/s\n",
            "2025-10-17 14:06:20,292 : INFO : EPOCH 4: training on 59890 raw words (32621 effective words) took 0.1s, 481195 effective words/s\n",
            "2025-10-17 14:06:20,292 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163223 effective words) took 0.4s, 463533 effective words/s', 'datetime': '2025-10-17T14:06:20.292749', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:20,293 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:20.293610', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:20,294 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:20,295 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:20,323 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:20,324 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:20,329 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:20.329646', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:20,330 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:20.330160', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:20,337 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:20,338 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:20,339 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:20.339001', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:20,352 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:20,353 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:20,354 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:20.354284', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:20,354 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:20.354708', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:20,429 : INFO : EPOCH 0: training on 59890 raw words (32517 effective words) took 0.1s, 446559 effective words/s\n",
            "2025-10-17 14:06:20,512 : INFO : EPOCH 1: training on 59890 raw words (32679 effective words) took 0.1s, 401383 effective words/s\n",
            "2025-10-17 14:06:20,598 : INFO : EPOCH 2: training on 59890 raw words (32527 effective words) took 0.1s, 385593 effective words/s\n",
            "2025-10-17 14:06:20,660 : INFO : EPOCH 3: training on 59890 raw words (32576 effective words) took 0.1s, 537566 effective words/s\n",
            "2025-10-17 14:06:20,730 : INFO : EPOCH 4: training on 59890 raw words (32581 effective words) took 0.1s, 476323 effective words/s\n",
            "2025-10-17 14:06:20,731 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162880 effective words) took 0.4s, 433322 effective words/s', 'datetime': '2025-10-17T14:06:20.731061', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:20,731 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:20.731602', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:20,732 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:20,733 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:20,745 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:20,746 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:20,751 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:20.751260', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:20,752 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:20.752277', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:20,760 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:20,760 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:20,761 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:20.761389', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:20,776 : INFO : estimated required memory for 1762 words and 100 dimensions: 2290600 bytes\n",
            "2025-10-17 14:06:20,777 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:20,778 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:20.778132', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:20,778 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:20.778522', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:20,845 : INFO : EPOCH 0: training on 59890 raw words (32596 effective words) took 0.1s, 498629 effective words/s\n",
            "2025-10-17 14:06:20,915 : INFO : EPOCH 1: training on 59890 raw words (32570 effective words) took 0.1s, 475448 effective words/s\n",
            "2025-10-17 14:06:20,980 : INFO : EPOCH 2: training on 59890 raw words (32503 effective words) took 0.1s, 514908 effective words/s\n",
            "2025-10-17 14:06:21,048 : INFO : EPOCH 3: training on 59890 raw words (32618 effective words) took 0.1s, 487573 effective words/s\n",
            "2025-10-17 14:06:21,118 : INFO : EPOCH 4: training on 59890 raw words (32586 effective words) took 0.1s, 475765 effective words/s\n",
            "2025-10-17 14:06:21,119 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162873 effective words) took 0.3s, 478782 effective words/s', 'datetime': '2025-10-17T14:06:21.119100', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:21,119 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:21.119735', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:21,120 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:21,121 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:21,135 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:21,136 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:21,141 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:21.141289', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:21,141 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:21.141821', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:21,149 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:21,150 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:21,150 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:21.150857', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:21,151 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:21,191 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:21,202 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:21,202 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:21,204 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:21.204342', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:21,204 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:21,205 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:21.205260', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #5: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 0.40966296195983887, 'train_time_std': 0.020924562955893372}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:21,341 : INFO : EPOCH 0: training on 59890 raw words (32668 effective words) took 0.1s, 242497 effective words/s\n",
            "2025-10-17 14:06:21,487 : INFO : EPOCH 1: training on 59890 raw words (32566 effective words) took 0.1s, 226058 effective words/s\n",
            "2025-10-17 14:06:21,619 : INFO : EPOCH 2: training on 59890 raw words (32623 effective words) took 0.1s, 249334 effective words/s\n",
            "2025-10-17 14:06:21,760 : INFO : EPOCH 3: training on 59890 raw words (32636 effective words) took 0.1s, 235311 effective words/s\n",
            "2025-10-17 14:06:21,896 : INFO : EPOCH 4: training on 59890 raw words (32662 effective words) took 0.1s, 242129 effective words/s\n",
            "2025-10-17 14:06:21,897 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163155 effective words) took 0.7s, 235994 effective words/s', 'datetime': '2025-10-17T14:06:21.897238', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:21,897 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:21.897860', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:21,898 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:21,899 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:21,915 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:21,916 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:21,921 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:21.921642', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:21,922 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:21.922562', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:21,930 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:21,931 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:21,932 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:21.932021', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:21,933 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:21,980 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:21,994 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:21,994 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:21,996 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:21.996677', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:21,997 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:21,997 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:21.997864', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:22,121 : INFO : EPOCH 0: training on 59890 raw words (32543 effective words) took 0.1s, 267778 effective words/s\n",
            "2025-10-17 14:06:22,262 : INFO : EPOCH 1: training on 59890 raw words (32616 effective words) took 0.1s, 234420 effective words/s\n",
            "2025-10-17 14:06:22,408 : INFO : EPOCH 2: training on 59890 raw words (32618 effective words) took 0.1s, 225340 effective words/s\n",
            "2025-10-17 14:06:22,531 : INFO : EPOCH 3: training on 59890 raw words (32482 effective words) took 0.1s, 266859 effective words/s\n",
            "2025-10-17 14:06:22,677 : INFO : EPOCH 4: training on 59890 raw words (32664 effective words) took 0.1s, 227591 effective words/s\n",
            "2025-10-17 14:06:22,677 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162923 effective words) took 0.7s, 239819 effective words/s', 'datetime': '2025-10-17T14:06:22.677818', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:22,678 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:22.678347', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:22,679 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:22,680 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:22,693 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:22,693 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:22,698 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:22.698748', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:22,699 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:22.699210', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:22,706 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:22,707 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:22,708 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:22.708665', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:22,709 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:22,747 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:22,758 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:22,759 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:22,760 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:22.760691', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:22,761 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:22,761 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:22.761420', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:22,912 : INFO : EPOCH 0: training on 59890 raw words (32543 effective words) took 0.1s, 217441 effective words/s\n",
            "2025-10-17 14:06:23,056 : INFO : EPOCH 1: training on 59890 raw words (32552 effective words) took 0.1s, 228587 effective words/s\n",
            "2025-10-17 14:06:23,232 : INFO : EPOCH 2: training on 59890 raw words (32603 effective words) took 0.2s, 186307 effective words/s\n",
            "2025-10-17 14:06:23,406 : INFO : EPOCH 3: training on 59890 raw words (32644 effective words) took 0.2s, 190447 effective words/s\n",
            "2025-10-17 14:06:23,542 : INFO : EPOCH 4: training on 59890 raw words (32558 effective words) took 0.1s, 242845 effective words/s\n",
            "2025-10-17 14:06:23,542 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162900 effective words) took 0.8s, 208530 effective words/s', 'datetime': '2025-10-17T14:06:23.542970', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:23,543 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:23.543556', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:23,545 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:23,546 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:23,559 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:23,560 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:23,565 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:23.565632', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:23,566 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:23.566289', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:23,574 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:23,575 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:23,575 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:23.575616', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:23,577 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:23,615 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:23,625 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:23,626 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:23,627 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:23.627605', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:23,627 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:23,628 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:23.628342', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #6: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 0.8081325689951578, 'train_time_std': 0.040626869392516037}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:23,773 : INFO : EPOCH 0: training on 59890 raw words (32668 effective words) took 0.1s, 227935 effective words/s\n",
            "2025-10-17 14:06:23,920 : INFO : EPOCH 1: training on 59890 raw words (32670 effective words) took 0.1s, 223091 effective words/s\n",
            "2025-10-17 14:06:24,052 : INFO : EPOCH 2: training on 59890 raw words (32552 effective words) took 0.1s, 250414 effective words/s\n",
            "2025-10-17 14:06:24,191 : INFO : EPOCH 3: training on 59890 raw words (32664 effective words) took 0.1s, 238018 effective words/s\n",
            "2025-10-17 14:06:24,331 : INFO : EPOCH 4: training on 59890 raw words (32644 effective words) took 0.1s, 236484 effective words/s\n",
            "2025-10-17 14:06:24,332 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163198 effective words) took 0.7s, 231978 effective words/s', 'datetime': '2025-10-17T14:06:24.332183', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:24,332 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:24.332657', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:24,334 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:24,335 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:24,348 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:24,348 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:24,357 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:24.357005', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:24,357 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:24.357547', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:24,365 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:24,366 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:24,366 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:24.366488', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:24,368 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:24,406 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:24,416 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:24,417 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:24,419 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:24.419144', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:24,419 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:24,420 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:24.420059', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:24,553 : INFO : EPOCH 0: training on 59890 raw words (32517 effective words) took 0.1s, 246152 effective words/s\n",
            "2025-10-17 14:06:24,712 : INFO : EPOCH 1: training on 59890 raw words (32650 effective words) took 0.2s, 206885 effective words/s\n",
            "2025-10-17 14:06:24,851 : INFO : EPOCH 2: training on 59890 raw words (32663 effective words) took 0.1s, 237602 effective words/s\n",
            "2025-10-17 14:06:24,984 : INFO : EPOCH 3: training on 59890 raw words (32547 effective words) took 0.1s, 249211 effective words/s\n",
            "2025-10-17 14:06:25,118 : INFO : EPOCH 4: training on 59890 raw words (32659 effective words) took 0.1s, 247001 effective words/s\n",
            "2025-10-17 14:06:25,118 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (163036 effective words) took 0.7s, 233519 effective words/s', 'datetime': '2025-10-17T14:06:25.118601', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:25,119 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:25.119159', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:25,120 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:25,120 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:25,136 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
            "2025-10-17 14:06:25,138 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:25,145 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2025-10-17T14:06:25.145338', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:25,146 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2025-10-17T14:06:25.146030', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:25,154 : INFO : deleting the raw counts dictionary of 10781 items\n",
            "2025-10-17 14:06:25,155 : INFO : sample=0.001 downsamples 45 most-common words\n",
            "2025-10-17 14:06:25,155 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2025-10-17T14:06:25.155689', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:25,158 : INFO : constructing a huffman tree from 1762 words\n",
            "2025-10-17 14:06:25,198 : INFO : built huffman tree with maximum node depth 13\n",
            "2025-10-17 14:06:25,210 : INFO : estimated required memory for 1762 words and 100 dimensions: 3347800 bytes\n",
            "2025-10-17 14:06:25,211 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:25,212 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:25.212600', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:25,212 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:25,213 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1762 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:25.213332', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:25,363 : INFO : EPOCH 0: training on 59890 raw words (32543 effective words) took 0.1s, 219368 effective words/s\n",
            "2025-10-17 14:06:25,513 : INFO : EPOCH 1: training on 59890 raw words (32552 effective words) took 0.1s, 218763 effective words/s\n",
            "2025-10-17 14:06:25,682 : INFO : EPOCH 2: training on 59890 raw words (32517 effective words) took 0.2s, 194703 effective words/s\n",
            "2025-10-17 14:06:25,817 : INFO : EPOCH 3: training on 59890 raw words (32654 effective words) took 0.1s, 245910 effective words/s\n",
            "2025-10-17 14:06:25,958 : INFO : EPOCH 4: training on 59890 raw words (32715 effective words) took 0.1s, 233036 effective words/s\n",
            "2025-10-17 14:06:25,959 : INFO : Word2Vec lifecycle event {'msg': 'training on 299450 raw words (162981 effective words) took 0.7s, 218523 effective words/s', 'datetime': '2025-10-17T14:06:25.959459', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:25,960 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1762, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:25.960236', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:25,962 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:25,975 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:26,009 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:26,010 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:26,026 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:26.026744', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:26,027 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:26.027441', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:26,046 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:26,047 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:26,048 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:26.048642', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:26,078 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:26,079 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:26,083 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:26.083532', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:26,084 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:26.084322', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:26,161 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.1s, 1678906 effective words/s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #7: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 0.8055164813995361, 'train_time_std': 0.0255577470170492}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:26,246 : INFO : EPOCH 1: training on 175599 raw words (110105 effective words) took 0.1s, 1464169 effective words/s\n",
            "2025-10-17 14:06:26,328 : INFO : EPOCH 2: training on 175599 raw words (110141 effective words) took 0.1s, 1601596 effective words/s\n",
            "2025-10-17 14:06:26,411 : INFO : EPOCH 3: training on 175599 raw words (110408 effective words) took 0.1s, 1497611 effective words/s\n",
            "2025-10-17 14:06:26,494 : INFO : EPOCH 4: training on 175599 raw words (110320 effective words) took 0.1s, 1512013 effective words/s\n",
            "2025-10-17 14:06:26,494 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550968 effective words) took 0.4s, 1344397 effective words/s', 'datetime': '2025-10-17T14:06:26.494865', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:26,495 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:26.495317', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:26,496 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:26,506 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:26,541 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:26,542 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:26,553 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:26.553278', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:26,553 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:26.553813', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:26,569 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:26,570 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:26,571 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:26.571049', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:26,595 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:26,595 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:26,597 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:26.597593', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:26,598 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:26.597998', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:26,672 : INFO : EPOCH 0: training on 175599 raw words (110317 effective words) took 0.1s, 1722805 effective words/s\n",
            "2025-10-17 14:06:26,745 : INFO : EPOCH 1: training on 175599 raw words (110047 effective words) took 0.1s, 1737101 effective words/s\n",
            "2025-10-17 14:06:26,853 : INFO : EPOCH 2: training on 175599 raw words (110163 effective words) took 0.1s, 1155167 effective words/s\n",
            "2025-10-17 14:06:26,940 : INFO : EPOCH 3: training on 175599 raw words (110474 effective words) took 0.1s, 1462773 effective words/s\n",
            "2025-10-17 14:06:27,019 : INFO : EPOCH 4: training on 175599 raw words (110284 effective words) took 0.1s, 1601076 effective words/s\n",
            "2025-10-17 14:06:27,019 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551285 effective words) took 0.4s, 1307738 effective words/s', 'datetime': '2025-10-17T14:06:27.019932', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:27,020 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:27.020654', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:27,022 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:27,032 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:27,067 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:27,068 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:27,082 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:27.082924', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:27,083 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:27.083550', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:27,100 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:27,102 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:27,103 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:27.103625', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:27,140 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:27,141 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:27,146 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:27.146178', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:27,146 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:27.146828', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:27,279 : INFO : EPOCH 0: training on 175599 raw words (110387 effective words) took 0.1s, 932973 effective words/s\n",
            "2025-10-17 14:06:27,395 : INFO : EPOCH 1: training on 175599 raw words (110094 effective words) took 0.1s, 1143114 effective words/s\n",
            "2025-10-17 14:06:27,478 : INFO : EPOCH 2: training on 175599 raw words (110150 effective words) took 0.1s, 1500149 effective words/s\n",
            "2025-10-17 14:06:27,553 : INFO : EPOCH 3: training on 175599 raw words (110112 effective words) took 0.1s, 1713125 effective words/s\n",
            "2025-10-17 14:06:27,631 : INFO : EPOCH 4: training on 175599 raw words (110171 effective words) took 0.1s, 1690557 effective words/s\n",
            "2025-10-17 14:06:27,631 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550914 effective words) took 0.5s, 1137257 effective words/s', 'datetime': '2025-10-17T14:06:27.631877', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:27,632 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:27.632530', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:27,633 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:27,643 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:27,678 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:27,679 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:27,692 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:27.692426', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:27,693 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:27.693075', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:27,713 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:27,713 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:27,714 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:27.714342', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:27,745 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:27,745 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:27,748 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:27.748934', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:27,749 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:27.749428', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #8: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.5571568012237549, 'train_time_std': 0.038710054112509454}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:27,838 : INFO : EPOCH 0: training on 175599 raw words (110135 effective words) took 0.1s, 1409103 effective words/s\n",
            "2025-10-17 14:06:27,923 : INFO : EPOCH 1: training on 175599 raw words (110411 effective words) took 0.1s, 1534562 effective words/s\n",
            "2025-10-17 14:06:28,000 : INFO : EPOCH 2: training on 175599 raw words (110207 effective words) took 0.1s, 1669336 effective words/s\n",
            "2025-10-17 14:06:28,072 : INFO : EPOCH 3: training on 175599 raw words (110104 effective words) took 0.1s, 1771107 effective words/s\n",
            "2025-10-17 14:06:28,147 : INFO : EPOCH 4: training on 175599 raw words (110399 effective words) took 0.1s, 1763645 effective words/s\n",
            "2025-10-17 14:06:28,148 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551256 effective words) took 0.4s, 1382395 effective words/s', 'datetime': '2025-10-17T14:06:28.148603', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:28,149 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:28.149091', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:28,150 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:28,160 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:28,193 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:28,193 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:28,205 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:28.205570', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:28,206 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:28.206114', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:28,222 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:28,223 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:28,224 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:28.224033', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:28,250 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:28,250 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:28,252 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:28.252727', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:28,253 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:28.253278', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:28,330 : INFO : EPOCH 0: training on 175599 raw words (110523 effective words) took 0.1s, 1668468 effective words/s\n",
            "2025-10-17 14:06:28,409 : INFO : EPOCH 1: training on 175599 raw words (110134 effective words) took 0.1s, 1593113 effective words/s\n",
            "2025-10-17 14:06:28,484 : INFO : EPOCH 2: training on 175599 raw words (110250 effective words) took 0.1s, 1709512 effective words/s\n",
            "2025-10-17 14:06:28,558 : INFO : EPOCH 3: training on 175599 raw words (110081 effective words) took 0.1s, 1721964 effective words/s\n",
            "2025-10-17 14:06:28,648 : INFO : EPOCH 4: training on 175599 raw words (110280 effective words) took 0.1s, 1460618 effective words/s\n",
            "2025-10-17 14:06:28,648 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551268 effective words) took 0.4s, 1394468 effective words/s', 'datetime': '2025-10-17T14:06:28.648953', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:28,649 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:28.649542', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:28,651 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:28,661 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:28,690 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:28,690 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:28,701 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:28.701535', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:28,702 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:28.702069', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:28,717 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:28,718 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:28,718 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:28.718943', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:28,744 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:28,745 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:28,747 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:28.747342', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:28,747 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:28.747884', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:28,832 : INFO : EPOCH 0: training on 175599 raw words (110284 effective words) took 0.1s, 1482368 effective words/s\n",
            "2025-10-17 14:06:28,947 : INFO : EPOCH 1: training on 175599 raw words (110268 effective words) took 0.1s, 1084279 effective words/s\n",
            "2025-10-17 14:06:29,064 : INFO : EPOCH 2: training on 175599 raw words (110164 effective words) took 0.1s, 1059392 effective words/s\n",
            "2025-10-17 14:06:29,145 : INFO : EPOCH 3: training on 175599 raw words (110146 effective words) took 0.1s, 1556173 effective words/s\n",
            "2025-10-17 14:06:29,223 : INFO : EPOCH 4: training on 175599 raw words (110257 effective words) took 0.1s, 1674749 effective words/s\n",
            "2025-10-17 14:06:29,223 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551119 effective words) took 0.5s, 1159301 effective words/s', 'datetime': '2025-10-17T14:06:29.223647', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:29,224 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:29.224057', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:29,225 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:29,235 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:29,265 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:29,265 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:29,276 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:29.276474', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:29,277 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:29.277131', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:29,293 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:29,294 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:29,295 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:29.295409', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:29,298 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:29,388 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:29,412 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:29,413 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:29,414 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:29.414915', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:29,415 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:29,415 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:29.415662', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #9: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.5303853352864584, 'train_time_std': 0.03153330569643895}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:29,564 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.1s, 800108 effective words/s\n",
            "2025-10-17 14:06:29,722 : INFO : EPOCH 1: training on 175599 raw words (110240 effective words) took 0.1s, 745350 effective words/s\n",
            "2025-10-17 14:06:29,862 : INFO : EPOCH 2: training on 175599 raw words (110006 effective words) took 0.1s, 848273 effective words/s\n",
            "2025-10-17 14:06:30,004 : INFO : EPOCH 3: training on 175599 raw words (110409 effective words) took 0.1s, 834441 effective words/s\n",
            "2025-10-17 14:06:30,152 : INFO : EPOCH 4: training on 175599 raw words (110151 effective words) took 0.1s, 801047 effective words/s\n",
            "2025-10-17 14:06:30,153 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550800 effective words) took 0.7s, 746902 effective words/s', 'datetime': '2025-10-17T14:06:30.153428', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:30,153 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:30.153966', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:30,155 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:30,164 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:30,193 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:30,193 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:30,205 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:30.205175', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:30,205 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:30.205685', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:30,223 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:30,224 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:30,224 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:30.224667', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:30,226 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:30,312 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:30,336 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:30,337 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:30,339 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:30.339104', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:30,339 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:30,339 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:30.339951', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:30,472 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.1s, 900981 effective words/s\n",
            "2025-10-17 14:06:30,613 : INFO : EPOCH 1: training on 175599 raw words (110178 effective words) took 0.1s, 851613 effective words/s\n",
            "2025-10-17 14:06:30,778 : INFO : EPOCH 2: training on 175599 raw words (110237 effective words) took 0.2s, 710083 effective words/s\n",
            "2025-10-17 14:06:30,923 : INFO : EPOCH 3: training on 175599 raw words (110135 effective words) took 0.1s, 826500 effective words/s\n",
            "2025-10-17 14:06:31,074 : INFO : EPOCH 4: training on 175599 raw words (110022 effective words) took 0.1s, 784201 effective words/s\n",
            "2025-10-17 14:06:31,074 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550566 effective words) took 0.7s, 749715 effective words/s', 'datetime': '2025-10-17T14:06:31.074929', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:31,075 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:31.075430', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:31,077 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:31,086 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:31,115 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:31,116 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:31,129 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:31.129528', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:31,130 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:31.130160', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:31,147 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:31,149 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:31,149 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:31.149618', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:31,151 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:31,248 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:31,274 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:31,274 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:31,277 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:31.277175', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:31,277 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:31,278 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:31.278207', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:31,418 : INFO : EPOCH 0: training on 175599 raw words (110284 effective words) took 0.1s, 849837 effective words/s\n",
            "2025-10-17 14:06:31,560 : INFO : EPOCH 1: training on 175599 raw words (110214 effective words) took 0.1s, 831012 effective words/s\n",
            "2025-10-17 14:06:31,706 : INFO : EPOCH 2: training on 175599 raw words (110054 effective words) took 0.1s, 810349 effective words/s\n",
            "2025-10-17 14:06:31,859 : INFO : EPOCH 3: training on 175599 raw words (110334 effective words) took 0.1s, 773332 effective words/s\n",
            "2025-10-17 14:06:32,017 : INFO : EPOCH 4: training on 175599 raw words (110308 effective words) took 0.1s, 746638 effective words/s\n",
            "2025-10-17 14:06:32,017 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551194 effective words) took 0.7s, 745839 effective words/s', 'datetime': '2025-10-17T14:06:32.017843', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:32,018 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:32.018509', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:32,021 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:32,033 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:32,066 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:32,067 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:32,080 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:32.080666', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:32,081 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:32.081353', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:32,099 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:32,101 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:32,102 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:32.101937', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:32,106 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:32,202 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:32,231 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #10: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 0.9319492975870768, 'train_time_std': 0.008903690813385734}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:32,232 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:32,234 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:32.234633', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:32,235 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:32,235 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:32.235511', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:32,382 : INFO : EPOCH 0: training on 175599 raw words (110075 effective words) took 0.1s, 807059 effective words/s\n",
            "2025-10-17 14:06:32,534 : INFO : EPOCH 1: training on 175599 raw words (110135 effective words) took 0.1s, 780573 effective words/s\n",
            "2025-10-17 14:06:32,678 : INFO : EPOCH 2: training on 175599 raw words (110158 effective words) took 0.1s, 828245 effective words/s\n",
            "2025-10-17 14:06:32,811 : INFO : EPOCH 3: training on 175599 raw words (110340 effective words) took 0.1s, 893952 effective words/s\n",
            "2025-10-17 14:06:32,944 : INFO : EPOCH 4: training on 175599 raw words (110348 effective words) took 0.1s, 896603 effective words/s\n",
            "2025-10-17 14:06:32,945 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551056 effective words) took 0.7s, 776994 effective words/s', 'datetime': '2025-10-17T14:06:32.945113', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:32,945 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:32.945771', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:32,947 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:32,957 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:32,991 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:32,992 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:33,006 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:33.006494', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:33,007 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:33.007073', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:33,025 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:33,025 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:33,026 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:33.026397', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:33,028 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:33,114 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:33,138 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:33,138 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:33,140 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:33.140753', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:33,141 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:33,141 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:33.141552', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:33,279 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.1s, 860749 effective words/s\n",
            "2025-10-17 14:06:33,421 : INFO : EPOCH 1: training on 175599 raw words (110037 effective words) took 0.1s, 836917 effective words/s\n",
            "2025-10-17 14:06:33,579 : INFO : EPOCH 2: training on 175599 raw words (110196 effective words) took 0.1s, 740783 effective words/s\n",
            "2025-10-17 14:06:33,736 : INFO : EPOCH 3: training on 175599 raw words (110351 effective words) took 0.1s, 765637 effective words/s\n",
            "2025-10-17 14:06:33,891 : INFO : EPOCH 4: training on 175599 raw words (110276 effective words) took 0.1s, 766546 effective words/s\n",
            "2025-10-17 14:06:33,892 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550854 effective words) took 0.8s, 734404 effective words/s', 'datetime': '2025-10-17T14:06:33.891992', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:33,892 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:33.892814', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:33,895 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:33,906 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:33,938 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:33,939 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:33,953 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:33.953578', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:33,954 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:33.954284', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:33,971 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:33,972 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:33,972 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:33.972829', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:33,974 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:34,064 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:34,089 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:34,090 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:34,092 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:34.092809', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:34,093 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:34,093 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:34.093835', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:34,237 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.1s, 837913 effective words/s\n",
            "2025-10-17 14:06:34,388 : INFO : EPOCH 1: training on 175599 raw words (110178 effective words) took 0.1s, 796772 effective words/s\n",
            "2025-10-17 14:06:34,538 : INFO : EPOCH 2: training on 175599 raw words (110176 effective words) took 0.1s, 787813 effective words/s\n",
            "2025-10-17 14:06:34,673 : INFO : EPOCH 3: training on 175599 raw words (110383 effective words) took 0.1s, 896971 effective words/s\n",
            "2025-10-17 14:06:34,846 : INFO : EPOCH 4: training on 175599 raw words (110081 effective words) took 0.2s, 698544 effective words/s\n",
            "2025-10-17 14:06:34,846 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550812 effective words) took 0.8s, 732207 effective words/s', 'datetime': '2025-10-17T14:06:34.846786', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:34,847 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:34.847348', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:34,849 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:34,862 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:34,900 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:34,900 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:34,915 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:34.915711', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:34,916 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:34.916484', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:34,935 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:34,937 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:34,937 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:34.937584', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:34,970 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:34,971 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:34,973 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:34.973670', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:34,974 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:34.974172', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #11: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 0.9427781105041504, 'train_time_std': 0.011839644536183214}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:35,226 : INFO : EPOCH 0: training on 175599 raw words (110087 effective words) took 0.2s, 453983 effective words/s\n",
            "2025-10-17 14:06:35,462 : INFO : EPOCH 1: training on 175599 raw words (110309 effective words) took 0.2s, 487847 effective words/s\n",
            "2025-10-17 14:06:35,695 : INFO : EPOCH 2: training on 175599 raw words (110151 effective words) took 0.2s, 494808 effective words/s\n",
            "2025-10-17 14:06:35,914 : INFO : EPOCH 3: training on 175599 raw words (110251 effective words) took 0.2s, 526094 effective words/s\n",
            "2025-10-17 14:06:36,158 : INFO : EPOCH 4: training on 175599 raw words (110252 effective words) took 0.2s, 473519 effective words/s\n",
            "2025-10-17 14:06:36,159 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551050 effective words) took 1.2s, 465075 effective words/s', 'datetime': '2025-10-17T14:06:36.159377', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:36,159 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:36.159844', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:36,163 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:36,174 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:36,207 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:36,208 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:36,225 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:36.225827', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:36,226 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:36.226504', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:36,246 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:36,247 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:36,248 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:36.248367', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:36,277 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:36,278 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:36,281 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:36.281293', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:36,281 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:36.281811', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:36,516 : INFO : EPOCH 0: training on 175599 raw words (110344 effective words) took 0.2s, 498104 effective words/s\n",
            "2025-10-17 14:06:36,747 : INFO : EPOCH 1: training on 175599 raw words (110214 effective words) took 0.2s, 500479 effective words/s\n",
            "2025-10-17 14:06:36,988 : INFO : EPOCH 2: training on 175599 raw words (110364 effective words) took 0.2s, 481853 effective words/s\n",
            "2025-10-17 14:06:37,230 : INFO : EPOCH 3: training on 175599 raw words (110241 effective words) took 0.2s, 475732 effective words/s\n",
            "2025-10-17 14:06:37,453 : INFO : EPOCH 4: training on 175599 raw words (110307 effective words) took 0.2s, 518196 effective words/s\n",
            "2025-10-17 14:06:37,454 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551470 effective words) took 1.2s, 470638 effective words/s', 'datetime': '2025-10-17T14:06:37.454159', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:37,454 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:37.454622', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:37,455 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:37,464 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:37,499 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:37,499 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:37,516 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:37.516838', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:37,517 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:37.517331', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:37,539 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:37,541 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:37,541 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:37.541834', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:37,569 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:37,569 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:37,573 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:37.573403', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:37,573 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:37.573939', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:37,806 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.2s, 499482 effective words/s\n",
            "2025-10-17 14:06:38,059 : INFO : EPOCH 1: training on 175599 raw words (110048 effective words) took 0.2s, 455808 effective words/s\n",
            "2025-10-17 14:06:38,293 : INFO : EPOCH 2: training on 175599 raw words (109961 effective words) took 0.2s, 490942 effective words/s\n",
            "2025-10-17 14:06:38,545 : INFO : EPOCH 3: training on 175599 raw words (110249 effective words) took 0.2s, 459373 effective words/s\n",
            "2025-10-17 14:06:38,776 : INFO : EPOCH 4: training on 175599 raw words (110108 effective words) took 0.2s, 503651 effective words/s\n",
            "2025-10-17 14:06:38,777 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550360 effective words) took 1.2s, 457523 effective words/s', 'datetime': '2025-10-17T14:06:38.777377', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:38,777 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:38.777730', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:38,778 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:38,787 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:38,825 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:38,826 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:38,840 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:38.840740', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:38,841 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:38.841331', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:38,859 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:38,860 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:38,860 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:38.860794', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:38,890 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:38,891 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:38,893 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:38.893736', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:38,894 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:38.894318', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #12: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 1.3096357186635335, 'train_time_std': 0.012777475442690794}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:39,114 : INFO : EPOCH 0: training on 175599 raw words (110344 effective words) took 0.2s, 525810 effective words/s\n",
            "2025-10-17 14:06:39,344 : INFO : EPOCH 1: training on 175599 raw words (110313 effective words) took 0.2s, 505501 effective words/s\n",
            "2025-10-17 14:06:39,580 : INFO : EPOCH 2: training on 175599 raw words (110382 effective words) took 0.2s, 489497 effective words/s\n",
            "2025-10-17 14:06:39,817 : INFO : EPOCH 3: training on 175599 raw words (110250 effective words) took 0.2s, 486312 effective words/s\n",
            "2025-10-17 14:06:40,039 : INFO : EPOCH 4: training on 175599 raw words (110454 effective words) took 0.2s, 524708 effective words/s\n",
            "2025-10-17 14:06:40,040 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551743 effective words) took 1.1s, 481577 effective words/s', 'datetime': '2025-10-17T14:06:40.040380', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:40,040 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:40.040867', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:40,041 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:40,051 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:40,081 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:40,082 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:40,097 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:40.097683', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:40,098 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:40.098274', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:40,116 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:40,117 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:40,117 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:40.117830', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:40,147 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:40,148 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:40,151 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:40.151734', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:40,152 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:40.152160', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:40,383 : INFO : EPOCH 0: training on 175599 raw words (110391 effective words) took 0.2s, 502693 effective words/s\n",
            "2025-10-17 14:06:40,614 : INFO : EPOCH 1: training on 175599 raw words (110426 effective words) took 0.2s, 506700 effective words/s\n",
            "2025-10-17 14:06:40,840 : INFO : EPOCH 2: training on 175599 raw words (109986 effective words) took 0.2s, 514902 effective words/s\n",
            "2025-10-17 14:06:41,054 : INFO : EPOCH 3: training on 175599 raw words (110302 effective words) took 0.2s, 540783 effective words/s\n",
            "2025-10-17 14:06:41,266 : INFO : EPOCH 4: training on 175599 raw words (110129 effective words) took 0.2s, 546135 effective words/s\n",
            "2025-10-17 14:06:41,266 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551234 effective words) took 1.1s, 494906 effective words/s', 'datetime': '2025-10-17T14:06:41.266656', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:41,267 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:41.267172', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:41,267 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:41,278 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:41,307 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:41,307 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:41,324 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:41.324754', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:41,325 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:41.325448', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:41,346 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:41,347 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:41,347 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:41.347758', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:41,377 : INFO : estimated required memory for 4125 words and 100 dimensions: 5362500 bytes\n",
            "2025-10-17 14:06:41,377 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:41,379 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:41.379374', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:41,380 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:41.380038', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:41,601 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.2s, 523969 effective words/s\n",
            "2025-10-17 14:06:41,835 : INFO : EPOCH 1: training on 175599 raw words (110192 effective words) took 0.2s, 492554 effective words/s\n",
            "2025-10-17 14:06:42,067 : INFO : EPOCH 2: training on 175599 raw words (110132 effective words) took 0.2s, 497302 effective words/s\n",
            "2025-10-17 14:06:42,293 : INFO : EPOCH 3: training on 175599 raw words (110094 effective words) took 0.2s, 510985 effective words/s\n",
            "2025-10-17 14:06:42,552 : INFO : EPOCH 4: training on 175599 raw words (110293 effective words) took 0.2s, 446765 effective words/s\n",
            "2025-10-17 14:06:42,552 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550705 effective words) took 1.2s, 469711 effective words/s', 'datetime': '2025-10-17T14:06:42.552837', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:42,553 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:42.553547', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:42,554 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:42,564 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:42,594 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:42,595 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:42,614 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:42.614024', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:42,614 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:42.614673', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:42,631 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:42,632 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:42,633 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:42.633518', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:42,637 : INFO : constructing a huffman tree from 4125 words\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #13: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 1.2585927645365398, 'train_time_std': 0.024879737614813165}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:42,810 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:42,836 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:42,836 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:42,838 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:42.838654', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:42,839 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:42,839 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:42.839765', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:43,361 : INFO : EPOCH 0: training on 175599 raw words (110086 effective words) took 0.5s, 215942 effective words/s\n",
            "2025-10-17 14:06:43,851 : INFO : EPOCH 1: training on 175599 raw words (110309 effective words) took 0.5s, 229969 effective words/s\n",
            "2025-10-17 14:06:44,344 : INFO : EPOCH 2: training on 175599 raw words (110151 effective words) took 0.5s, 228327 effective words/s\n",
            "2025-10-17 14:06:44,845 : INFO : EPOCH 3: training on 175599 raw words (110196 effective words) took 0.5s, 224377 effective words/s\n",
            "2025-10-17 14:06:45,328 : INFO : EPOCH 4: training on 175599 raw words (110459 effective words) took 0.5s, 234141 effective words/s\n",
            "2025-10-17 14:06:45,328 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551201 effective words) took 2.5s, 221493 effective words/s', 'datetime': '2025-10-17T14:06:45.328912', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:45,329 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:45.329472', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:45,330 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:45,339 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:45,374 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:45,374 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:45,387 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:45.387002', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:45,387 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:45.387523', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:45,406 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:45,407 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:45,408 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:45.408003', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:45,412 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:45,505 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:45,529 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:45,530 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:45,532 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:45.532741', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:45,533 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:45,534 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:45.534018', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:45,982 : INFO : EPOCH 0: training on 175599 raw words (110391 effective words) took 0.4s, 251778 effective words/s\n",
            "2025-10-17 14:06:46,457 : INFO : EPOCH 1: training on 175599 raw words (110220 effective words) took 0.5s, 238104 effective words/s\n",
            "2025-10-17 14:06:46,961 : INFO : EPOCH 2: training on 175599 raw words (110027 effective words) took 0.5s, 223025 effective words/s\n",
            "2025-10-17 14:06:47,467 : INFO : EPOCH 3: training on 175599 raw words (110106 effective words) took 0.5s, 222011 effective words/s\n",
            "2025-10-17 14:06:47,961 : INFO : EPOCH 4: training on 175599 raw words (110459 effective words) took 0.5s, 228274 effective words/s\n",
            "2025-10-17 14:06:47,962 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551203 effective words) took 2.4s, 227007 effective words/s', 'datetime': '2025-10-17T14:06:47.962522', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:47,963 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:47.963022', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:47,966 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:47,983 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:48,053 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:48,054 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:48,084 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:48.084223', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:48,084 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:48.084977', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:48,122 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:48,124 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:48,124 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:48.124542', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:48,128 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:48,322 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:48,366 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:48,367 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:48,373 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:48.373206', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:48,373 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:48,374 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:48.374192', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:48,853 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.5s, 235218 effective words/s\n",
            "2025-10-17 14:06:49,402 : INFO : EPOCH 1: training on 175599 raw words (110363 effective words) took 0.5s, 205071 effective words/s\n",
            "2025-10-17 14:06:49,913 : INFO : EPOCH 2: training on 175599 raw words (110042 effective words) took 0.5s, 219894 effective words/s\n",
            "2025-10-17 14:06:50,383 : INFO : EPOCH 3: training on 175599 raw words (110247 effective words) took 0.5s, 239719 effective words/s\n",
            "2025-10-17 14:06:50,847 : INFO : EPOCH 4: training on 175599 raw words (110232 effective words) took 0.5s, 242811 effective words/s\n",
            "2025-10-17 14:06:50,848 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550878 effective words) took 2.5s, 222683 effective words/s', 'datetime': '2025-10-17T14:06:50.848416', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:50,848 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:50.848892', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:50,851 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:50,865 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:50,905 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:50,906 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:50,936 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:50.936669', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:50,937 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:50.937239', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:50,981 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:50,983 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:50,983 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:50.983720', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:50,987 : INFO : constructing a huffman tree from 4125 words\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #14: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 2.765448013941447, 'train_time_std': 0.10220001702041183}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:51,108 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:51,137 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:51,137 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:51,139 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:51.139685', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:51,140 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:51,140 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:51.140503', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:51,617 : INFO : EPOCH 0: training on 175599 raw words (109994 effective words) took 0.5s, 235391 effective words/s\n",
            "2025-10-17 14:06:52,126 : INFO : EPOCH 1: training on 175599 raw words (110201 effective words) took 0.5s, 222028 effective words/s\n",
            "2025-10-17 14:06:52,601 : INFO : EPOCH 2: training on 175599 raw words (110211 effective words) took 0.5s, 236648 effective words/s\n",
            "2025-10-17 14:06:53,084 : INFO : EPOCH 3: training on 175599 raw words (110250 effective words) took 0.5s, 233103 effective words/s\n",
            "2025-10-17 14:06:53,547 : INFO : EPOCH 4: training on 175599 raw words (110343 effective words) took 0.5s, 243884 effective words/s\n",
            "2025-10-17 14:06:53,547 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (550999 effective words) took 2.4s, 228938 effective words/s', 'datetime': '2025-10-17T14:06:53.547625', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:53,548 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:53.548363', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:53,551 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:53,562 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:53,591 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:53,591 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:53,606 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:53.606484', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:53,607 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:53.607302', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:53,627 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:53,628 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:53,629 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:53.629118', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:53,632 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:53,742 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:53,770 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:53,771 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:53,776 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:53.775965', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:53,776 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:53,777 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:53.777320', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:54,249 : INFO : EPOCH 0: training on 175599 raw words (110391 effective words) took 0.5s, 239202 effective words/s\n",
            "2025-10-17 14:06:54,674 : INFO : EPOCH 1: training on 175599 raw words (110220 effective words) took 0.4s, 265633 effective words/s\n",
            "2025-10-17 14:06:55,176 : INFO : EPOCH 2: training on 175599 raw words (110303 effective words) took 0.5s, 224057 effective words/s\n",
            "2025-10-17 14:06:55,670 : INFO : EPOCH 3: training on 175599 raw words (110237 effective words) took 0.5s, 227792 effective words/s\n",
            "2025-10-17 14:06:56,157 : INFO : EPOCH 4: training on 175599 raw words (110218 effective words) took 0.5s, 231600 effective words/s\n",
            "2025-10-17 14:06:56,158 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551369 effective words) took 2.4s, 231568 effective words/s', 'datetime': '2025-10-17T14:06:56.158919', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:56,159 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:56.159592', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:56,162 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:56,180 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:06:56,213 : INFO : collected 17251 word types from a corpus of 175599 raw words and 18 sentences\n",
            "2025-10-17 14:06:56,213 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:56,226 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 4125 unique words (23.91% of original 17251, drops 13126)', 'datetime': '2025-10-17T14:06:56.226207', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:56,226 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 154201 word corpus (87.81% of original 175599, drops 21398)', 'datetime': '2025-10-17T14:06:56.226771', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:56,244 : INFO : deleting the raw counts dictionary of 17251 items\n",
            "2025-10-17 14:06:56,245 : INFO : sample=0.001 downsamples 40 most-common words\n",
            "2025-10-17 14:06:56,245 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 110199.4281334271 word corpus (71.5%% of prior 154201)', 'datetime': '2025-10-17T14:06:56.245914', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:56,247 : INFO : constructing a huffman tree from 4125 words\n",
            "2025-10-17 14:06:56,343 : INFO : built huffman tree with maximum node depth 15\n",
            "2025-10-17 14:06:56,368 : INFO : estimated required memory for 4125 words and 100 dimensions: 7837500 bytes\n",
            "2025-10-17 14:06:56,368 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:56,370 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:56.370899', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:56,371 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:06:56,371 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 4125 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:56.371866', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:56,949 : INFO : EPOCH 0: training on 175599 raw words (110317 effective words) took 0.6s, 194812 effective words/s\n",
            "2025-10-17 14:06:57,450 : INFO : EPOCH 1: training on 175599 raw words (110160 effective words) took 0.5s, 225231 effective words/s\n",
            "2025-10-17 14:06:57,962 : INFO : EPOCH 2: training on 175599 raw words (110288 effective words) took 0.5s, 219862 effective words/s\n",
            "2025-10-17 14:06:58,466 : INFO : EPOCH 3: training on 175599 raw words (110224 effective words) took 0.5s, 223063 effective words/s\n",
            "2025-10-17 14:06:58,951 : INFO : EPOCH 4: training on 175599 raw words (110212 effective words) took 0.5s, 235751 effective words/s\n",
            "2025-10-17 14:06:58,952 : INFO : Word2Vec lifecycle event {'msg': 'training on 877995 raw words (551201 effective words) took 2.6s, 213642 effective words/s', 'datetime': '2025-10-17T14:06:58.952328', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:06:58,952 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4125, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:06:58.952774', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:06:58,954 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:06:59,073 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #15: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 2.7010581493377686, 'train_time_std': 0.07374958487184224}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:06:59,426 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:06:59,426 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:06:59,494 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:06:59.494623', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:59,495 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:06:59.495226', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:59,583 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:06:59,587 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:06:59,588 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:06:59.588606', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:06:59,907 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:06:59,908 : INFO : resetting layer weights\n",
            "2025-10-17 14:06:59,929 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:06:59.929281', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:06:59,930 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:06:59.930089', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:00,934 : INFO : EPOCH 0: training on 1788017 raw words (1241950 effective words) took 1.0s, 1257327 effective words/s\n",
            "2025-10-17 14:07:01,957 : INFO : EPOCH 1 - PROGRESS: at 96.09% examples, 1189178 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:01,993 : INFO : EPOCH 1: training on 1788017 raw words (1242569 effective words) took 1.0s, 1193258 effective words/s\n",
            "2025-10-17 14:07:03,022 : INFO : EPOCH 2 - PROGRESS: at 93.30% examples, 1155917 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:03,114 : INFO : EPOCH 2: training on 1788017 raw words (1241947 effective words) took 1.1s, 1133650 effective words/s\n",
            "2025-10-17 14:07:04,136 : INFO : EPOCH 3 - PROGRESS: at 94.97% examples, 1180294 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:07:04,177 : INFO : EPOCH 3: training on 1788017 raw words (1242762 effective words) took 1.0s, 1191732 effective words/s\n",
            "2025-10-17 14:07:05,194 : INFO : EPOCH 4 - PROGRESS: at 99.44% examples, 1236921 words/s, in_qsize 1, out_qsize 1\n",
            "2025-10-17 14:07:05,200 : INFO : EPOCH 4: training on 1788017 raw words (1242756 effective words) took 1.0s, 1235241 effective words/s\n",
            "2025-10-17 14:07:05,201 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211984 effective words) took 5.3s, 1178688 effective words/s', 'datetime': '2025-10-17T14:07:05.201095', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:05,201 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:05.201657', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:05,204 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:05,291 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:07:05,658 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:05,659 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:05,722 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:05.722913', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:05,723 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:05.723466', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:05,803 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:05,806 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:05,806 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:05.806604', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:05,930 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:07:05,931 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:05,938 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:05.938748', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:05,939 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:05.939287', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:06,965 : INFO : EPOCH 0 - PROGRESS: at 87.71% examples, 1080044 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:07,113 : INFO : EPOCH 0: training on 1788017 raw words (1242189 effective words) took 1.2s, 1071687 effective words/s\n",
            "2025-10-17 14:07:08,132 : INFO : EPOCH 1 - PROGRESS: at 91.62% examples, 1135895 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:08,221 : INFO : EPOCH 1: training on 1788017 raw words (1242444 effective words) took 1.1s, 1137548 effective words/s\n",
            "2025-10-17 14:07:09,210 : INFO : EPOCH 2: training on 1788017 raw words (1241744 effective words) took 1.0s, 1280212 effective words/s\n",
            "2025-10-17 14:07:10,224 : INFO : EPOCH 3: training on 1788017 raw words (1241419 effective words) took 1.0s, 1243655 effective words/s\n",
            "2025-10-17 14:07:11,244 : INFO : EPOCH 4 - PROGRESS: at 96.09% examples, 1193026 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:11,278 : INFO : EPOCH 4: training on 1788017 raw words (1242471 effective words) took 1.0s, 1199705 effective words/s\n",
            "2025-10-17 14:07:11,278 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6210267 effective words) took 5.3s, 1163146 effective words/s', 'datetime': '2025-10-17T14:07:11.278890', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:11,279 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:11.279503', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:11,283 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:11,370 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:07:11,755 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:11,756 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:11,833 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:11.833669', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:11,834 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:11.834284', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:11,926 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:11,929 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:11,929 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:11.929835', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:12,077 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:07:12,078 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:12,086 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:12.086932', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:12,087 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:12.087957', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:13,136 : INFO : EPOCH 0 - PROGRESS: at 95.53% examples, 1157659 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:07:13,184 : INFO : EPOCH 0: training on 1788017 raw words (1243003 effective words) took 1.1s, 1155878 effective words/s\n",
            "2025-10-17 14:07:14,205 : INFO : EPOCH 1 - PROGRESS: at 91.62% examples, 1135211 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:14,325 : INFO : EPOCH 1: training on 1788017 raw words (1241657 effective words) took 1.1s, 1105526 effective words/s\n",
            "2025-10-17 14:07:15,369 : INFO : EPOCH 2 - PROGRESS: at 94.97% examples, 1151116 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:07:15,407 : INFO : EPOCH 2: training on 1788017 raw words (1241942 effective words) took 1.1s, 1167792 effective words/s\n",
            "2025-10-17 14:07:16,382 : INFO : EPOCH 3: training on 1788017 raw words (1242716 effective words) took 1.0s, 1297127 effective words/s\n",
            "2025-10-17 14:07:17,395 : INFO : EPOCH 4: training on 1788017 raw words (1242513 effective words) took 1.0s, 1246185 effective words/s\n",
            "2025-10-17 14:07:17,396 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211831 effective words) took 5.3s, 1170431 effective words/s', 'datetime': '2025-10-17T14:07:17.396306', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:17,397 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:17.397141', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:17,401 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:17,510 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #16: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 6.148722728093465, 'train_time_std': 0.07312011559651219}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:07:17,863 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:17,863 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:17,927 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:17.927948', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:17,928 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:17.928557', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:18,014 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:18,016 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:18,017 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:18.017388', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:18,154 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:07:18,155 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:18,165 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:18.165731', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:18,166 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:18.166258', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:19,186 : INFO : EPOCH 0 - PROGRESS: at 92.74% examples, 1151044 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:07:19,298 : INFO : EPOCH 0: training on 1788017 raw words (1242604 effective words) took 1.1s, 1114805 effective words/s\n",
            "2025-10-17 14:07:20,348 : INFO : EPOCH 1 - PROGRESS: at 94.41% examples, 1142926 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:07:20,400 : INFO : EPOCH 1: training on 1788017 raw words (1242685 effective words) took 1.1s, 1150078 effective words/s\n",
            "2025-10-17 14:07:21,334 : INFO : EPOCH 2: training on 1788017 raw words (1242681 effective words) took 0.9s, 1355020 effective words/s\n",
            "2025-10-17 14:07:22,319 : INFO : EPOCH 3: training on 1788017 raw words (1241678 effective words) took 1.0s, 1281136 effective words/s\n",
            "2025-10-17 14:07:23,297 : INFO : EPOCH 4: training on 1788017 raw words (1242503 effective words) took 1.0s, 1295011 effective words/s\n",
            "2025-10-17 14:07:23,297 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212151 effective words) took 5.1s, 1210635 effective words/s', 'datetime': '2025-10-17T14:07:23.297952', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:23,298 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:23.298455', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:23,301 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:23,394 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:07:23,731 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:23,731 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:23,791 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:23.791861', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:23,792 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:23.792399', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:23,878 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:23,881 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:23,881 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:23.881850', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:24,016 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:07:24,016 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:24,025 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:24.025522', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:24,026 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:24.026065', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:25,003 : INFO : EPOCH 0: training on 1788017 raw words (1242904 effective words) took 1.0s, 1293976 effective words/s\n",
            "2025-10-17 14:07:25,930 : INFO : EPOCH 1: training on 1788017 raw words (1242975 effective words) took 0.9s, 1364955 effective words/s\n",
            "2025-10-17 14:07:26,835 : INFO : EPOCH 2: training on 1788017 raw words (1242231 effective words) took 0.9s, 1398365 effective words/s\n",
            "2025-10-17 14:07:27,745 : INFO : EPOCH 3: training on 1788017 raw words (1242757 effective words) took 0.9s, 1389388 effective words/s\n",
            "2025-10-17 14:07:28,663 : INFO : EPOCH 4: training on 1788017 raw words (1241650 effective words) took 0.9s, 1377015 effective words/s\n",
            "2025-10-17 14:07:28,664 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212517 effective words) took 4.6s, 1339512 effective words/s', 'datetime': '2025-10-17T14:07:28.664371', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:28,664 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:28.664914', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:28,667 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:28,754 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:07:29,060 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:29,060 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:29,116 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:29.116533', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:29,117 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:29.117153', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:29,196 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:29,199 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:29,200 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:29.200554', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:29,320 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:07:29,320 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:29,329 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:29.329589', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:29,330 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:29.330137', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:30,217 : INFO : EPOCH 0: training on 1788017 raw words (1242155 effective words) took 0.9s, 1426705 effective words/s\n",
            "2025-10-17 14:07:31,139 : INFO : EPOCH 1: training on 1788017 raw words (1242440 effective words) took 0.9s, 1371038 effective words/s\n",
            "2025-10-17 14:07:32,053 : INFO : EPOCH 2: training on 1788017 raw words (1242060 effective words) took 0.9s, 1382139 effective words/s\n",
            "2025-10-17 14:07:33,009 : INFO : EPOCH 3: training on 1788017 raw words (1242651 effective words) took 0.9s, 1322734 effective words/s\n",
            "2025-10-17 14:07:33,949 : INFO : EPOCH 4: training on 1788017 raw words (1242325 effective words) took 0.9s, 1343592 effective words/s\n",
            "2025-10-17 14:07:33,949 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211631 effective words) took 4.6s, 1344706 effective words/s', 'datetime': '2025-10-17T14:07:33.949871', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:33,950 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:33.950458', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:33,952 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:34,041 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #17: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 5.517084995905559, 'train_time_std': 0.27259832355268065}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:07:34,439 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:34,440 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:34,501 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:34.501776', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:34,502 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:34.502341', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:34,723 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:34,728 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:34,728 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:34.728908', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:34,745 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:07:35,586 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:07:35,728 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:07:35,728 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:35,738 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:35.738736', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:35,739 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:07:35,740 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:35.740349', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:36,772 : INFO : EPOCH 0 - PROGRESS: at 52.51% examples, 647712 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:37,571 : INFO : EPOCH 0: training on 1788017 raw words (1241638 effective words) took 1.8s, 684554 effective words/s\n",
            "2025-10-17 14:07:38,603 : INFO : EPOCH 1 - PROGRESS: at 55.31% examples, 679903 words/s, in_qsize 4, out_qsize 1\n",
            "2025-10-17 14:07:39,379 : INFO : EPOCH 1: training on 1788017 raw words (1241971 effective words) took 1.8s, 693333 effective words/s\n",
            "2025-10-17 14:07:40,418 : INFO : EPOCH 2 - PROGRESS: at 48.04% examples, 589543 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:07:41,415 : INFO : EPOCH 2: training on 1788017 raw words (1242632 effective words) took 2.0s, 615133 effective words/s\n",
            "2025-10-17 14:07:42,463 : INFO : EPOCH 3 - PROGRESS: at 53.07% examples, 644528 words/s, in_qsize 4, out_qsize 1\n",
            "2025-10-17 14:07:43,412 : INFO : EPOCH 3: training on 1788017 raw words (1241911 effective words) took 2.0s, 627757 effective words/s\n",
            "2025-10-17 14:07:44,436 : INFO : EPOCH 4 - PROGRESS: at 45.81% examples, 570202 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:45,442 : INFO : EPOCH 4 - PROGRESS: at 99.44% examples, 613875 words/s, in_qsize 1, out_qsize 1\n",
            "2025-10-17 14:07:45,444 : INFO : EPOCH 4: training on 1788017 raw words (1242551 effective words) took 2.0s, 616032 effective words/s\n",
            "2025-10-17 14:07:45,445 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6210703 effective words) took 9.7s, 640004 effective words/s', 'datetime': '2025-10-17T14:07:45.445016', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:45,445 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:45.445494', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:45,448 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:45,567 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:07:45,918 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:45,918 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:45,980 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:45.980783', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:45,981 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:45.981427', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:46,069 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:46,073 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:46,073 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:46.073845', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:46,082 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:07:46,686 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:07:46,828 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:07:46,829 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:46,837 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:46.837946', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:46,838 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:07:46,839 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:46.839013', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:47,877 : INFO : EPOCH 0 - PROGRESS: at 52.51% examples, 643363 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:48,737 : INFO : EPOCH 0: training on 1788017 raw words (1242894 effective words) took 1.9s, 660490 effective words/s\n",
            "2025-10-17 14:07:49,754 : INFO : EPOCH 1 - PROGRESS: at 46.93% examples, 588555 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:50,775 : INFO : EPOCH 1 - PROGRESS: at 97.77% examples, 601878 words/s, in_qsize 4, out_qsize 0\n",
            "2025-10-17 14:07:50,807 : INFO : EPOCH 1: training on 1788017 raw words (1242968 effective words) took 2.1s, 605339 effective words/s\n",
            "2025-10-17 14:07:51,835 : INFO : EPOCH 2 - PROGRESS: at 50.28% examples, 623864 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:52,769 : INFO : EPOCH 2: training on 1788017 raw words (1242230 effective words) took 1.9s, 638580 effective words/s\n",
            "2025-10-17 14:07:53,796 : INFO : EPOCH 3 - PROGRESS: at 45.81% examples, 569268 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:07:54,803 : INFO : EPOCH 3 - PROGRESS: at 97.21% examples, 599517 words/s, in_qsize 3, out_qsize 2\n",
            "2025-10-17 14:07:54,838 : INFO : EPOCH 3: training on 1788017 raw words (1241937 effective words) took 2.1s, 605212 effective words/s\n",
            "2025-10-17 14:07:55,871 : INFO : EPOCH 4 - PROGRESS: at 46.37% examples, 573690 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:07:56,876 : INFO : EPOCH 4 - PROGRESS: at 98.88% examples, 608882 words/s, in_qsize 2, out_qsize 1\n",
            "2025-10-17 14:07:56,882 : INFO : EPOCH 4: training on 1788017 raw words (1242007 effective words) took 2.0s, 613226 effective words/s\n",
            "2025-10-17 14:07:56,883 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212036 effective words) took 10.0s, 618499 effective words/s', 'datetime': '2025-10-17T14:07:56.883316', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:56,883 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:07:56.883783', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:07:56,893 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:07:56,989 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:07:57,370 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:07:57,371 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:07:57,428 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:07:57.427982', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:57,428 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:07:57.428619', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:57,511 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:07:57,514 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:07:57,514 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:07:57.514757', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:07:57,523 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:07:58,023 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:07:58,145 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:07:58,146 : INFO : resetting layer weights\n",
            "2025-10-17 14:07:58,156 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:07:58.156377', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:07:58,157 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:07:58,157 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:07:58.157886', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:07:59,284 : INFO : EPOCH 0 - PROGRESS: at 58.66% examples, 713038 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:00,033 : INFO : EPOCH 0: training on 1788017 raw words (1242002 effective words) took 1.8s, 700642 effective words/s\n",
            "2025-10-17 14:08:01,055 : INFO : EPOCH 1 - PROGRESS: at 47.49% examples, 591815 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:02,055 : INFO : EPOCH 1 - PROGRESS: at 98.88% examples, 612891 words/s, in_qsize 2, out_qsize 1\n",
            "2025-10-17 14:08:02,064 : INFO : EPOCH 1: training on 1788017 raw words (1243089 effective words) took 2.0s, 616305 effective words/s\n",
            "2025-10-17 14:08:03,090 : INFO : EPOCH 2 - PROGRESS: at 53.63% examples, 664701 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:03,920 : INFO : EPOCH 2: training on 1788017 raw words (1242241 effective words) took 1.8s, 675790 effective words/s\n",
            "2025-10-17 14:08:04,938 : INFO : EPOCH 3 - PROGRESS: at 52.51% examples, 655380 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:05,812 : INFO : EPOCH 3: training on 1788017 raw words (1242578 effective words) took 1.9s, 662086 effective words/s\n",
            "2025-10-17 14:08:06,838 : INFO : EPOCH 4 - PROGRESS: at 43.58% examples, 543445 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:07,840 : INFO : EPOCH 4 - PROGRESS: at 96.65% examples, 598315 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:07,885 : INFO : EPOCH 4: training on 1788017 raw words (1242362 effective words) took 2.1s, 604721 effective words/s\n",
            "2025-10-17 14:08:07,885 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212272 effective words) took 9.7s, 638646 effective words/s', 'datetime': '2025-10-17T14:08:07.885751', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:07,886 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:08:07.886369', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:08:07,893 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:08:07,985 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #18: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 11.313425143559774, 'train_time_std': 0.22267623597938954}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:08:08,328 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:08:08,328 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:08:08,392 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:08:08.392104', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:08,392 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:08:08.392910', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:08,485 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:08:08,488 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:08:08,488 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:08:08.488739', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:08,498 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:08:09,185 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:08:09,357 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:08:09,358 : INFO : resetting layer weights\n",
            "2025-10-17 14:08:09,366 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:08:09.366436', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:08:09,366 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:08:09,367 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:08:09.367330', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:10,385 : INFO : EPOCH 0 - PROGRESS: at 52.51% examples, 656161 words/s, in_qsize 4, out_qsize 0\n",
            "2025-10-17 14:08:11,231 : INFO : EPOCH 0: training on 1788017 raw words (1242621 effective words) took 1.8s, 672517 effective words/s\n",
            "2025-10-17 14:08:12,250 : INFO : EPOCH 1 - PROGRESS: at 48.60% examples, 607146 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:08:13,098 : INFO : EPOCH 1: training on 1788017 raw words (1242351 effective words) took 1.9s, 670511 effective words/s\n",
            "2025-10-17 14:08:14,122 : INFO : EPOCH 2 - PROGRESS: at 50.84% examples, 632379 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:14,995 : INFO : EPOCH 2: training on 1788017 raw words (1242553 effective words) took 1.9s, 660479 effective words/s\n",
            "2025-10-17 14:08:16,017 : INFO : EPOCH 3 - PROGRESS: at 48.04% examples, 600910 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:16,899 : INFO : EPOCH 3: training on 1788017 raw words (1242151 effective words) took 1.9s, 658870 effective words/s\n",
            "2025-10-17 14:08:17,925 : INFO : EPOCH 4 - PROGRESS: at 56.42% examples, 697126 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:18,602 : INFO : EPOCH 4: training on 1788017 raw words (1242598 effective words) took 1.7s, 736665 effective words/s\n",
            "2025-10-17 14:08:18,602 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212274 effective words) took 9.2s, 672688 effective words/s', 'datetime': '2025-10-17T14:08:18.602969', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:18,603 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:08:18.603645', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:08:18,611 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:08:18,705 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:08:19,031 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:08:19,031 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:08:19,093 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:08:19.093893', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:19,094 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:08:19.094410', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:19,183 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:08:19,186 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:08:19,186 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:08:19.186716', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:19,195 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:08:19,722 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:08:19,858 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:08:19,859 : INFO : resetting layer weights\n",
            "2025-10-17 14:08:19,867 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:08:19.867933', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:08:19,868 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:08:19,869 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:08:19.869359', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:20,888 : INFO : EPOCH 0 - PROGRESS: at 43.02% examples, 539228 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:21,901 : INFO : EPOCH 0 - PROGRESS: at 83.24% examples, 513840 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:08:22,341 : INFO : EPOCH 0: training on 1788017 raw words (1242408 effective words) took 2.5s, 505816 effective words/s\n",
            "2025-10-17 14:08:23,365 : INFO : EPOCH 1 - PROGRESS: at 51.40% examples, 641608 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:08:24,166 : INFO : EPOCH 1: training on 1788017 raw words (1242186 effective words) took 1.8s, 688359 effective words/s\n",
            "2025-10-17 14:08:25,190 : INFO : EPOCH 2 - PROGRESS: at 51.40% examples, 639962 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:26,056 : INFO : EPOCH 2: training on 1788017 raw words (1242336 effective words) took 1.9s, 663366 effective words/s\n",
            "2025-10-17 14:08:27,080 : INFO : EPOCH 3 - PROGRESS: at 45.25% examples, 562543 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:28,091 : INFO : EPOCH 3 - PROGRESS: at 94.97% examples, 584493 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:08:28,188 : INFO : EPOCH 3: training on 1788017 raw words (1241658 effective words) took 2.1s, 586528 effective words/s\n",
            "2025-10-17 14:08:29,222 : INFO : EPOCH 4 - PROGRESS: at 43.02% examples, 533759 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:30,163 : INFO : EPOCH 4: training on 1788017 raw words (1242598 effective words) took 2.0s, 636126 effective words/s\n",
            "2025-10-17 14:08:30,164 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211186 effective words) took 10.3s, 603360 effective words/s', 'datetime': '2025-10-17T14:08:30.164102', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:30,164 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:08:30.164556', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:08:30,173 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:08:30,271 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:08:30,631 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:08:30,631 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:08:30,696 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:08:30.696055', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:30,696 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:08:30.696642', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:30,782 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:08:30,786 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:08:30,787 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:08:30.787428', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:30,795 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:08:31,352 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:08:31,470 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:08:31,471 : INFO : resetting layer weights\n",
            "2025-10-17 14:08:31,479 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:08:31.479548', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:08:31,480 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:08:31,480 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:08:31.480460', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:32,505 : INFO : EPOCH 0 - PROGRESS: at 51.40% examples, 638901 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:33,398 : INFO : EPOCH 0: training on 1788017 raw words (1242385 effective words) took 1.9s, 653271 effective words/s\n",
            "2025-10-17 14:08:34,418 : INFO : EPOCH 1 - PROGRESS: at 54.75% examples, 681846 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:35,159 : INFO : EPOCH 1: training on 1788017 raw words (1242229 effective words) took 1.7s, 712327 effective words/s\n",
            "2025-10-17 14:08:36,194 : INFO : EPOCH 2 - PROGRESS: at 54.75% examples, 671479 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:36,940 : INFO : EPOCH 2: training on 1788017 raw words (1243169 effective words) took 1.8s, 704595 effective words/s\n",
            "2025-10-17 14:08:37,973 : INFO : EPOCH 3 - PROGRESS: at 55.31% examples, 680235 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:08:38,770 : INFO : EPOCH 3: training on 1788017 raw words (1242432 effective words) took 1.8s, 685487 effective words/s\n",
            "2025-10-17 14:08:39,791 : INFO : EPOCH 4 - PROGRESS: at 59.78% examples, 740196 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:40,463 : INFO : EPOCH 4: training on 1788017 raw words (1242278 effective words) took 1.7s, 740824 effective words/s\n",
            "2025-10-17 14:08:40,464 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212493 effective words) took 9.0s, 691551 effective words/s', 'datetime': '2025-10-17T14:08:40.464235', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:40,464 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:08:40.464721', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:08:40,472 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:08:40,558 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #19: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 10.859464486440023, 'train_time_std': 0.525564304551739}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:08:40,904 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:08:40,904 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:08:40,964 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:08:40.964121', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:40,964 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:08:40.964706', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:41,047 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:08:41,050 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:08:41,050 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:08:41.050873', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:41,178 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:08:41,179 : INFO : resetting layer weights\n",
            "2025-10-17 14:08:41,187 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:08:41.187341', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:08:41,187 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:08:41.187866', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:42,215 : INFO : EPOCH 0 - PROGRESS: at 29.05% examples, 364081 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:43,236 : INFO : EPOCH 0 - PROGRESS: at 65.36% examples, 399146 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:44,223 : INFO : EPOCH 0: training on 1788017 raw words (1241447 effective words) took 3.0s, 411163 effective words/s\n",
            "2025-10-17 14:08:45,264 : INFO : EPOCH 1 - PROGRESS: at 31.84% examples, 391220 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:46,271 : INFO : EPOCH 1 - PROGRESS: at 68.72% examples, 420146 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:47,199 : INFO : EPOCH 1: training on 1788017 raw words (1242352 effective words) took 3.0s, 419701 effective words/s\n",
            "2025-10-17 14:08:48,215 : INFO : EPOCH 2 - PROGRESS: at 28.49% examples, 360993 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:49,218 : INFO : EPOCH 2 - PROGRESS: at 62.57% examples, 387986 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:50,239 : INFO : EPOCH 2 - PROGRESS: at 99.44% examples, 408275 words/s, in_qsize 1, out_qsize 1\n",
            "2025-10-17 14:08:50,248 : INFO : EPOCH 2: training on 1788017 raw words (1242133 effective words) took 3.0s, 409367 effective words/s\n",
            "2025-10-17 14:08:51,283 : INFO : EPOCH 3 - PROGRESS: at 31.28% examples, 390246 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:52,311 : INFO : EPOCH 3 - PROGRESS: at 68.16% examples, 415554 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:53,187 : INFO : EPOCH 3: training on 1788017 raw words (1242455 effective words) took 2.9s, 426425 effective words/s\n",
            "2025-10-17 14:08:54,211 : INFO : EPOCH 4 - PROGRESS: at 31.84% examples, 397806 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:08:55,214 : INFO : EPOCH 4 - PROGRESS: at 67.60% examples, 416923 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:56,145 : INFO : EPOCH 4: training on 1788017 raw words (1242195 effective words) took 2.9s, 422115 effective words/s\n",
            "2025-10-17 14:08:56,146 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6210582 effective words) took 15.0s, 415209 effective words/s', 'datetime': '2025-10-17T14:08:56.146383', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:56,146 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:08:56.146762', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:08:56,153 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:08:56,252 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:08:56,574 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:08:56,575 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:08:56,631 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:08:56.631741', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:56,632 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:08:56.632304', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:56,714 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:08:56,717 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:08:56,717 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:08:56.717778', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:08:56,861 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:08:56,862 : INFO : resetting layer weights\n",
            "2025-10-17 14:08:56,873 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:08:56.873691', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:08:56,874 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:08:56.874258', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:08:57,904 : INFO : EPOCH 0 - PROGRESS: at 31.28% examples, 389221 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:58,933 : INFO : EPOCH 0 - PROGRESS: at 69.83% examples, 424806 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:08:59,763 : INFO : EPOCH 0: training on 1788017 raw words (1241447 effective words) took 2.9s, 432149 effective words/s\n",
            "2025-10-17 14:09:00,812 : INFO : EPOCH 1 - PROGRESS: at 32.96% examples, 402400 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:01,826 : INFO : EPOCH 1 - PROGRESS: at 69.83% examples, 424034 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:02,652 : INFO : EPOCH 1: training on 1788017 raw words (1242049 effective words) took 2.9s, 432491 effective words/s\n",
            "2025-10-17 14:09:03,673 : INFO : EPOCH 2 - PROGRESS: at 32.96% examples, 413775 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:04,684 : INFO : EPOCH 2 - PROGRESS: at 69.83% examples, 430403 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:05,532 : INFO : EPOCH 2: training on 1788017 raw words (1242524 effective words) took 2.9s, 433774 effective words/s\n",
            "2025-10-17 14:09:06,551 : INFO : EPOCH 3 - PROGRESS: at 31.28% examples, 393847 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:07,563 : INFO : EPOCH 3 - PROGRESS: at 66.48% examples, 410152 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:08,555 : INFO : EPOCH 3: training on 1788017 raw words (1242330 effective words) took 3.0s, 413378 effective words/s\n",
            "2025-10-17 14:09:09,583 : INFO : EPOCH 4 - PROGRESS: at 35.20% examples, 438180 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:10,597 : INFO : EPOCH 4 - PROGRESS: at 75.42% examples, 463562 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:11,235 : INFO : EPOCH 4: training on 1788017 raw words (1242241 effective words) took 2.7s, 466618 effective words/s\n",
            "2025-10-17 14:09:11,235 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6210591 effective words) took 14.4s, 432464 effective words/s', 'datetime': '2025-10-17T14:09:11.235693', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:09:11,236 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:09:11.236300', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:09:11,239 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:09:11,334 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:09:11,670 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:09:11,671 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:09:11,731 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:09:11.731315', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:11,731 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:09:11.731854', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:11,817 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:09:11,820 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:09:11,821 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:09:11.821274', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:11,989 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:09:11,990 : INFO : resetting layer weights\n",
            "2025-10-17 14:09:12,000 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:09:12.000581', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:09:12,001 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:09:12.001215', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:09:13,023 : INFO : EPOCH 0 - PROGRESS: at 30.73% examples, 387309 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:14,023 : INFO : EPOCH 0 - PROGRESS: at 69.83% examples, 433774 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:14,725 : INFO : EPOCH 0: training on 1788017 raw words (1242951 effective words) took 2.7s, 459718 effective words/s\n",
            "2025-10-17 14:09:15,744 : INFO : EPOCH 1 - PROGRESS: at 35.75% examples, 447798 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:16,755 : INFO : EPOCH 1 - PROGRESS: at 74.86% examples, 461879 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:17,419 : INFO : EPOCH 1: training on 1788017 raw words (1242297 effective words) took 2.7s, 463573 effective words/s\n",
            "2025-10-17 14:09:18,436 : INFO : EPOCH 2 - PROGRESS: at 29.61% examples, 374399 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:19,447 : INFO : EPOCH 2 - PROGRESS: at 62.57% examples, 385960 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:20,482 : INFO : EPOCH 2 - PROGRESS: at 96.09% examples, 391870 words/s, in_qsize 4, out_qsize 2\n",
            "2025-10-17 14:09:20,574 : INFO : EPOCH 2: training on 1788017 raw words (1241893 effective words) took 3.1s, 395485 effective words/s\n",
            "2025-10-17 14:09:21,599 : INFO : EPOCH 3 - PROGRESS: at 32.40% examples, 404409 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:22,632 : INFO : EPOCH 3 - PROGRESS: at 70.39% examples, 428225 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:23,477 : INFO : EPOCH 3: training on 1788017 raw words (1241916 effective words) took 2.9s, 430041 effective words/s\n",
            "2025-10-17 14:09:24,530 : INFO : EPOCH 4 - PROGRESS: at 30.17% examples, 368538 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:25,553 : INFO : EPOCH 4 - PROGRESS: at 65.36% examples, 394981 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:26,553 : INFO : EPOCH 4 - PROGRESS: at 97.77% examples, 397862 words/s, in_qsize 4, out_qsize 0\n",
            "2025-10-17 14:09:26,627 : INFO : EPOCH 4: training on 1788017 raw words (1242769 effective words) took 3.1s, 396863 effective words/s\n",
            "2025-10-17 14:09:26,628 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211826 effective words) took 14.6s, 424702 effective words/s', 'datetime': '2025-10-17T14:09:26.628182', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:09:26,628 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:09:26.628908', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:09:26,631 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:09:26,745 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #20: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 15.386495033899942, 'train_time_std': 0.24339972917902924}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:09:27,087 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:09:27,088 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:09:27,145 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:09:27.145629', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:27,146 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:09:27.146129', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:27,230 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:09:27,235 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:09:27,236 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:09:27.236203', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:27,360 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:09:27,360 : INFO : resetting layer weights\n",
            "2025-10-17 14:09:27,370 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:09:27.370155', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:09:27,370 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:09:27.370741', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:09:28,406 : INFO : EPOCH 0 - PROGRESS: at 30.17% examples, 374317 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:29,417 : INFO : EPOCH 0 - PROGRESS: at 66.48% examples, 407220 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:30,429 : INFO : EPOCH 0 - PROGRESS: at 99.44% examples, 406404 words/s, in_qsize 1, out_qsize 1\n",
            "2025-10-17 14:09:30,436 : INFO : EPOCH 0: training on 1788017 raw words (1243054 effective words) took 3.0s, 407746 effective words/s\n",
            "2025-10-17 14:09:31,480 : INFO : EPOCH 1 - PROGRESS: at 27.37% examples, 338799 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:32,499 : INFO : EPOCH 1 - PROGRESS: at 60.34% examples, 367395 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:33,530 : INFO : EPOCH 1 - PROGRESS: at 90.50% examples, 366415 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:33,850 : INFO : EPOCH 1: training on 1788017 raw words (1242462 effective words) took 3.4s, 365915 effective words/s\n",
            "2025-10-17 14:09:34,880 : INFO : EPOCH 2 - PROGRESS: at 30.17% examples, 377158 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:35,885 : INFO : EPOCH 2 - PROGRESS: at 63.69% examples, 392421 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:36,898 : INFO : EPOCH 2 - PROGRESS: at 93.30% examples, 382854 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:37,092 : INFO : EPOCH 2: training on 1788017 raw words (1242325 effective words) took 3.2s, 385295 effective words/s\n",
            "2025-10-17 14:09:38,132 : INFO : EPOCH 3 - PROGRESS: at 29.05% examples, 359402 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:39,177 : INFO : EPOCH 3 - PROGRESS: at 60.34% examples, 362761 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:40,190 : INFO : EPOCH 3 - PROGRESS: at 93.30% examples, 376426 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:40,399 : INFO : EPOCH 3: training on 1788017 raw words (1242287 effective words) took 3.3s, 377447 effective words/s\n",
            "2025-10-17 14:09:41,445 : INFO : EPOCH 4 - PROGRESS: at 30.17% examples, 371115 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:42,475 : INFO : EPOCH 4 - PROGRESS: at 64.80% examples, 391606 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:43,478 : INFO : EPOCH 4 - PROGRESS: at 96.09% examples, 390810 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:43,574 : INFO : EPOCH 4: training on 1788017 raw words (1242765 effective words) took 3.2s, 393695 effective words/s\n",
            "2025-10-17 14:09:43,575 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212893 effective words) took 16.2s, 383411 effective words/s', 'datetime': '2025-10-17T14:09:43.575418', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:09:43,575 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:09:43.575841', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:09:43,577 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:09:43,668 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:09:43,987 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:09:43,987 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:09:44,045 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:09:44.045844', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:44,046 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:09:44.046440', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:44,128 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:09:44,131 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:09:44,132 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:09:44.132894', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:09:44,263 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:09:44,263 : INFO : resetting layer weights\n",
            "2025-10-17 14:09:44,272 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:09:44.272753', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:09:44,273 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:09:44.273304', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:09:45,303 : INFO : EPOCH 0 - PROGRESS: at 27.93% examples, 349748 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:46,303 : INFO : EPOCH 0 - PROGRESS: at 59.22% examples, 366488 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:47,309 : INFO : EPOCH 0 - PROGRESS: at 91.62% examples, 377764 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:47,568 : INFO : EPOCH 0: training on 1788017 raw words (1243025 effective words) took 3.3s, 379164 effective words/s\n",
            "2025-10-17 14:09:48,588 : INFO : EPOCH 1 - PROGRESS: at 29.05% examples, 367102 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:49,605 : INFO : EPOCH 1 - PROGRESS: at 62.57% examples, 384636 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:50,621 : INFO : EPOCH 1 - PROGRESS: at 94.97% examples, 389032 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:09:50,768 : INFO : EPOCH 1: training on 1788017 raw words (1242160 effective words) took 3.2s, 390230 effective words/s\n",
            "2025-10-17 14:09:51,792 : INFO : EPOCH 2 - PROGRESS: at 27.93% examples, 351823 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:52,802 : INFO : EPOCH 2 - PROGRESS: at 60.89% examples, 375381 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:53,812 : INFO : EPOCH 2 - PROGRESS: at 96.09% examples, 395007 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:53,911 : INFO : EPOCH 2: training on 1788017 raw words (1242329 effective words) took 3.1s, 397471 effective words/s\n",
            "2025-10-17 14:09:54,948 : INFO : EPOCH 3 - PROGRESS: at 30.73% examples, 380310 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:09:55,952 : INFO : EPOCH 3 - PROGRESS: at 65.92% examples, 404775 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:56,947 : INFO : EPOCH 3: training on 1788017 raw words (1242755 effective words) took 3.0s, 411713 effective words/s\n",
            "2025-10-17 14:09:57,986 : INFO : EPOCH 4 - PROGRESS: at 29.61% examples, 367031 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:09:58,999 : INFO : EPOCH 4 - PROGRESS: at 62.57% examples, 382287 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:00,008 : INFO : EPOCH 4 - PROGRESS: at 94.41% examples, 385933 words/s, in_qsize 5, out_qsize 1\n",
            "2025-10-17 14:10:00,147 : INFO : EPOCH 4: training on 1788017 raw words (1242776 effective words) took 3.2s, 390531 effective words/s\n",
            "2025-10-17 14:10:00,147 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6213045 effective words) took 15.9s, 391394 effective words/s', 'datetime': '2025-10-17T14:10:00.147880', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:10:00,148 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:10:00.148461', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:10:00,152 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:10:00,251 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:10:00,562 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:10:00,562 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:10:00,623 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:10:00.623837', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:00,624 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:10:00.624506', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:00,740 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:10:00,743 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:10:00,743 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:10:00.743962', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:00,885 : INFO : estimated required memory for 20167 words and 100 dimensions: 26217100 bytes\n",
            "2025-10-17 14:10:00,885 : INFO : resetting layer weights\n",
            "2025-10-17 14:10:00,901 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:10:00.900979', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:10:00,901 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:10:00.901631', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:10:01,928 : INFO : EPOCH 0 - PROGRESS: at 30.17% examples, 378448 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:02,933 : INFO : EPOCH 0 - PROGRESS: at 63.69% examples, 393010 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:03,951 : INFO : EPOCH 0 - PROGRESS: at 94.41% examples, 387333 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:04,144 : INFO : EPOCH 0: training on 1788017 raw words (1242357 effective words) took 3.2s, 385097 effective words/s\n",
            "2025-10-17 14:10:05,188 : INFO : EPOCH 1 - PROGRESS: at 30.73% examples, 377380 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:06,195 : INFO : EPOCH 1 - PROGRESS: at 67.04% examples, 409216 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:07,134 : INFO : EPOCH 1: training on 1788017 raw words (1242224 effective words) took 3.0s, 417763 effective words/s\n",
            "2025-10-17 14:10:08,204 : INFO : EPOCH 2 - PROGRESS: at 27.93% examples, 336913 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:09,213 : INFO : EPOCH 2 - PROGRESS: at 60.89% examples, 367400 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:10,220 : INFO : EPOCH 2 - PROGRESS: at 94.41% examples, 382814 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:10,384 : INFO : EPOCH 2: training on 1788017 raw words (1242258 effective words) took 3.2s, 384512 effective words/s\n",
            "2025-10-17 14:10:11,420 : INFO : EPOCH 3 - PROGRESS: at 31.28% examples, 386604 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:12,459 : INFO : EPOCH 3 - PROGRESS: at 61.45% examples, 370970 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:13,465 : INFO : EPOCH 3 - PROGRESS: at 89.39% examples, 362807 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:13,793 : INFO : EPOCH 3: training on 1788017 raw words (1242351 effective words) took 3.4s, 366159 effective words/s\n",
            "2025-10-17 14:10:14,811 : INFO : EPOCH 4 - PROGRESS: at 27.93% examples, 353826 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:15,827 : INFO : EPOCH 4 - PROGRESS: at 56.42% examples, 349161 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:16,827 : INFO : EPOCH 4 - PROGRESS: at 90.50% examples, 373286 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:17,129 : INFO : EPOCH 4: training on 1788017 raw words (1242397 effective words) took 3.3s, 374354 effective words/s\n",
            "2025-10-17 14:10:17,130 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211587 effective words) took 16.2s, 382770 effective words/s', 'datetime': '2025-10-17T14:10:17.130226', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:10:17,131 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:10:17.131133', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:10:17,134 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:10:17,230 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #21: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 16.83411478996277, 'train_time_std': 0.1844806263072886}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:10:17,582 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:10:17,582 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:10:17,645 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:10:17.645802', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:17,646 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:10:17.646306', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:17,781 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:10:17,784 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:10:17,785 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:10:17.785379', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:17,801 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:10:18,437 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:10:18,572 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:10:18,572 : INFO : resetting layer weights\n",
            "2025-10-17 14:10:18,581 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:10:18.581068', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:10:18,581 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:10:18,582 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:10:18.581992', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:10:19,604 : INFO : EPOCH 0 - PROGRESS: at 13.97% examples, 176266 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:20,648 : INFO : EPOCH 0 - PROGRESS: at 31.84% examples, 195782 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:21,656 : INFO : EPOCH 0 - PROGRESS: at 48.60% examples, 199393 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:22,666 : INFO : EPOCH 0 - PROGRESS: at 66.48% examples, 202967 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:23,753 : INFO : EPOCH 0 - PROGRESS: at 82.12% examples, 198077 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:24,772 : INFO : EPOCH 0 - PROGRESS: at 98.88% examples, 199208 words/s, in_qsize 2, out_qsize 1\n",
            "2025-10-17 14:10:24,833 : INFO : EPOCH 0: training on 1788017 raw words (1242132 effective words) took 6.2s, 199257 effective words/s\n",
            "2025-10-17 14:10:25,928 : INFO : EPOCH 1 - PROGRESS: at 14.53% examples, 170866 words/s, in_qsize 5, out_qsize 1\n",
            "2025-10-17 14:10:26,961 : INFO : EPOCH 1 - PROGRESS: at 29.61% examples, 177200 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:28,018 : INFO : EPOCH 1 - PROGRESS: at 46.37% examples, 183688 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:29,055 : INFO : EPOCH 1 - PROGRESS: at 61.45% examples, 181620 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:30,085 : INFO : EPOCH 1 - PROGRESS: at 74.86% examples, 177864 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:31,091 : INFO : EPOCH 1 - PROGRESS: at 89.94% examples, 179341 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:31,837 : INFO : EPOCH 1: training on 1788017 raw words (1242474 effective words) took 7.0s, 177803 effective words/s\n",
            "2025-10-17 14:10:32,891 : INFO : EPOCH 2 - PROGRESS: at 11.73% examples, 144758 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:33,919 : INFO : EPOCH 2 - PROGRESS: at 28.49% examples, 175183 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:35,006 : INFO : EPOCH 2 - PROGRESS: at 44.13% examples, 176139 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:36,028 : INFO : EPOCH 2 - PROGRESS: at 61.45% examples, 183282 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:37,030 : INFO : EPOCH 2 - PROGRESS: at 78.21% examples, 188172 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:38,106 : INFO : EPOCH 2 - PROGRESS: at 94.97% examples, 189104 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:38,420 : INFO : EPOCH 2: training on 1788017 raw words (1242327 effective words) took 6.6s, 189325 effective words/s\n",
            "2025-10-17 14:10:39,459 : INFO : EPOCH 3 - PROGRESS: at 14.53% examples, 180001 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:40,465 : INFO : EPOCH 3 - PROGRESS: at 30.73% examples, 191035 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:41,495 : INFO : EPOCH 3 - PROGRESS: at 47.49% examples, 194798 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:42,497 : INFO : EPOCH 3 - PROGRESS: at 63.69% examples, 194870 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:43,511 : INFO : EPOCH 3 - PROGRESS: at 79.33% examples, 194535 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:44,610 : INFO : EPOCH 3 - PROGRESS: at 96.09% examples, 193563 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:44,849 : INFO : EPOCH 3: training on 1788017 raw words (1242440 effective words) took 6.4s, 193719 effective words/s\n",
            "2025-10-17 14:10:45,868 : INFO : EPOCH 4 - PROGRESS: at 13.97% examples, 176962 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:46,869 : INFO : EPOCH 4 - PROGRESS: at 29.61% examples, 187510 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:47,875 : INFO : EPOCH 4 - PROGRESS: at 45.81% examples, 191135 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:48,889 : INFO : EPOCH 4 - PROGRESS: at 60.89% examples, 188249 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:49,913 : INFO : EPOCH 4 - PROGRESS: at 77.09% examples, 190202 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:50,955 : INFO : EPOCH 4 - PROGRESS: at 94.41% examples, 192849 words/s, in_qsize 5, out_qsize 1\n",
            "2025-10-17 14:10:51,241 : INFO : EPOCH 4: training on 1788017 raw words (1242211 effective words) took 6.4s, 194873 effective words/s\n",
            "2025-10-17 14:10:51,242 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211584 effective words) took 32.7s, 190191 effective words/s', 'datetime': '2025-10-17T14:10:51.242447', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:10:51,243 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:10:51.242982', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:10:51,245 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:10:51,339 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:10:51,714 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:10:51,715 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:10:51,776 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:10:51.776066', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:51,776 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:10:51.776704', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:51,877 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:10:51,881 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:10:51,881 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:10:51.881691', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:10:51,895 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:10:52,625 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:10:52,787 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:10:52,787 : INFO : resetting layer weights\n",
            "2025-10-17 14:10:52,799 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:10:52.799164', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:10:52,799 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:10:52,800 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:10:52.800571', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:10:53,847 : INFO : EPOCH 0 - PROGRESS: at 12.29% examples, 152295 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:54,851 : INFO : EPOCH 0 - PROGRESS: at 28.49% examples, 177704 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:55,875 : INFO : EPOCH 0 - PROGRESS: at 45.25% examples, 185852 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:10:56,920 : INFO : EPOCH 0 - PROGRESS: at 62.01% examples, 187826 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:58,023 : INFO : EPOCH 0 - PROGRESS: at 78.21% examples, 186923 words/s, in_qsize 4, out_qsize 1\n",
            "2025-10-17 14:10:59,055 : INFO : EPOCH 0 - PROGRESS: at 93.30% examples, 186024 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:10:59,602 : INFO : EPOCH 0: training on 1788017 raw words (1242503 effective words) took 6.8s, 183142 effective words/s\n",
            "2025-10-17 14:11:00,651 : INFO : EPOCH 1 - PROGRESS: at 13.41% examples, 165275 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:01,672 : INFO : EPOCH 1 - PROGRESS: at 25.70% examples, 158794 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:02,738 : INFO : EPOCH 1 - PROGRESS: at 40.22% examples, 162429 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:03,778 : INFO : EPOCH 1 - PROGRESS: at 56.98% examples, 170905 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:04,787 : INFO : EPOCH 1 - PROGRESS: at 73.74% examples, 177523 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:05,819 : INFO : EPOCH 1 - PROGRESS: at 90.50% examples, 181777 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:06,418 : INFO : EPOCH 1: training on 1788017 raw words (1242745 effective words) took 6.8s, 182825 effective words/s\n",
            "2025-10-17 14:11:07,479 : INFO : EPOCH 2 - PROGRESS: at 15.64% examples, 190050 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:08,521 : INFO : EPOCH 2 - PROGRESS: at 36.31% examples, 218916 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:09,536 : INFO : EPOCH 2 - PROGRESS: at 55.31% examples, 222728 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:10,550 : INFO : EPOCH 2 - PROGRESS: at 73.74% examples, 222767 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:11,625 : INFO : EPOCH 2 - PROGRESS: at 92.18% examples, 220902 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:12,042 : INFO : EPOCH 2: training on 1788017 raw words (1242140 effective words) took 5.6s, 221509 effective words/s\n",
            "2025-10-17 14:11:13,063 : INFO : EPOCH 3 - PROGRESS: at 13.97% examples, 176436 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:14,081 : INFO : EPOCH 3 - PROGRESS: at 31.28% examples, 195061 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:15,091 : INFO : EPOCH 3 - PROGRESS: at 48.04% examples, 198993 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:16,093 : INFO : EPOCH 3 - PROGRESS: at 67.04% examples, 206470 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:17,094 : INFO : EPOCH 3 - PROGRESS: at 85.47% examples, 211199 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:17,872 : INFO : EPOCH 3: training on 1788017 raw words (1242683 effective words) took 5.8s, 213767 effective words/s\n",
            "2025-10-17 14:11:18,904 : INFO : EPOCH 4 - PROGRESS: at 14.53% examples, 181326 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:19,937 : INFO : EPOCH 4 - PROGRESS: at 33.52% examples, 206132 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:20,970 : INFO : EPOCH 4 - PROGRESS: at 52.51% examples, 213328 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:21,980 : INFO : EPOCH 4 - PROGRESS: at 71.51% examples, 217300 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:22,985 : INFO : EPOCH 4 - PROGRESS: at 89.39% examples, 218250 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:23,563 : INFO : EPOCH 4: training on 1788017 raw words (1242683 effective words) took 5.7s, 218949 effective words/s\n",
            "2025-10-17 14:11:23,564 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212754 effective words) took 30.8s, 201956 effective words/s', 'datetime': '2025-10-17T14:11:23.564211', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:11:23,564 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:11:23.564727', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:11:23,571 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:11:23,666 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:11:23,991 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:11:23,991 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:11:24,049 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:11:24.049695', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:11:24,050 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:11:24.050416', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:11:24,134 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:11:24,138 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:11:24,139 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:11:24.139185', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:11:24,147 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:11:24,716 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:11:24,843 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:11:24,844 : INFO : resetting layer weights\n",
            "2025-10-17 14:11:24,854 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:11:24.854154', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:11:24,854 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:11:24,855 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:11:24.855162', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:11:25,928 : INFO : EPOCH 0 - PROGRESS: at 16.76% examples, 201655 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:26,965 : INFO : EPOCH 0 - PROGRESS: at 35.20% examples, 211918 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:27,970 : INFO : EPOCH 0 - PROGRESS: at 53.63% examples, 216444 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:28,987 : INFO : EPOCH 0 - PROGRESS: at 73.18% examples, 221115 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:30,062 : INFO : EPOCH 0 - PROGRESS: at 90.50% examples, 217019 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:30,565 : INFO : EPOCH 0: training on 1788017 raw words (1242521 effective words) took 5.7s, 218279 effective words/s\n",
            "2025-10-17 14:11:31,600 : INFO : EPOCH 1 - PROGRESS: at 16.76% examples, 208940 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:32,614 : INFO : EPOCH 1 - PROGRESS: at 35.75% examples, 221474 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:33,633 : INFO : EPOCH 1 - PROGRESS: at 53.07% examples, 217583 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:34,676 : INFO : EPOCH 1 - PROGRESS: at 71.51% examples, 217264 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:35,734 : INFO : EPOCH 1 - PROGRESS: at 88.83% examples, 214545 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:36,390 : INFO : EPOCH 1: training on 1788017 raw words (1242707 effective words) took 5.8s, 213959 effective words/s\n",
            "2025-10-17 14:11:37,405 : INFO : EPOCH 2 - PROGRESS: at 14.53% examples, 184185 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:38,420 : INFO : EPOCH 2 - PROGRESS: at 31.84% examples, 199319 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:39,447 : INFO : EPOCH 2 - PROGRESS: at 50.84% examples, 209563 words/s, in_qsize 6, out_qsize 1\n",
            "2025-10-17 14:11:40,466 : INFO : EPOCH 2 - PROGRESS: at 70.39% examples, 215394 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:41,468 : INFO : EPOCH 2 - PROGRESS: at 88.83% examples, 218273 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:42,084 : INFO : EPOCH 2: training on 1788017 raw words (1242481 effective words) took 5.7s, 218795 effective words/s\n",
            "2025-10-17 14:11:43,104 : INFO : EPOCH 3 - PROGRESS: at 15.08% examples, 190847 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:44,139 : INFO : EPOCH 3 - PROGRESS: at 32.40% examples, 200653 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:45,226 : INFO : EPOCH 3 - PROGRESS: at 50.84% examples, 204066 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:46,244 : INFO : EPOCH 3 - PROGRESS: at 68.72% examples, 206061 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:47,274 : INFO : EPOCH 3 - PROGRESS: at 87.15% examples, 209637 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:48,018 : INFO : EPOCH 3: training on 1788017 raw words (1242469 effective words) took 5.9s, 209978 effective words/s\n",
            "2025-10-17 14:11:49,050 : INFO : EPOCH 4 - PROGRESS: at 15.08% examples, 188334 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:50,100 : INFO : EPOCH 4 - PROGRESS: at 32.96% examples, 201212 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:51,142 : INFO : EPOCH 4 - PROGRESS: at 50.28% examples, 202814 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:52,197 : INFO : EPOCH 4 - PROGRESS: at 68.16% examples, 203278 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:53,211 : INFO : EPOCH 4 - PROGRESS: at 85.47% examples, 205261 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:54,086 : INFO : EPOCH 4: training on 1788017 raw words (1241945 effective words) took 6.1s, 205215 effective words/s\n",
            "2025-10-17 14:11:54,087 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212123 effective words) took 29.2s, 212515 effective words/s', 'datetime': '2025-10-17T14:11:54.087049', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:11:54,087 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:11:54.087485', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:11:54,094 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:11:54,180 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #22: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 32.31974673271179, 'train_time_std': 1.4650705962983597}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:11:54,481 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:11:54,481 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:11:54,561 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:11:54.561414', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:11:54,562 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:11:54.561987', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:11:54,710 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:11:54,713 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:11:54,714 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:11:54.714564', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:11:54,723 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:11:55,226 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:11:55,342 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:11:55,343 : INFO : resetting layer weights\n",
            "2025-10-17 14:11:55,350 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:11:55.350857', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:11:55,351 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:11:55,351 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:11:55.351707', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:11:56,394 : INFO : EPOCH 0 - PROGRESS: at 16.76% examples, 206954 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:11:57,411 : INFO : EPOCH 0 - PROGRESS: at 35.75% examples, 220193 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:58,435 : INFO : EPOCH 0 - PROGRESS: at 54.19% examples, 220689 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:11:59,436 : INFO : EPOCH 0 - PROGRESS: at 72.07% examples, 220153 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:00,448 : INFO : EPOCH 0 - PROGRESS: at 87.15% examples, 213402 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:01,251 : INFO : EPOCH 0: training on 1788017 raw words (1242424 effective words) took 5.9s, 211135 effective words/s\n",
            "2025-10-17 14:12:02,273 : INFO : EPOCH 1 - PROGRESS: at 12.85% examples, 162734 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:03,291 : INFO : EPOCH 1 - PROGRESS: at 29.05% examples, 182114 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:04,330 : INFO : EPOCH 1 - PROGRESS: at 44.69% examples, 183425 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:05,375 : INFO : EPOCH 1 - PROGRESS: at 61.45% examples, 186126 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:06,429 : INFO : EPOCH 1 - PROGRESS: at 76.54% examples, 184757 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:07,481 : INFO : EPOCH 1 - PROGRESS: at 92.74% examples, 185778 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:07,976 : INFO : EPOCH 1: training on 1788017 raw words (1242804 effective words) took 6.7s, 185302 effective words/s\n",
            "2025-10-17 14:12:09,049 : INFO : EPOCH 2 - PROGRESS: at 11.73% examples, 141980 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:10,072 : INFO : EPOCH 2 - PROGRESS: at 23.46% examples, 143106 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:11,127 : INFO : EPOCH 2 - PROGRESS: at 40.78% examples, 163822 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:12,176 : INFO : EPOCH 2 - PROGRESS: at 58.10% examples, 173018 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:13,178 : INFO : EPOCH 2 - PROGRESS: at 74.30% examples, 178150 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:14,210 : INFO : EPOCH 2 - PROGRESS: at 90.50% examples, 181109 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:14,807 : INFO : EPOCH 2: training on 1788017 raw words (1241761 effective words) took 6.8s, 182232 effective words/s\n",
            "2025-10-17 14:12:15,847 : INFO : EPOCH 3 - PROGRESS: at 13.97% examples, 173202 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:16,856 : INFO : EPOCH 3 - PROGRESS: at 29.61% examples, 184645 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:17,879 : INFO : EPOCH 3 - PROGRESS: at 44.69% examples, 183865 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:18,890 : INFO : EPOCH 3 - PROGRESS: at 59.78% examples, 183047 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:19,976 : INFO : EPOCH 3 - PROGRESS: at 76.54% examples, 184834 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:21,002 : INFO : EPOCH 3 - PROGRESS: at 91.62% examples, 184537 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:21,480 : INFO : EPOCH 3: training on 1788017 raw words (1242331 effective words) took 6.7s, 186648 effective words/s\n",
            "2025-10-17 14:12:22,507 : INFO : EPOCH 4 - PROGRESS: at 13.97% examples, 175276 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:23,575 : INFO : EPOCH 4 - PROGRESS: at 30.73% examples, 186523 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:24,608 : INFO : EPOCH 4 - PROGRESS: at 47.49% examples, 191544 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:25,652 : INFO : EPOCH 4 - PROGRESS: at 63.69% examples, 190404 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:26,652 : INFO : EPOCH 4 - PROGRESS: at 79.33% examples, 191465 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:27,655 : INFO : EPOCH 4 - PROGRESS: at 93.30% examples, 188397 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:28,084 : INFO : EPOCH 4: training on 1788017 raw words (1242481 effective words) took 6.6s, 188578 effective words/s\n",
            "2025-10-17 14:12:28,085 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6211801 effective words) took 32.7s, 189771 effective words/s', 'datetime': '2025-10-17T14:12:28.085499', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:12:28,085 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:12:28.085970', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:12:28,093 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:12:28,188 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:12:28,550 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:12:28,551 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:12:28,627 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:12:28.627884', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:12:28,628 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:12:28.628463', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:12:28,756 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:12:28,759 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:12:28,760 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:12:28.760527', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:12:28,771 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:12:29,335 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:12:29,459 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:12:29,460 : INFO : resetting layer weights\n",
            "2025-10-17 14:12:29,470 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:12:29.470918', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:12:29,471 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:12:29,472 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:12:29.472087', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:12:30,500 : INFO : EPOCH 0 - PROGRESS: at 13.97% examples, 175207 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:31,534 : INFO : EPOCH 0 - PROGRESS: at 30.17% examples, 186768 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:32,594 : INFO : EPOCH 0 - PROGRESS: at 46.93% examples, 189719 words/s, in_qsize 5, out_qsize 1\n",
            "2025-10-17 14:12:33,611 : INFO : EPOCH 0 - PROGRESS: at 63.69% examples, 191957 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:34,617 : INFO : EPOCH 0 - PROGRESS: at 79.33% examples, 192446 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:35,735 : INFO : EPOCH 0 - PROGRESS: at 96.09% examples, 191260 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:35,953 : INFO : EPOCH 0: training on 1788017 raw words (1241493 effective words) took 6.5s, 192053 effective words/s\n",
            "2025-10-17 14:12:37,004 : INFO : EPOCH 1 - PROGRESS: at 12.29% examples, 151595 words/s, in_qsize 4, out_qsize 1\n",
            "2025-10-17 14:12:38,075 : INFO : EPOCH 1 - PROGRESS: at 27.93% examples, 168202 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:39,087 : INFO : EPOCH 1 - PROGRESS: at 45.25% examples, 182134 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:40,095 : INFO : EPOCH 1 - PROGRESS: at 64.25% examples, 193478 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:41,110 : INFO : EPOCH 1 - PROGRESS: at 79.33% examples, 192012 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:42,115 : INFO : EPOCH 1 - PROGRESS: at 92.18% examples, 186555 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:42,596 : INFO : EPOCH 1: training on 1788017 raw words (1241795 effective words) took 6.6s, 187418 effective words/s\n",
            "2025-10-17 14:12:43,676 : INFO : EPOCH 2 - PROGRESS: at 13.97% examples, 166517 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:44,712 : INFO : EPOCH 2 - PROGRESS: at 30.17% examples, 181913 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:45,736 : INFO : EPOCH 2 - PROGRESS: at 46.37% examples, 186395 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:46,740 : INFO : EPOCH 2 - PROGRESS: at 64.25% examples, 193364 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:47,753 : INFO : EPOCH 2 - PROGRESS: at 81.01% examples, 196098 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:48,769 : INFO : EPOCH 2 - PROGRESS: at 96.09% examples, 194136 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:48,965 : INFO : EPOCH 2: training on 1788017 raw words (1242023 effective words) took 6.4s, 195544 effective words/s\n",
            "2025-10-17 14:12:50,024 : INFO : EPOCH 3 - PROGRESS: at 13.41% examples, 163317 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:51,054 : INFO : EPOCH 3 - PROGRESS: at 29.05% examples, 177633 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:52,111 : INFO : EPOCH 3 - PROGRESS: at 45.81% examples, 183714 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:53,121 : INFO : EPOCH 3 - PROGRESS: at 62.57% examples, 187825 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:54,147 : INFO : EPOCH 3 - PROGRESS: at 79.33% examples, 191131 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:55,170 : INFO : EPOCH 3 - PROGRESS: at 95.53% examples, 192009 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:55,377 : INFO : EPOCH 3: training on 1788017 raw words (1242210 effective words) took 6.4s, 194232 effective words/s\n",
            "2025-10-17 14:12:56,422 : INFO : EPOCH 4 - PROGRESS: at 12.29% examples, 152668 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:57,481 : INFO : EPOCH 4 - PROGRESS: at 27.37% examples, 166585 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:12:58,482 : INFO : EPOCH 4 - PROGRESS: at 43.58% examples, 177445 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:12:59,484 : INFO : EPOCH 4 - PROGRESS: at 58.66% examples, 178820 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:00,496 : INFO : EPOCH 4 - PROGRESS: at 75.42% examples, 184059 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:01,543 : INFO : EPOCH 4 - PROGRESS: at 93.30% examples, 188798 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:01,926 : INFO : EPOCH 4: training on 1788017 raw words (1242646 effective words) took 6.5s, 190283 effective words/s\n",
            "2025-10-17 14:13:01,927 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6210167 effective words) took 32.5s, 191349 effective words/s', 'datetime': '2025-10-17T14:13:01.927448', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:13:01,927 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:13:01.927974', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n",
            "2025-10-17 14:13:01,937 : INFO : collecting all words and their counts\n",
            "2025-10-17 14:13:02,028 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-17 14:13:02,367 : INFO : collected 73167 word types from a corpus of 1788017 raw words and 179 sentences\n",
            "2025-10-17 14:13:02,367 : INFO : Creating a fresh vocabulary\n",
            "2025-10-17 14:13:02,423 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 20167 unique words (27.56% of original 73167, drops 53000)', 'datetime': '2025-10-17T14:13:02.423527', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:13:02,424 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1703716 word corpus (95.29% of original 1788017, drops 84301)', 'datetime': '2025-10-17T14:13:02.424032', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:13:02,517 : INFO : deleting the raw counts dictionary of 73167 items\n",
            "2025-10-17 14:13:02,520 : INFO : sample=0.001 downsamples 38 most-common words\n",
            "2025-10-17 14:13:02,521 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1242287.3013176506 word corpus (72.9%% of prior 1703716)', 'datetime': '2025-10-17T14:13:02.521266', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'prepare_vocab'}\n",
            "2025-10-17 14:13:02,528 : INFO : constructing a huffman tree from 20167 words\n",
            "2025-10-17 14:13:03,028 : INFO : built huffman tree with maximum node depth 18\n",
            "2025-10-17 14:13:03,161 : INFO : estimated required memory for 20167 words and 100 dimensions: 38317300 bytes\n",
            "2025-10-17 14:13:03,161 : INFO : resetting layer weights\n",
            "2025-10-17 14:13:03,170 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-17T14:13:03.170786', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'build_vocab'}\n",
            "2025-10-17 14:13:03,171 : WARNING : Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n",
            "2025-10-17 14:13:03,172 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 20167 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-10-17T14:13:03.172251', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:13:04,235 : INFO : EPOCH 0 - PROGRESS: at 15.64% examples, 189763 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:05,246 : INFO : EPOCH 0 - PROGRESS: at 32.96% examples, 202134 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:06,290 : INFO : EPOCH 0 - PROGRESS: at 50.84% examples, 205533 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:07,307 : INFO : EPOCH 0 - PROGRESS: at 68.72% examples, 207266 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:08,341 : INFO : EPOCH 0 - PROGRESS: at 86.03% examples, 207748 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:09,124 : INFO : EPOCH 0: training on 1788017 raw words (1242495 effective words) took 5.9s, 209335 effective words/s\n",
            "2025-10-17 14:13:10,140 : INFO : EPOCH 1 - PROGRESS: at 15.64% examples, 198483 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:11,149 : INFO : EPOCH 1 - PROGRESS: at 34.08% examples, 213904 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:12,156 : INFO : EPOCH 1 - PROGRESS: at 53.63% examples, 222518 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:13,162 : INFO : EPOCH 1 - PROGRESS: at 72.07% examples, 222894 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:14,192 : INFO : EPOCH 1 - PROGRESS: at 91.06% examples, 224405 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:14,659 : INFO : EPOCH 1: training on 1788017 raw words (1242810 effective words) took 5.5s, 225166 effective words/s\n",
            "2025-10-17 14:13:15,733 : INFO : EPOCH 2 - PROGRESS: at 15.64% examples, 187905 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:16,767 : INFO : EPOCH 2 - PROGRESS: at 31.84% examples, 192060 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:17,789 : INFO : EPOCH 2 - PROGRESS: at 48.60% examples, 195910 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:18,800 : INFO : EPOCH 2 - PROGRESS: at 64.25% examples, 193629 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:19,810 : INFO : EPOCH 2 - PROGRESS: at 81.56% examples, 197598 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:20,832 : INFO : EPOCH 2 - PROGRESS: at 97.77% examples, 197577 words/s, in_qsize 4, out_qsize 0\n",
            "2025-10-17 14:13:20,925 : INFO : EPOCH 2: training on 1788017 raw words (1242430 effective words) took 6.2s, 198801 effective words/s\n",
            "2025-10-17 14:13:21,977 : INFO : EPOCH 3 - PROGRESS: at 15.08% examples, 184765 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:22,995 : INFO : EPOCH 3 - PROGRESS: at 32.96% examples, 202376 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:24,000 : INFO : EPOCH 3 - PROGRESS: at 51.40% examples, 210786 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:25,004 : INFO : EPOCH 3 - PROGRESS: at 70.39% examples, 215430 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:26,067 : INFO : EPOCH 3 - PROGRESS: at 89.94% examples, 218479 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:26,584 : INFO : EPOCH 3: training on 1788017 raw words (1242659 effective words) took 5.6s, 220192 effective words/s\n",
            "2025-10-17 14:13:27,613 : INFO : EPOCH 4 - PROGRESS: at 14.53% examples, 181620 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:28,649 : INFO : EPOCH 4 - PROGRESS: at 32.40% examples, 199258 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:29,650 : INFO : EPOCH 4 - PROGRESS: at 49.72% examples, 204342 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-17 14:13:30,731 : INFO : EPOCH 4 - PROGRESS: at 68.16% examples, 204769 words/s, in_qsize 4, out_qsize 1\n",
            "2025-10-17 14:13:31,741 : INFO : EPOCH 4 - PROGRESS: at 85.47% examples, 206665 words/s, in_qsize 6, out_qsize 0\n",
            "2025-10-17 14:13:32,571 : INFO : EPOCH 4: training on 1788017 raw words (1242527 effective words) took 6.0s, 208019 effective words/s\n",
            "2025-10-17 14:13:32,572 : INFO : Word2Vec lifecycle event {'msg': 'training on 8940085 raw words (6212921 effective words) took 29.4s, 211338 effective words/s', 'datetime': '2025-10-17T14:13:32.572334', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'train'}\n",
            "2025-10-17 14:13:32,573 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20167, vector_size=100, alpha=0.025>', 'datetime': '2025-10-17T14:13:32.573013', 'gensim': '4.3.3', 'python': '3.11.14 (main, Oct 10 2025, 10:21:20) [GCC 14.2.0]', 'platform': 'Linux-6.12.48+deb13-amd64-x86_64-with-glibc2.41', 'event': 'created'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec model #23: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 32.82801310221354, 'train_time_std': 1.547465817587142}\n",
            "   train_data  compute_loss  sg  hs  train_time_mean  train_time_std\n",
            "4        25kB          True   1   0         0.407955        0.011482\n",
            "5        25kB         False   1   0         0.409663        0.020925\n",
            "6        25kB          True   1   1         0.808133        0.040627\n",
            "7        25kB         False   1   1         0.805516        0.025558\n",
            "0        25kB          True   0   0         0.208782        0.009089\n",
            "1        25kB         False   0   0         0.195498        0.009373\n",
            "2        25kB          True   0   1         0.334870        0.013009\n",
            "3        25kB         False   0   1         0.388519        0.030624\n",
            "12        1MB          True   1   0         1.309636        0.012777\n",
            "13        1MB         False   1   0         1.258593        0.024880\n",
            "14        1MB          True   1   1         2.765448        0.102200\n",
            "15        1MB         False   1   1         2.701058        0.073750\n",
            "8         1MB          True   0   0         0.557157        0.038710\n",
            "9         1MB         False   0   0         0.530385        0.031533\n",
            "10        1MB          True   0   1         0.931949        0.008904\n",
            "11        1MB         False   0   1         0.942778        0.011840\n",
            "20       10MB          True   1   0        15.386495        0.243400\n",
            "21       10MB         False   1   0        16.834115        0.184481\n",
            "22       10MB          True   1   1        32.319747        1.465071\n",
            "23       10MB         False   1   1        32.828013        1.547466\n",
            "16       10MB          True   0   0         6.148723        0.073120\n",
            "17       10MB         False   0   0         5.517085        0.272598\n",
            "18       10MB          True   0   1        11.313425        0.222676\n",
            "19       10MB         False   0   1        10.859464        0.525564\n"
          ]
        }
      ],
      "source": [
        "# Temporarily reduce logging verbosity\n",
        "logging.root.level = logging.ERROR\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_time_values = []\n",
        "seed_val = 42\n",
        "sg_values = [0, 1]\n",
        "hs_values = [0, 1]\n",
        "\n",
        "fast = True\n",
        "if fast:\n",
        "    input_data_subset = input_data[:3]\n",
        "else:\n",
        "    input_data_subset = input_data\n",
        "\n",
        "\n",
        "for data in input_data_subset:\n",
        "    for sg_val in sg_values:\n",
        "        for hs_val in hs_values:\n",
        "            for loss_flag in [True, False]:\n",
        "                time_taken_list = []\n",
        "                for i in range(3):\n",
        "                    start_time = time.time()\n",
        "                    w2v_model = gensim.models.Word2Vec(\n",
        "                        data,\n",
        "                        compute_loss=loss_flag,\n",
        "                        sg=sg_val,\n",
        "                        hs=hs_val,\n",
        "                        seed=seed_val,\n",
        "                    )\n",
        "                    time_taken_list.append(time.time() - start_time)\n",
        "\n",
        "                time_taken_list = np.array(time_taken_list)\n",
        "                time_mean = np.mean(time_taken_list)\n",
        "                time_std = np.std(time_taken_list)\n",
        "\n",
        "                model_result = {\n",
        "                    'train_data': data.name,\n",
        "                    'compute_loss': loss_flag,\n",
        "                    'sg': sg_val,\n",
        "                    'hs': hs_val,\n",
        "                    'train_time_mean': time_mean,\n",
        "                    'train_time_std': time_std,\n",
        "                }\n",
        "                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n",
        "                train_time_values.append(model_result)\n",
        "\n",
        "train_times_table = pd.DataFrame(train_time_values)\n",
        "train_times_table = train_times_table.sort_values(\n",
        "    by=['train_data', 'sg', 'hs', 'compute_loss'],\n",
        "    ascending=[False, False, True, False],\n",
        ")\n",
        "print(train_times_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualising Word Embeddings\n",
        "---------------------------\n",
        "\n",
        "The word embeddings made by the model can be visualised by reducing\n",
        "dimensionality of the words to 2 dimensions using tSNE.\n",
        "\n",
        "Visualisations can be used to notice semantic and syntactic trends in the data.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by\n",
        "* Syntactic: words like run, running or cut, cutting lie close together.\n",
        "\n",
        "Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n",
        "\n",
        ".. Important::\n",
        "  The model used for the visualisation is trained on a small corpus. Thus\n",
        "  some of the relations might not be so clear.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        </script>\n",
              "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.1.1.min\"</script>\n",
              "        "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "linkText": "Export to plot.ly",
                "plotlyServerURL": "https://plot.ly",
                "showLink": false
              },
              "data": [
                {
                  "mode": "text",
                  "text": [
                    "the",
                    "to",
                    "of",
                    "in",
                    "and",
                    "he",
                    "is",
                    "for",
                    "on",
                    "said",
                    "that",
                    "has",
                    "says",
                    "was",
                    "have",
                    "it",
                    "be",
                    "are",
                    "with",
                    "will",
                    "at",
                    "mr",
                    "from",
                    "by",
                    "we",
                    "been",
                    "as",
                    "an",
                    "not",
                    "his",
                    "but",
                    "they",
                    "after",
                    "were",
                    "had",
                    "there",
                    "new",
                    "this",
                    "australia",
                    "australian",
                    "who",
                    "palestinian",
                    "people",
                    "their",
                    "government",
                    "two",
                    "up",
                    "south",
                    "us",
                    "which",
                    "year",
                    "one",
                    "about",
                    "out",
                    "if",
                    "also",
                    "more",
                    "when",
                    "its",
                    "into",
                    "would",
                    "first",
                    "last",
                    "against",
                    "israeli",
                    "minister",
                    "arafat",
                    "over",
                    "all",
                    "three",
                    "afghanistan",
                    "united",
                    "world",
                    "no",
                    "or",
                    "police",
                    "than",
                    "attacks",
                    "fire",
                    "before",
                    "some",
                    "security",
                    "day",
                    "you",
                    "states",
                    "could",
                    "them",
                    "say",
                    "today",
                    "now",
                    "told",
                    "time",
                    "any",
                    "laden",
                    "very",
                    "bin",
                    "just",
                    "can",
                    "sydney",
                    "what",
                    "still",
                    "company",
                    "president",
                    "man",
                    "four",
                    "taliban",
                    "killed",
                    "our",
                    "forces",
                    "al",
                    "around",
                    "being",
                    "days",
                    "west",
                    "old",
                    "other",
                    "officials",
                    "where",
                    "so",
                    "test",
                    "qaeda",
                    "israel",
                    "think",
                    "per",
                    "general",
                    "next",
                    "federal",
                    "force",
                    "cent",
                    "she",
                    "leader",
                    "yesterday",
                    "workers",
                    "take",
                    "him",
                    "hamas",
                    "under",
                    "state",
                    "those",
                    "years",
                    "meeting",
                    "bank",
                    "suicide",
                    "back",
                    "action",
                    "commission",
                    "made",
                    "down",
                    "morning",
                    "re",
                    "pakistan",
                    "international",
                    "city",
                    "attack",
                    "centre",
                    "group",
                    "afghan",
                    "members",
                    "while",
                    "military",
                    "well",
                    "number",
                    "through",
                    "qantas",
                    "five",
                    "local",
                    "called",
                    "area",
                    "union",
                    "gaza",
                    "week",
                    "national",
                    "since",
                    "wales",
                    "including",
                    "hours",
                    "september",
                    "another",
                    "east",
                    "night",
                    "report",
                    "off",
                    "north",
                    "should",
                    "get",
                    "second",
                    "go",
                    "earlier",
                    "war",
                    "staff",
                    "six",
                    "these",
                    "between",
                    "islamic",
                    "months",
                    "further",
                    "end",
                    "defence",
                    "do",
                    "sharon",
                    "near",
                    "team",
                    "foreign",
                    "power",
                    "areas",
                    "work",
                    "going",
                    "authority",
                    "because",
                    "way",
                    "eight",
                    "india",
                    "only",
                    "know",
                    "month",
                    "during",
                    "died",
                    "many",
                    "match",
                    "make",
                    "air",
                    "metres",
                    "left",
                    "claims",
                    "spokesman",
                    "ve",
                    "former",
                    "melbourne",
                    "northern",
                    "good",
                    "authorities",
                    "most",
                    "osama",
                    "support",
                    "prime",
                    "peace",
                    "like",
                    "set",
                    "ago",
                    "expected",
                    "saying",
                    "given",
                    "am",
                    "come",
                    "looking",
                    "militants",
                    "bora",
                    "tora",
                    "put",
                    "place",
                    "several",
                    "fighters",
                    "children",
                    "arrested",
                    "injured",
                    "found",
                    "river",
                    "royal",
                    "groups",
                    "africa",
                    "unions",
                    "christmas",
                    "troops",
                    "meanwhile",
                    "indian",
                    "child",
                    "hospital",
                    "terrorist",
                    "interim",
                    "part",
                    "reports",
                    "talks",
                    "official",
                    "whether",
                    "then",
                    "yasser",
                    "statement",
                    "leaders",
                    "economy",
                    "mountains",
                    "how",
                    "industrial",
                    "third",
                    "terrorism",
                    "senior",
                    "start",
                    "don",
                    "early",
                    "radio",
                    "john",
                    "hit",
                    "trying",
                    "weather",
                    "public",
                    "both",
                    "believe",
                    "family",
                    "pay",
                    "million",
                    "army",
                    "court",
                    "dr",
                    "long",
                    "best",
                    "control",
                    "help",
                    "however",
                    "lead",
                    "adelaide",
                    "asked",
                    "following",
                    "chief",
                    "pressure",
                    "agreement",
                    "does",
                    "service",
                    "firefighters",
                    "close",
                    "few",
                    "services",
                    "labor",
                    "play",
                    "better",
                    "community",
                    "taken",
                    "want",
                    "arrest",
                    "queensland",
                    "house",
                    "need",
                    "overnight",
                    "australians",
                    "high",
                    "confirmed",
                    "process",
                    "information",
                    "came",
                    "believed",
                    "williams",
                    "must",
                    "opposition",
                    "detainees",
                    "won",
                    "secretary",
                    "did",
                    "peter",
                    "party",
                    "held",
                    "damage",
                    "governor",
                    "maintenance",
                    "released",
                    "win",
                    "pentagon",
                    "possible",
                    "her",
                    "brought",
                    "hicks",
                    "much",
                    "shot",
                    "took",
                    "accused",
                    "nations",
                    "british",
                    "weekend",
                    "lot",
                    "violence",
                    "building",
                    "despite",
                    "council",
                    "return",
                    "got",
                    "airline",
                    "asylum",
                    "york",
                    "dead",
                    "kandahar",
                    "conditions",
                    "across",
                    "hill",
                    "winds",
                    "safety",
                    "even",
                    "such",
                    "change",
                    "cut",
                    "eastern",
                    "without",
                    "director",
                    "armed",
                    "working",
                    "aircraft",
                    "call",
                    "here",
                    "see",
                    "palestinians",
                    "december",
                    "economic",
                    "news",
                    "american",
                    "too",
                    "home",
                    "men",
                    "seekers",
                    "strip",
                    "lee",
                    "waugh",
                    "role",
                    "country",
                    "region",
                    "trade",
                    "emergency",
                    "crew",
                    "strong",
                    "race",
                    "captured",
                    "david",
                    "southern",
                    "fighting",
                    "continuing",
                    "fires",
                    "monday",
                    "far",
                    "anti",
                    "board",
                    "cricket",
                    "training",
                    "key",
                    "plans",
                    "bush",
                    "bureau",
                    "act",
                    "industry",
                    "george",
                    "head",
                    "past",
                    "water",
                    "charged",
                    "used",
                    "administration",
                    "received",
                    "offer",
                    "alliance",
                    "rate",
                    "zinni",
                    "health",
                    "least",
                    "leading",
                    "person",
                    "captain",
                    "your",
                    "town",
                    "boat",
                    "large",
                    "decision",
                    "stop",
                    "known",
                    "airport",
                    "operations",
                    "may",
                    "line",
                    "within",
                    "risk",
                    "use",
                    "downer",
                    "israelis",
                    "soldiers",
                    "major",
                    "britain",
                    "final",
                    "parliament",
                    "department",
                    "zealand",
                    "hundreds",
                    "issue",
                    "strikes",
                    "hih",
                    "station",
                    "legal",
                    "shane",
                    "plane",
                    "might",
                    "series",
                    "interest",
                    "un",
                    "laws",
                    "policy",
                    "right",
                    "ahead",
                    "hollingworth",
                    "tomorrow",
                    "network",
                    "pm",
                    "able",
                    "due",
                    "kabul",
                    "latest",
                    "death",
                    "homes",
                    "weapons",
                    "behind",
                    "great",
                    "coast",
                    "western",
                    "position",
                    "give",
                    "later",
                    "late",
                    "half",
                    "officers",
                    "my",
                    "taking",
                    "every",
                    "remain",
                    "campaign",
                    "seen",
                    "thought",
                    "bill",
                    "timor",
                    "special",
                    "side",
                    "failed",
                    "same",
                    "flight",
                    "along",
                    "jobs",
                    "storm",
                    "me",
                    "forced",
                    "life",
                    "others",
                    "continue",
                    "hard",
                    "event",
                    "abuse",
                    "cup",
                    "victory",
                    "jihad",
                    "guilty",
                    "point",
                    "towards",
                    "really",
                    "concerned",
                    "heard",
                    "already",
                    "territory",
                    "washington",
                    "deaths",
                    "mcgrath",
                    "helicopters",
                    "envoy",
                    "canyoning",
                    "capital",
                    "bus",
                    "bichel",
                    "november",
                    "likely",
                    "details",
                    "case",
                    "member",
                    "launched",
                    "innings",
                    "according",
                    "enough",
                    "bombings",
                    "weeks",
                    "countries",
                    "again",
                    "detention",
                    "move",
                    "woomera",
                    "seven",
                    "cabinet",
                    "bowler",
                    "buildings",
                    "hour",
                    "mark",
                    "matter",
                    "middle",
                    "bombing",
                    "th",
                    "sunday",
                    "situation",
                    "rates",
                    "space",
                    "important",
                    "warne",
                    "dispute",
                    "caught",
                    "jail",
                    "claimed",
                    "wants",
                    "perth",
                    "adventure",
                    "targets",
                    "run",
                    "swiss",
                    "asio",
                    "added",
                    "commonwealth",
                    "raids",
                    "office",
                    "evidence",
                    "deal",
                    "guides",
                    "disease",
                    "show",
                    "boy",
                    "women",
                    "own",
                    "freeze",
                    "opened",
                    "human",
                    "forward",
                    "carried",
                    "african",
                    "mission",
                    "movement",
                    "based",
                    "sure",
                    "reported",
                    "immediately",
                    "political",
                    "warplanes",
                    "young",
                    "rule",
                    "ms",
                    "blue",
                    "top",
                    "justice",
                    "money",
                    "aedt",
                    "cancer",
                    "crash",
                    "march",
                    "banks",
                    "border",
                    "using",
                    "although",
                    "access",
                    "financial",
                    "allegations",
                    "certainly",
                    "planning",
                    "probably",
                    "break",
                    "find",
                    "wicket",
                    "ground",
                    "beat",
                    "prepared",
                    "burning",
                    "become",
                    "always",
                    "job",
                    "proposed",
                    "each",
                    "full",
                    "reached",
                    "collapse",
                    "growth",
                    "order",
                    "island",
                    "sector",
                    "flying",
                    "carrying",
                    "result",
                    "face",
                    "investigation",
                    "times",
                    "relations",
                    "militant",
                    "road",
                    "sex",
                    "needs",
                    "organisation",
                    "until",
                    "serious",
                    "program",
                    "fight",
                    "calls",
                    "stage",
                    "getting",
                    "lives",
                    "responsibility",
                    "reserve",
                    "thursday",
                    "comes",
                    "management",
                    "sent",
                    "drop",
                    "surrender",
                    "allow",
                    "soon",
                    "afp",
                    "tried",
                    "post",
                    "killing",
                    "radical",
                    "hewitt",
                    "himself",
                    "senator",
                    "executive",
                    "outside",
                    "believes",
                    "inquiry",
                    "short",
                    "caves",
                    "different",
                    "flights",
                    "immigration",
                    "tourists",
                    "future",
                    "inside",
                    "bid",
                    "energy",
                    "clear",
                    "trees",
                    "thousands",
                    "argentina",
                    "militia",
                    "suspected",
                    "making",
                    "bowling",
                    "ariel",
                    "went",
                    "alleged",
                    "rejected",
                    "howard",
                    "quickly",
                    "wave",
                    "harrison",
                    "travel",
                    "opening",
                    "ansett",
                    "kilometres",
                    "declared",
                    "running",
                    "measures",
                    "biggest",
                    "list",
                    "figures",
                    "rise",
                    "residents",
                    "sea",
                    "form",
                    "annual",
                    "anything",
                    "attempt",
                    "open",
                    "parties",
                    "available",
                    "announced",
                    "shortly",
                    "among",
                    "currently",
                    "bombers",
                    "circumstances",
                    "accident",
                    "donald",
                    "ministers",
                    "look",
                    "brisbane",
                    "decided",
                    "ruddock",
                    "changes",
                    "yet",
                    "issues",
                    "address",
                    "destroyed",
                    "actually",
                    "rights",
                    "increase",
                    "terms",
                    "school",
                    "rural",
                    "fighter",
                    "quite",
                    "happened",
                    "wounded",
                    "victoria",
                    "television",
                    "nine",
                    "something",
                    "try",
                    "parts",
                    "white",
                    "response",
                    "done",
                    "wickets",
                    "witnesses",
                    "refused",
                    "karzai",
                    "sentence",
                    "ended",
                    "tanks",
                    "gunmen",
                    "sources",
                    "kallis",
                    "agency",
                    "july",
                    "jewish",
                    "warned",
                    "directors",
                    "understand",
                    "meet",
                    "means",
                    "returned",
                    "offices",
                    "yacht",
                    "source",
                    "alexander",
                    "ll",
                    "fact",
                    "difficult",
                    "though",
                    "period",
                    "confidence",
                    "wage",
                    "airlines",
                    "virus",
                    "advice",
                    "caused",
                    "musharraf",
                    "allan",
                    "recession",
                    "less",
                    "ensure",
                    "strike",
                    "appeared",
                    "islands",
                    "crowd",
                    "suharto",
                    "highway",
                    "afternoon",
                    "step",
                    "commanders",
                    "began",
                    "gave",
                    "worst",
                    "glenn",
                    "bomb",
                    "commissioner",
                    "powell",
                    "having",
                    "beginning",
                    "intelligence",
                    "rafter",
                    "prevent",
                    "gives",
                    "expressed",
                    "huge",
                    "ever",
                    "big",
                    "business",
                    "ses",
                    "media",
                    "friday",
                    "pacific",
                    "robert",
                    "expect",
                    "blake",
                    "runs",
                    "involved",
                    "followed",
                    "deputy",
                    "hobart",
                    "whose",
                    "market",
                    "tour",
                    "rather",
                    "attorney",
                    "elected",
                    "beyond",
                    "arrived",
                    "away",
                    "facility",
                    "commander",
                    "total",
                    "law",
                    "field",
                    "supporters",
                    "struck",
                    "car",
                    "cost",
                    "sir",
                    "negotiations",
                    "nauru",
                    "tennis",
                    "massive",
                    "entered",
                    "threat",
                    "plan",
                    "explosives",
                    "debt",
                    "entitlements",
                    "criticism",
                    "decide",
                    "quarter",
                    "saturday",
                    "assistance",
                    "labour",
                    "geoff",
                    "together",
                    "finished",
                    "chance",
                    "endeavour",
                    "chairman",
                    "main",
                    "heavy",
                    "base",
                    "places",
                    "tragedy",
                    "sort",
                    "vote",
                    "giving",
                    "jenin",
                    "front",
                    "powers",
                    "anglican",
                    "son",
                    "zimbabwe",
                    "themselves",
                    "conflict",
                    "yes",
                    "muslim",
                    "lockett",
                    "daryl",
                    "helicopter",
                    "current",
                    "fast",
                    "complex",
                    "terror",
                    "smoke",
                    "france",
                    "anthony",
                    "calling",
                    "hearings",
                    "population",
                    "tasmania",
                    "game",
                    "jacques",
                    "placed",
                    "denied",
                    "reid",
                    "pakistani",
                    "indonesia",
                    "bring",
                    "ballot",
                    "played",
                    "protect",
                    "level",
                    "conference",
                    "organisations",
                    "martin",
                    "employees",
                    "feel",
                    "costs",
                    "changed",
                    "study",
                    "survey",
                    "brett",
                    "potential",
                    "macgill",
                    "cannot",
                    "crean",
                    "lost",
                    "storms",
                    "round",
                    "russian",
                    "trip",
                    "crisis",
                    "nearly",
                    "americans",
                    "speaking",
                    "ambush",
                    "never",
                    "significant",
                    "boxing",
                    "longer",
                    "low",
                    "tribal",
                    "deadly",
                    "record",
                    "problem",
                    "professor",
                    "hayden",
                    "fleeing",
                    "absolutely",
                    "continues",
                    "fired",
                    "rumsfeld",
                    "claim",
                    "ramallah",
                    "hold",
                    "anyone",
                    "election",
                    "construction",
                    "technology",
                    "doubles",
                    "cities",
                    "companies",
                    "research",
                    "whole",
                    "efforts",
                    "needed",
                    "small",
                    "moved",
                    "confident",
                    "land",
                    "proposals",
                    "sign",
                    "little",
                    "affected",
                    "tape",
                    "ruled",
                    "environment",
                    "everything",
                    "severe",
                    "led",
                    "closed",
                    "forecast",
                    "pilot",
                    "overall",
                    "gillespie",
                    "signed",
                    "coming",
                    "receive",
                    "rival",
                    "provide",
                    "representation",
                    "simon",
                    "accept",
                    "sides",
                    "mountain",
                    "receiving",
                    "mean",
                    "secret",
                    "injuries",
                    "dozens",
                    "steve",
                    "payment",
                    "hope",
                    "battle",
                    "shuttle",
                    "gun",
                    "central",
                    "bomber",
                    "starting",
                    "activity",
                    "damaged",
                    "bonn",
                    "disaster",
                    "problems",
                    "verdict",
                    "flames",
                    "condition",
                    "french",
                    "tony",
                    "resolution",
                    "rest",
                    "coalition",
                    "richard",
                    "treatment",
                    "recorded",
                    "grant",
                    "stopped",
                    "hotel",
                    "insurance",
                    "carry",
                    "rain",
                    "almost",
                    "ice",
                    "continued",
                    "greater",
                    "global",
                    "share",
                    "direct",
                    "nation",
                    "paid",
                    "vaughan",
                    "statistics",
                    "fellow",
                    "winner",
                    "civil",
                    "review",
                    "private",
                    "gas",
                    "twice",
                    "interlaken",
                    "concern",
                    "cars",
                    "started",
                    "red",
                    "fell",
                    "disappointed",
                    "debate",
                    "determined",
                    "michael",
                    "seles",
                    "begin",
                    "krishna",
                    "didn",
                    "refugees",
                    "remaining",
                    "tough",
                    "ceremony",
                    "property",
                    "january",
                    "qc",
                    "stand",
                    "operation",
                    "territories",
                    "above",
                    "lower",
                    "respond",
                    "reduce",
                    "resolve",
                    "victims",
                    "strategic",
                    "asic",
                    "alongside",
                    "include",
                    "revealed",
                    "august",
                    "season",
                    "charge",
                    "completed",
                    "seeking",
                    "bit",
                    "park",
                    "lines",
                    "heritage",
                    "traditional",
                    "enter",
                    "tuesday",
                    "guard",
                    "ray",
                    "avoid",
                    "markets",
                    "visit",
                    "europe",
                    "winning",
                    "playing",
                    "self",
                    "yachts",
                    "met",
                    "charges",
                    "vice",
                    "cease",
                    "roads",
                    "factory",
                    "america",
                    "itself",
                    "created",
                    "wake",
                    "levels",
                    "fall",
                    "related",
                    "outlook",
                    "ministry",
                    "lung",
                    "hearing",
                    "non",
                    "volunteers",
                    "civilians",
                    "voted",
                    "liquidation",
                    "search",
                    "provisional",
                    "rescue",
                    "victorian",
                    "table",
                    "successful",
                    "track",
                    "conducted",
                    "heading",
                    "spread",
                    "accompanied",
                    "delhi",
                    "operating",
                    "wanted",
                    "expects",
                    "leg",
                    "ponting",
                    "pulled",
                    "knew",
                    "heart",
                    "coach",
                    "confirm",
                    "ball",
                    "virgin",
                    "press",
                    "suffered",
                    "illawarra",
                    "approach",
                    "manslaughter",
                    "costello",
                    "showed",
                    "threatened",
                    "warning",
                    "helped",
                    "resume",
                    "japan",
                    "individuals",
                    "mayor",
                    "giuliani",
                    "friedli",
                    "wind",
                    "served",
                    "andy",
                    "range",
                    "responsible",
                    "unemployment",
                    "mckenzie",
                    "initial",
                    "keep",
                    "families",
                    "lord",
                    "incident",
                    "october",
                    "finance",
                    "treated",
                    "ian",
                    "why",
                    "solution",
                    "apparently",
                    "body",
                    "club",
                    "crackdown",
                    "reach",
                    "officer",
                    "institute",
                    "shaun",
                    "pollock",
                    "hopes",
                    "structure",
                    "data",
                    "nice",
                    "food",
                    "seriously",
                    "suspended",
                    "attacked",
                    "jason",
                    "elections",
                    "edge",
                    "affairs",
                    "nothing",
                    "questions",
                    "mid",
                    "built",
                    "negotiating",
                    "peacekeepers",
                    "saw",
                    "issued",
                    "spokeswoman",
                    "assisting",
                    "remains",
                    "finding",
                    "recovery",
                    "woman",
                    "gang",
                    "kashmir",
                    "farmers",
                    "oil",
                    "networks",
                    "sheikh",
                    "adequate",
                    "doubt",
                    "products",
                    "secure",
                    "beatle",
                    "single",
                    "options",
                    "clearly",
                    "blaze",
                    "present",
                    "ford",
                    "cfmeu",
                    "tailenders",
                    "fatah",
                    "scene",
                    "co",
                    "lording",
                    "factions",
                    "st",
                    "raid",
                    "career",
                    "streets",
                    "butterfly",
                    "amin",
                    "outcome",
                    "traveland",
                    "peres",
                    "inappropriate",
                    "austar",
                    "scored",
                    "champion",
                    "races",
                    "cave",
                    "scheduled",
                    "clean",
                    "nearby",
                    "philip",
                    "shows",
                    "invasion",
                    "aboard",
                    "coup",
                    "senate",
                    "doug",
                    "solomon",
                    "eve",
                    "sarah",
                    "holiday",
                    "mohammad",
                    "university",
                    "murder",
                    "whiting",
                    "gorge",
                    "tensions",
                    "manufacturing",
                    "wayne",
                    "yallourn",
                    "diplomatic",
                    "drug",
                    "promised",
                    "cause",
                    "natural",
                    "afroz",
                    "ethnic",
                    "singles",
                    "crews",
                    "meetings",
                    "toll",
                    "apra",
                    "administrators",
                    "corporation",
                    "leadership",
                    "canberra",
                    "exchange",
                    "nuclear",
                    "germany",
                    "numbers",
                    "attacking",
                    "largest",
                    "petrol",
                    "customers",
                    "prior",
                    "internet",
                    "awards",
                    "extremists",
                    "attempting",
                    "personnel",
                    "hand",
                    "criminal",
                    "mandate",
                    "things",
                    "deployed",
                    "follows",
                    "unrest",
                    "dropped",
                    "manager",
                    "injury",
                    "settlement",
                    "roof",
                    "honours",
                    "appears",
                    "metre",
                    "boats",
                    "often",
                    "speech",
                    "squad",
                    "fair",
                    "budget",
                    "ready",
                    "ask",
                    "band",
                    "proteas",
                    "king",
                    "grand",
                    "recent",
                    "happens",
                    "classic",
                    "suburbs",
                    "resign",
                    "swept",
                    "collapsed",
                    "true",
                    "agreed",
                    "batsmen",
                    "presence",
                    "felt",
                    "billion",
                    "resistance",
                    "giant",
                    "increased",
                    "described",
                    "unit",
                    "create",
                    "concerns",
                    "protection",
                    "targeted",
                    "boys",
                    "saudi",
                    "leave",
                    "unity",
                    "planes",
                    "halt",
                    "read",
                    "marine",
                    "neil",
                    "walk",
                    "crossed",
                    "fleet",
                    "knowledge",
                    "minute",
                    "greatest",
                    "extensive",
                    "backed",
                    "ocean",
                    "assa",
                    "ricky",
                    "abloy",
                    "light",
                    "premier",
                    "names",
                    "explanation",
                    "wall",
                    "possibility",
                    "real",
                    "live",
                    "switzerland",
                    "japanese",
                    "shopping",
                    "reveal",
                    "fierce",
                    "tree",
                    "elders",
                    "blame",
                    "tension",
                    "employment",
                    "detain",
                    "positive",
                    "income",
                    "haifa",
                    "jerusalem",
                    "pre",
                    "programs",
                    "jets",
                    "transport",
                    "regional",
                    "save",
                    "hunt",
                    "advance",
                    "gone",
                    "battling",
                    "suspect",
                    "representing",
                    "investigating",
                    "reduced",
                    "acting",
                    "projects",
                    "investment",
                    "spencer",
                    "findings",
                    "students",
                    "nablus",
                    "actions",
                    "trial",
                    "declaration",
                    "handed",
                    "custody",
                    "growing",
                    "system",
                    "prisoners",
                    "domestic",
                    "education",
                    "society",
                    "summit",
                    "assault",
                    "langer",
                    "matthew",
                    "requested",
                    "westpac",
                    "doctor",
                    "wing",
                    "republic",
                    "searching",
                    "eliminated",
                    "approval",
                    "anz",
                    "term",
                    "bargaining",
                    "various",
                    "balls",
                    "klusener",
                    "boucher",
                    "humanity",
                    "suggested",
                    "adding",
                    "history",
                    "normal",
                    "cuts",
                    "signs",
                    "gunships",
                    "blasted",
                    "turn",
                    "hare",
                    "smaller",
                    "guess",
                    "benares",
                    "ashes",
                    "path",
                    "terrorists",
                    "blazes",
                    "hijacked",
                    "adam",
                    "follow",
                    "comment",
                    "aware",
                    "connection",
                    "underway",
                    "kieren",
                    "rabbani",
                    "completely",
                    "tonight",
                    "understanding",
                    "infected",
                    "masood",
                    "treasurer",
                    "crime",
                    "gambier",
                    "henderson",
                    "returning",
                    "results",
                    "kingham",
                    "question",
                    "kissinger",
                    "gerber",
                    "stuart",
                    "launceston",
                    "sergeant",
                    "flood",
                    "committee",
                    "hundred",
                    "goshen",
                    "handling",
                    "church",
                    "thing",
                    "escaped",
                    "injuring",
                    "slightly",
                    "francs",
                    "hunter",
                    "ahmed",
                    "actor",
                    "wednesday",
                    "aged",
                    "centrelink",
                    "threatening",
                    "sultan",
                    "improve",
                    "passed",
                    "stability",
                    "project",
                    "dollars",
                    "decades",
                    "course",
                    "ill",
                    "faces",
                    "chosen",
                    "bob",
                    "hamid",
                    "passengers",
                    "davis",
                    "neville",
                    "ways",
                    "pace",
                    "whatever",
                    "headed",
                    "launch",
                    "replied",
                    "hopefully",
                    "determine",
                    "archbishop",
                    "unable",
                    "throughout",
                    "average",
                    "unidentified",
                    "survived",
                    "approached",
                    "convicted",
                    "cooperation",
                    "redundancy",
                    "waiting",
                    "request",
                    "paying",
                    "observers",
                    "aboriginal",
                    "procedures",
                    "reject",
                    "document",
                    "improved",
                    "holding",
                    "mass",
                    "unfortunately",
                    "welcomed",
                    "whereabouts",
                    "appropriate",
                    "lack",
                    "delay",
                    "trapped",
                    "facilities",
                    "decisions",
                    "prepare",
                    "medical",
                    "necessary",
                    "spinner",
                    "examination",
                    "losing",
                    "channel",
                    "occupation",
                    "title",
                    "consumers",
                    "firm",
                    "creditors",
                    "fine",
                    "vehicle",
                    "staying",
                    "relationship",
                    "delivered",
                    "begun",
                    "hot",
                    "coroner",
                    "temperatures",
                    "containment",
                    "cross",
                    "contested",
                    "strongly",
                    "experts",
                    "celebrations",
                    "focus",
                    "named",
                    "sometimes",
                    "marines",
                    "player",
                    "jalalabad",
                    "games",
                    "breaking",
                    "contained",
                    "counts",
                    "stay",
                    "allowed",
                    "temporary",
                    "assembly",
                    "draft",
                    "understood",
                    "toowoomba",
                    "voice",
                    "twenty",
                    "strachan",
                    "harris",
                    "discussions",
                    "hopman",
                    "crashed",
                    "farm",
                    "violent",
                    "communities",
                    "kilometre",
                    "doctors",
                    "hoping",
                    "ban",
                    "colin",
                    "effective",
                    "success",
                    "offered",
                    "positions",
                    "abu",
                    "worked",
                    "documents",
                    "tell",
                    "phillips",
                    "retired",
                    "choosing",
                    "responding",
                    "allegedly",
                    "indonesian",
                    "detail",
                    "free",
                    "bringing",
                    "hiv",
                    "proposal",
                    "doesn",
                    "mining",
                    "embassy",
                    "heights",
                    "mt",
                    "trading",
                    "room",
                    "fund",
                    "impact",
                    "male",
                    "mohammed",
                    "interests",
                    "effort",
                    "antarctic",
                    "previous",
                    "target",
                    "words",
                    "publicly",
                    "walked",
                    "credit",
                    "provided",
                    "investigate",
                    "telephone",
                    "eventually",
                    "leaving",
                    "banking",
                    "interview",
                    "headquarters",
                    "clashes",
                    "doing",
                    "fear",
                    "predicted",
                    "picked",
                    "happy",
                    "visa",
                    "tie",
                    "putting",
                    "escalating",
                    "hoped",
                    "landed",
                    "sharing",
                    "mind",
                    "skipper",
                    "gary",
                    "soft",
                    "became",
                    "sending",
                    "shoes",
                    "paris",
                    "required",
                    "seemed",
                    "cameron",
                    "ability",
                    "locked",
                    "travelled",
                    "finally",
                    "separate",
                    "owen"
                  ],
                  "type": "scatter",
                  "x": [
                    66.3838882446289,
                    70.03727722167969,
                    69.80760192871094,
                    69.58170318603516,
                    70.17229461669922,
                    59.32894515991211,
                    67.90863037109375,
                    68.62193298339844,
                    70.41272735595703,
                    66.06658172607422,
                    64.53382110595703,
                    70.55182647705078,
                    69.12042236328125,
                    62.315887451171875,
                    69.25253295898438,
                    63.64190673828125,
                    62.95341873168945,
                    68.9612045288086,
                    70.59469604492188,
                    67.53556060791016,
                    70.44652557373047,
                    62.15291976928711,
                    70.43708038330078,
                    70.86611938476562,
                    67.21956634521484,
                    60.01408767700195,
                    69.8149185180664,
                    69.13595581054688,
                    64.30243682861328,
                    66.39900970458984,
                    66.85686492919922,
                    66.91075897216797,
                    70.90409851074219,
                    68.50463104248047,
                    66.11347198486328,
                    59.614356994628906,
                    67.72840881347656,
                    65.19670867919922,
                    60.0964469909668,
                    66.10891723632812,
                    67.81172943115234,
                    63.38094711303711,
                    60.10725784301758,
                    66.7314224243164,
                    58.13697052001953,
                    69.31446838378906,
                    66.17745208740234,
                    64.22927856445312,
                    62.257164001464844,
                    63.980464935302734,
                    64.89633178710938,
                    60.08439254760742,
                    63.56083297729492,
                    60.83586120605469,
                    63.003475189208984,
                    62.15724182128906,
                    62.65536117553711,
                    66.37185668945312,
                    63.28640365600586,
                    67.29171752929688,
                    63.66503143310547,
                    62.39086151123047,
                    61.182151794433594,
                    61.4775390625,
                    60.093868255615234,
                    67.90023040771484,
                    58.82817840576172,
                    62.39879608154297,
                    57.200950622558594,
                    60.76457595825195,
                    55.72461700439453,
                    54.35950469970703,
                    60.38949966430664,
                    57.73248291015625,
                    59.83946990966797,
                    58.65781021118164,
                    59.50355529785156,
                    54.15410232543945,
                    58.81045913696289,
                    61.133907318115234,
                    57.30542755126953,
                    56.60201644897461,
                    55.558658599853516,
                    59.82984161376953,
                    55.419227600097656,
                    58.43398666381836,
                    55.31121826171875,
                    55.26466369628906,
                    60.903953552246094,
                    56.804283142089844,
                    59.37510299682617,
                    54.440608978271484,
                    56.14787673950195,
                    54.661624908447266,
                    55.03977966308594,
                    56.84988784790039,
                    54.820030212402344,
                    56.31010437011719,
                    55.765106201171875,
                    54.554107666015625,
                    55.43418884277344,
                    56.72661590576172,
                    55.956687927246094,
                    53.285396575927734,
                    56.841407775878906,
                    55.161075592041016,
                    53.02909469604492,
                    54.721866607666016,
                    54.134498596191406,
                    54.88554763793945,
                    55.339927673339844,
                    55.19314193725586,
                    54.11680603027344,
                    53.56168746948242,
                    53.360511779785156,
                    54.034854888916016,
                    54.017269134521484,
                    54.655364990234375,
                    54.09534454345703,
                    54.29253005981445,
                    53.67941665649414,
                    54.16633224487305,
                    54.5442008972168,
                    53.127403259277344,
                    57.2662353515625,
                    53.73740005493164,
                    52.40613555908203,
                    54.06179428100586,
                    53.729801177978516,
                    53.800357818603516,
                    52.503387451171875,
                    54.10138702392578,
                    55.37040710449219,
                    52.68584060668945,
                    53.719356536865234,
                    54.23197937011719,
                    54.02332305908203,
                    54.10688018798828,
                    53.67628479003906,
                    53.37466812133789,
                    53.98497772216797,
                    52.87056350708008,
                    54.08102798461914,
                    52.55867004394531,
                    53.70336151123047,
                    54.316097259521484,
                    53.50819778442383,
                    53.758487701416016,
                    53.31901931762695,
                    53.71604537963867,
                    53.86891174316406,
                    54.650535583496094,
                    53.27031326293945,
                    53.041839599609375,
                    53.96886444091797,
                    54.414337158203125,
                    54.115055084228516,
                    53.80965042114258,
                    54.065277099609375,
                    54.372833251953125,
                    50.06894302368164,
                    51.054786682128906,
                    54.288639068603516,
                    53.83853530883789,
                    54.4201545715332,
                    54.5213737487793,
                    51.709869384765625,
                    52.91292953491211,
                    54.55896759033203,
                    53.05800247192383,
                    52.86412811279297,
                    53.64644241333008,
                    53.85977554321289,
                    40.47349548339844,
                    53.873573303222656,
                    53.63609313964844,
                    36.0756950378418,
                    52.88836669921875,
                    53.49000549316406,
                    53.2493782043457,
                    53.262725830078125,
                    53.58715057373047,
                    49.68132400512695,
                    53.08605194091797,
                    53.09371566772461,
                    53.8241081237793,
                    50.05182647705078,
                    53.91777038574219,
                    37.591026306152344,
                    49.0499153137207,
                    53.391963958740234,
                    48.0398063659668,
                    53.992149353027344,
                    53.69087600708008,
                    52.60426330566406,
                    51.56808090209961,
                    41.640464782714844,
                    51.74321746826172,
                    50.17192840576172,
                    51.83246994018555,
                    53.96410369873047,
                    51.724037170410156,
                    53.78411865234375,
                    53.83463668823242,
                    50.8928337097168,
                    44.05918502807617,
                    38.417869567871094,
                    44.571929931640625,
                    53.825286865234375,
                    36.513580322265625,
                    52.49834442138672,
                    46.794559478759766,
                    47.91973114013672,
                    48.08796691894531,
                    53.49114227294922,
                    53.2877082824707,
                    53.157196044921875,
                    47.70852279663086,
                    47.407535552978516,
                    50.273353576660156,
                    51.19487762451172,
                    53.341556549072266,
                    51.643043518066406,
                    51.053802490234375,
                    46.569175720214844,
                    37.53000259399414,
                    52.813194274902344,
                    52.00128936767578,
                    50.55269241333008,
                    42.78109359741211,
                    29.90513801574707,
                    49.378116607666016,
                    49.949005126953125,
                    36.50753402709961,
                    44.88375473022461,
                    46.44334030151367,
                    42.68423080444336,
                    51.5224494934082,
                    48.86923599243164,
                    40.26498794555664,
                    41.85823059082031,
                    36.430519104003906,
                    49.35749053955078,
                    42.560604095458984,
                    36.716556549072266,
                    43.69744110107422,
                    52.1346435546875,
                    48.82204055786133,
                    44.11542892456055,
                    38.52657699584961,
                    52.51730728149414,
                    45.97466278076172,
                    49.795623779296875,
                    28.130367279052734,
                    28.4909725189209,
                    48.658897399902344,
                    42.02653884887695,
                    46.25815200805664,
                    47.30986785888672,
                    27.424694061279297,
                    47.30442428588867,
                    33.9825325012207,
                    37.03540802001953,
                    44.77467727661133,
                    37.69233322143555,
                    52.77803421020508,
                    37.02115249633789,
                    26.992536544799805,
                    37.175662994384766,
                    34.88822555541992,
                    27.795921325683594,
                    51.38943862915039,
                    45.855403900146484,
                    44.14128875732422,
                    51.30164337158203,
                    27.535877227783203,
                    41.75183868408203,
                    30.078048706054688,
                    29.55145263671875,
                    37.35546112060547,
                    45.10432434082031,
                    48.227962493896484,
                    50.46978759765625,
                    41.34557342529297,
                    46.987972259521484,
                    27.034664154052734,
                    41.62954330444336,
                    31.56039810180664,
                    43.875858306884766,
                    38.95124435424805,
                    45.385948181152344,
                    32.70208740234375,
                    46.06510925292969,
                    45.482994079589844,
                    49.97248840332031,
                    29.53412437438965,
                    28.692340850830078,
                    41.07302474975586,
                    28.44556999206543,
                    22.719282150268555,
                    49.137088775634766,
                    43.1151008605957,
                    50.829917907714844,
                    25.982969284057617,
                    35.18517303466797,
                    30.655784606933594,
                    34.39967727661133,
                    28.45579719543457,
                    27.971054077148438,
                    36.28323745727539,
                    49.804813385009766,
                    50.689937591552734,
                    32.230995178222656,
                    45.08646774291992,
                    29.103534698486328,
                    39.71873092651367,
                    32.343048095703125,
                    32.12438201904297,
                    27.312349319458008,
                    48.08551788330078,
                    32.2723388671875,
                    33.789363861083984,
                    42.23685073852539,
                    26.239280700683594,
                    28.930429458618164,
                    29.582103729248047,
                    29.13603973388672,
                    47.94231414794922,
                    28.89218521118164,
                    27.138090133666992,
                    38.826927185058594,
                    28.25901222229004,
                    49.40243148803711,
                    28.493783950805664,
                    28.16080093383789,
                    32.80537033081055,
                    28.508392333984375,
                    19.424339294433594,
                    29.571794509887695,
                    34.21099853515625,
                    28.905757904052734,
                    33.12464904785156,
                    37.721675872802734,
                    50.3474235534668,
                    28.69346809387207,
                    43.64868927001953,
                    28.394800186157227,
                    29.970102310180664,
                    28.54316520690918,
                    30.733495712280273,
                    40.748783111572266,
                    28.1602783203125,
                    29.289623260498047,
                    24.16069984436035,
                    29.946365356445312,
                    31.425460815429688,
                    23.3623046875,
                    28.565235137939453,
                    27.703975677490234,
                    27.232297897338867,
                    28.487375259399414,
                    25.805986404418945,
                    26.704317092895508,
                    30.26924705505371,
                    24.538042068481445,
                    23.734569549560547,
                    28.14321517944336,
                    33.3218994140625,
                    36.96942138671875,
                    32.398380279541016,
                    27.37734031677246,
                    25.279827117919922,
                    29.00090217590332,
                    36.221229553222656,
                    19.30573844909668,
                    31.406984329223633,
                    26.24303436279297,
                    27.8632755279541,
                    27.556854248046875,
                    29.22592544555664,
                    28.083593368530273,
                    28.315881729125977,
                    27.70564079284668,
                    27.27078628540039,
                    28.04432487487793,
                    28.286840438842773,
                    26.912601470947266,
                    41.85068893432617,
                    37.03776931762695,
                    28.111236572265625,
                    27.683799743652344,
                    35.26738739013672,
                    25.249191284179688,
                    27.803810119628906,
                    27.8295841217041,
                    21.506635665893555,
                    27.122053146362305,
                    28.516603469848633,
                    30.348329544067383,
                    25.966522216796875,
                    31.631914138793945,
                    27.560211181640625,
                    27.72195816040039,
                    31.960433959960938,
                    22.032522201538086,
                    31.485153198242188,
                    51.308349609375,
                    28.013986587524414,
                    23.513071060180664,
                    24.866796493530273,
                    13.46588134765625,
                    28.202234268188477,
                    27.891908645629883,
                    29.771093368530273,
                    41.76749801635742,
                    27.32257843017578,
                    27.038747787475586,
                    5.165525436401367,
                    26.4559326171875,
                    24.270267486572266,
                    27.693864822387695,
                    36.14649200439453,
                    6.40425968170166,
                    27.243051528930664,
                    2.1814167499542236,
                    28.409141540527344,
                    27.09673309326172,
                    16.923433303833008,
                    27.147850036621094,
                    21.7128849029541,
                    28.469451904296875,
                    27.9125919342041,
                    22.30403709411621,
                    28.454240798950195,
                    23.106712341308594,
                    28.009443283081055,
                    7.598287105560303,
                    27.305587768554688,
                    27.75697135925293,
                    27.839189529418945,
                    27.54913902282715,
                    26.914962768554688,
                    27.428525924682617,
                    28.25181007385254,
                    27.517976760864258,
                    30.542692184448242,
                    2.1803441047668457,
                    27.897581100463867,
                    22.080963134765625,
                    41.11630630493164,
                    41.16336441040039,
                    12.49476432800293,
                    25.744155883789062,
                    26.4406795501709,
                    27.389041900634766,
                    27.744342803955078,
                    18.759206771850586,
                    2.3007452487945557,
                    23.09798812866211,
                    26.452186584472656,
                    27.469505310058594,
                    26.979598999023438,
                    27.553050994873047,
                    28.225378036499023,
                    25.989089965820312,
                    28.90704345703125,
                    18.28961181640625,
                    27.608003616333008,
                    27.03302001953125,
                    25.48565101623535,
                    26.691898345947266,
                    12.573896408081055,
                    22.512466430664062,
                    0.8397967219352722,
                    11.868453025817871,
                    27.983911514282227,
                    27.475404739379883,
                    33.758750915527344,
                    29.380020141601562,
                    27.91515350341797,
                    7.097864627838135,
                    27.690401077270508,
                    3.7515642642974854,
                    27.6750545501709,
                    25.608840942382812,
                    5.808915138244629,
                    27.439363479614258,
                    24.614315032958984,
                    27.01204490661621,
                    24.193885803222656,
                    28.899864196777344,
                    26.27663230895996,
                    26.855979919433594,
                    13.999480247497559,
                    27.522512435913086,
                    24.16776466369629,
                    26.913480758666992,
                    4.08782958984375,
                    15.51730728149414,
                    1.1541047096252441,
                    28.02761459350586,
                    25.971439361572266,
                    25.529569625854492,
                    27.344867706298828,
                    24.5886173248291,
                    28.946125030517578,
                    25.901979446411133,
                    23.354774475097656,
                    15.970992088317871,
                    22.075679779052734,
                    11.912951469421387,
                    24.472997665405273,
                    28.654918670654297,
                    27.824657440185547,
                    18.8824405670166,
                    4.894108772277832,
                    1.5086770057678223,
                    26.476163864135742,
                    7.328629493713379,
                    25.692373275756836,
                    11.628061294555664,
                    26.214431762695312,
                    3.3513801097869873,
                    28.660533905029297,
                    28.476247787475586,
                    14.726469993591309,
                    25.534502029418945,
                    1.2554723024368286,
                    18.26544761657715,
                    25.045711517333984,
                    23.83209991455078,
                    2.0431032180786133,
                    26.608129501342773,
                    26.91000747680664,
                    27.992433547973633,
                    28.792884826660156,
                    17.272539138793945,
                    16.319602966308594,
                    20.974990844726562,
                    27.645204544067383,
                    27.86296272277832,
                    28.4322566986084,
                    0.8844791054725647,
                    11.569622039794922,
                    26.622886657714844,
                    5.456864833831787,
                    4.4964399337768555,
                    24.14083480834961,
                    16.280582427978516,
                    28.515932083129883,
                    22.48073387145996,
                    25.410104751586914,
                    16.24913787841797,
                    6.100963592529297,
                    28.01543426513672,
                    1.5293060541152954,
                    2.237093448638916,
                    6.232179164886475,
                    2.5287437438964844,
                    27.473634719848633,
                    14.832275390625,
                    23.640398025512695,
                    11.998573303222656,
                    2.999349594116211,
                    2.9549834728240967,
                    22.090473175048828,
                    18.259281158447266,
                    25.833908081054688,
                    9.03244400024414,
                    26.463098526000977,
                    1.7597659826278687,
                    21.416236877441406,
                    12.743522644042969,
                    37.328514099121094,
                    21.284135818481445,
                    23.576753616333008,
                    28.27518081665039,
                    2.5570313930511475,
                    26.818845748901367,
                    24.210681915283203,
                    28.041954040527344,
                    20.791751861572266,
                    24.484399795532227,
                    32.80032730102539,
                    26.3421573638916,
                    0.09230969101190567,
                    28.08559799194336,
                    25.81791877746582,
                    1.6279246807098389,
                    4.607223987579346,
                    28.515363693237305,
                    7.194377899169922,
                    2.851445436477661,
                    2.600938081741333,
                    14.93978500366211,
                    14.269180297851562,
                    4.863690376281738,
                    26.457374572753906,
                    6.581759452819824,
                    13.60167121887207,
                    1.6223018169403076,
                    3.824173927307129,
                    2.8084356784820557,
                    14.751022338867188,
                    2.614793300628662,
                    4.2886810302734375,
                    14.321739196777344,
                    2.0924649238586426,
                    7.112168312072754,
                    1.6337255239486694,
                    27.16103744506836,
                    1.8644171953201294,
                    20.2393856048584,
                    17.702089309692383,
                    17.15961265563965,
                    27.38419532775879,
                    10.352058410644531,
                    15.574044227600098,
                    19.40694236755371,
                    4.251465797424316,
                    2.3676388263702393,
                    15.1710786819458,
                    27.688793182373047,
                    19.895923614501953,
                    1.8956060409545898,
                    22.77171516418457,
                    25.743404388427734,
                    26.99852752685547,
                    2.7900521755218506,
                    23.383150100708008,
                    14.355363845825195,
                    15.371696472167969,
                    27.548171997070312,
                    3.981768846511841,
                    15.672492027282715,
                    3.93953275680542,
                    5.641819953918457,
                    0.8990104794502258,
                    -0.09478674083948135,
                    3.438084602355957,
                    16.134737014770508,
                    17.66996192932129,
                    1.2829066514968872,
                    5.714400768280029,
                    10.948832511901855,
                    14.798127174377441,
                    -22.019123077392578,
                    25.674428939819336,
                    2.831275701522827,
                    3.8538360595703125,
                    -26.2596435546875,
                    13.488704681396484,
                    2.2827463150024414,
                    9.65277099609375,
                    26.63282585144043,
                    18.127553939819336,
                    22.458404541015625,
                    14.668371200561523,
                    17.094236373901367,
                    20.168529510498047,
                    1.9926667213439941,
                    -9.958386421203613,
                    4.522586345672607,
                    2.254894495010376,
                    5.95841646194458,
                    20.59882164001465,
                    9.842700004577637,
                    25.07673454284668,
                    3.673198699951172,
                    8.466893196105957,
                    3.1400318145751953,
                    13.41666316986084,
                    4.848123550415039,
                    28.3035945892334,
                    13.79228687286377,
                    5.7416253089904785,
                    3.5767228603363037,
                    3.538796901702881,
                    3.111776113510132,
                    4.934834957122803,
                    3.1435060501098633,
                    -48.395301818847656,
                    10.921762466430664,
                    5.539677143096924,
                    3.6126296520233154,
                    6.315653324127197,
                    -5.4607038497924805,
                    3.549403667449951,
                    16.818178176879883,
                    2.6422510147094727,
                    0.39706671237945557,
                    2.6191697120666504,
                    12.997390747070312,
                    -18.988994598388672,
                    -0.07471291720867157,
                    -3.4602224826812744,
                    6.497378826141357,
                    5.610983848571777,
                    1.9137448072433472,
                    4.636561393737793,
                    16.041826248168945,
                    21.743528366088867,
                    27.536575317382812,
                    13.9342679977417,
                    0.11018408834934235,
                    -14.232396125793457,
                    26.652263641357422,
                    -34.05174255371094,
                    5.946363925933838,
                    4.025294303894043,
                    11.091974258422852,
                    2.8148176670074463,
                    -18.614063262939453,
                    0.22319979965686798,
                    1.0443254709243774,
                    27.687788009643555,
                    -10.905630111694336,
                    5.751302719116211,
                    -11.886563301086426,
                    2.7913568019866943,
                    1.9477788209915161,
                    1.829982042312622,
                    2.8389782905578613,
                    2.4332804679870605,
                    -11.808037757873535,
                    -60.07505798339844,
                    2.882476806640625,
                    1.7558921575546265,
                    1.8366779088974,
                    1.834519624710083,
                    7.671560764312744,
                    16.485618591308594,
                    6.357071399688721,
                    3.6160244941711426,
                    6.082180500030518,
                    1.194061517715454,
                    2.521075963973999,
                    2.766570568084717,
                    3.224632978439331,
                    4.364656448364258,
                    -20.37794303894043,
                    1.2343870401382446,
                    2.9400410652160645,
                    -2.3610451221466064,
                    3.250816822052002,
                    -17.589054107666016,
                    4.831930637359619,
                    2.593717336654663,
                    3.3238797187805176,
                    25.134132385253906,
                    3.1181726455688477,
                    2.4069879055023193,
                    0.7780314683914185,
                    2.6356284618377686,
                    22.76288604736328,
                    2.2611138820648193,
                    9.093058586120605,
                    1.5544226169586182,
                    1.5777761936187744,
                    14.20523452758789,
                    2.467942714691162,
                    -42.299415588378906,
                    13.038534164428711,
                    2.442448854446411,
                    3.6444506645202637,
                    -6.982672214508057,
                    -17.241376876831055,
                    2.399423599243164,
                    18.893871307373047,
                    -3.528175115585327,
                    1.3329962491989136,
                    18.14756202697754,
                    0.9159736633300781,
                    15.308525085449219,
                    23.17407989501953,
                    3.2088451385498047,
                    8.396228790283203,
                    2.3511064052581787,
                    -4.477787017822266,
                    2.3173553943634033,
                    6.50845193862915,
                    1.1969385147094727,
                    -48.99125289916992,
                    -2.6219642162323,
                    5.134729862213135,
                    -29.83340072631836,
                    2.431367874145508,
                    3.7803544998168945,
                    4.501986980438232,
                    1.63693368434906,
                    1.437120795249939,
                    1.9982972145080566,
                    1.3213491439819336,
                    4.87343692779541,
                    1.0840729475021362,
                    4.832571983337402,
                    -15.761204719543457,
                    12.859333992004395,
                    25.258529663085938,
                    8.02536678314209,
                    1.5325126647949219,
                    5.047880172729492,
                    20.963428497314453,
                    3.7389769554138184,
                    -7.994770050048828,
                    2.7070529460906982,
                    3.570358991622925,
                    -21.63253402709961,
                    -5.488454341888428,
                    0.8224937915802002,
                    3.697751522064209,
                    2.540470600128174,
                    -20.30014419555664,
                    -16.84880256652832,
                    -4.175836563110352,
                    13.18851089477539,
                    -0.5631857514381409,
                    -18.377901077270508,
                    23.092647552490234,
                    3.1351914405822754,
                    0.8130359053611755,
                    4.264707088470459,
                    3.043280601501465,
                    -25.32920265197754,
                    2.771852970123291,
                    -18.359804153442383,
                    5.287017822265625,
                    3.8873836994171143,
                    -12.417696952819824,
                    -22.245325088500977,
                    3.376875400543213,
                    3.464177370071411,
                    10.409667015075684,
                    1.5362823009490967,
                    1.3611325025558472,
                    1.0643528699874878,
                    -2.477494239807129,
                    2.341675043106079,
                    -12.356472969055176,
                    2.378873586654663,
                    4.857940196990967,
                    1.6040449142456055,
                    11.21350383758545,
                    -46.606048583984375,
                    3.8406639099121094,
                    -19.943696975708008,
                    5.567195892333984,
                    2.031053066253662,
                    3.7208189964294434,
                    5.100648403167725,
                    -14.414823532104492,
                    4.1344380378723145,
                    -6.836599349975586,
                    1.1928707361221313,
                    -22.28136444091797,
                    1.4056849479675293,
                    2.846891164779663,
                    2.6196415424346924,
                    5.166737079620361,
                    6.643126010894775,
                    1.148411512374878,
                    4.516327857971191,
                    -14.3594388961792,
                    5.336153984069824,
                    2.2935032844543457,
                    3.202310085296631,
                    4.435025691986084,
                    4.021119594573975,
                    -7.223355770111084,
                    3.9001832008361816,
                    0.6658019423484802,
                    3.4002110958099365,
                    2.9167354106903076,
                    2.757338285446167,
                    -15.542720794677734,
                    3.5402467250823975,
                    2.2656409740448,
                    0.8967165350914001,
                    -12.18293285369873,
                    4.942546367645264,
                    -27.9440860748291,
                    3.373436212539673,
                    1.549004077911377,
                    -20.332815170288086,
                    3.242124557495117,
                    1.9936981201171875,
                    0.6630045175552368,
                    -1.664788007736206,
                    3.9958033561706543,
                    1.2124913930892944,
                    -28.44381332397461,
                    1.8240278959274292,
                    -17.44756507873535,
                    -10.051637649536133,
                    -23.48295783996582,
                    2.0383782386779785,
                    -19.38912582397461,
                    4.025132179260254,
                    1.942064642906189,
                    4.913013458251953,
                    -18.379262924194336,
                    -6.302505970001221,
                    5.523695468902588,
                    4.018278121948242,
                    -34.81912612915039,
                    -26.088909149169922,
                    4.449319362640381,
                    3.675814151763916,
                    2.533050775527954,
                    -27.28433609008789,
                    -0.248331218957901,
                    -0.8774054050445557,
                    -20.510284423828125,
                    -21.685083389282227,
                    -5.700313091278076,
                    3.6414055824279785,
                    -7.773971080780029,
                    4.2360920906066895,
                    3.799140214920044,
                    12.086825370788574,
                    -8.120000839233398,
                    1.8660506010055542,
                    -14.366547584533691,
                    3.606553077697754,
                    3.431110143661499,
                    2.015012502670288,
                    2.5313122272491455,
                    4.473135471343994,
                    4.07548189163208,
                    -0.5527340173721313,
                    6.191308975219727,
                    1.1621131896972656,
                    -30.99024772644043,
                    -4.519214153289795,
                    -28.08330726623535,
                    4.074141025543213,
                    0.6053165197372437,
                    3.7250614166259766,
                    1.0981745719909668,
                    -2.457209825515747,
                    2.450261116027832,
                    3.0515642166137695,
                    0.8563485145568848,
                    -0.9771766662597656,
                    -30.481975555419922,
                    3.2942306995391846,
                    4.334439754486084,
                    -14.438127517700195,
                    2.425342321395874,
                    -9.540046691894531,
                    4.373692512512207,
                    -18.32874870300293,
                    0.3210470676422119,
                    2.9571516513824463,
                    10.46633529663086,
                    2.5930979251861572,
                    -27.380290985107422,
                    5.199720859527588,
                    -18.553890228271484,
                    7.5512471199035645,
                    -4.564673900604248,
                    15.156754493713379,
                    2.1769862174987793,
                    5.087087154388428,
                    -16.09311866760254,
                    4.492702960968018,
                    -23.846223831176758,
                    2.800111770629883,
                    -2.701289415359497,
                    -15.649179458618164,
                    -3.317441701889038,
                    5.668086051940918,
                    7.644052505493164,
                    -34.203670501708984,
                    6.239250659942627,
                    2.6600539684295654,
                    -13.70698356628418,
                    1.9300743341445923,
                    -10.072098731994629,
                    3.840024948120117,
                    -27.063217163085938,
                    -19.868886947631836,
                    -7.076589107513428,
                    -21.347675323486328,
                    2.7123124599456787,
                    -0.01598326489329338,
                    3.044312000274658,
                    -18.811569213867188,
                    -26.701990127563477,
                    -26.344995498657227,
                    -21.137710571289062,
                    -20.03350830078125,
                    -17.660688400268555,
                    -25.763391494750977,
                    -23.91257095336914,
                    -14.138050079345703,
                    -43.253787994384766,
                    1.1253706216812134,
                    -20.571752548217773,
                    -5.5707926750183105,
                    -47.07274627685547,
                    -14.075246810913086,
                    3.1600935459136963,
                    -39.5892448425293,
                    -11.735111236572266,
                    -41.542476654052734,
                    -33.081146240234375,
                    -22.169227600097656,
                    -43.85806655883789,
                    -43.305213928222656,
                    -15.854799270629883,
                    -45.547569274902344,
                    -4.298437595367432,
                    -1.568683385848999,
                    2.7418479919433594,
                    -11.507096290588379,
                    -50.04982376098633,
                    2.3007915019989014,
                    5.939353942871094,
                    -18.981597900390625,
                    -16.463409423828125,
                    -18.580720901489258,
                    -3.7836220264434814,
                    0.7240985035896301,
                    17.69293212890625,
                    -9.487251281738281,
                    -50.6611328125,
                    -15.453362464904785,
                    -2.3199105262756348,
                    -25.51790428161621,
                    -3.313126802444458,
                    1.4291775226593018,
                    2.500075340270996,
                    -25.711105346679688,
                    1.882426381111145,
                    -16.16091537475586,
                    -23.373123168945312,
                    -3.5132968425750732,
                    -24.732072830200195,
                    -27.700281143188477,
                    4.726027965545654,
                    -37.39472579956055,
                    -41.72591018676758,
                    -4.081757545471191,
                    3.7436976432800293,
                    -56.92491149902344,
                    -12.976746559143066,
                    3.614142417907715,
                    -0.09466857463121414,
                    4.458282947540283,
                    -16.475284576416016,
                    -33.28458786010742,
                    4.659688949584961,
                    -21.457752227783203,
                    -39.22913360595703,
                    4.724618911743164,
                    -20.88513946533203,
                    -13.096759796142578,
                    5.140501499176025,
                    -19.489940643310547,
                    -47.14329147338867,
                    -14.799365997314453,
                    -18.765140533447266,
                    -20.087934494018555,
                    -14.420880317687988,
                    -6.6205596923828125,
                    -17.61183738708496,
                    -16.22751808166504,
                    3.504249334335327,
                    -15.9464750289917,
                    -42.503936767578125,
                    -22.88743782043457,
                    6.342711448669434,
                    4.005825042724609,
                    -9.006139755249023,
                    1.1691282987594604,
                    -17.22394371032715,
                    -7.435420513153076,
                    1.315794825553894,
                    -37.77734375,
                    -22.76690101623535,
                    -22.979736328125,
                    0.8209572434425354,
                    2.682723045349121,
                    -1.2028775215148926,
                    -14.484460830688477,
                    -10.851127624511719,
                    -9.339320182800293,
                    -17.509929656982422,
                    4.438039302825928,
                    3.35709285736084,
                    -23.07425880432129,
                    -46.24554443359375,
                    0.959308385848999,
                    -19.38573455810547,
                    -35.5521240234375,
                    3.6439669132232666,
                    -51.30702209472656,
                    -9.378650665283203,
                    -28.46576690673828,
                    -13.113876342773438,
                    5.800864219665527,
                    -7.219264984130859,
                    -33.74116897583008,
                    -16.171091079711914,
                    -24.285675048828125,
                    -30.125667572021484,
                    -5.310688018798828,
                    -44.047218322753906,
                    3.5112650394439697,
                    -35.81993103027344,
                    -32.983001708984375,
                    -31.34107780456543,
                    1.2104607820510864,
                    2.928075075149536,
                    -38.83811950683594,
                    -13.343935012817383,
                    2.1761529445648193,
                    -42.89789962768555,
                    5.5079498291015625,
                    -20.107940673828125,
                    -24.75234031677246,
                    2.358909845352173,
                    -48.06236267089844,
                    -20.399887084960938,
                    -2.9242992401123047,
                    -27.02735710144043,
                    -55.80746841430664,
                    3.6558837890625,
                    -24.56902313232422,
                    -29.17045783996582,
                    -6.068840026855469,
                    -22.184261322021484,
                    -17.401113510131836,
                    -25.346715927124023,
                    -15.458365440368652,
                    -45.90984344482422,
                    3.1354241371154785,
                    -23.16278839111328,
                    2.5992767810821533,
                    -28.567533493041992,
                    -23.966445922851562,
                    -22.537946701049805,
                    -4.179891586303711,
                    -27.094905853271484,
                    -46.93598556518555,
                    -45.66786575317383,
                    -25.203704833984375,
                    1.6003302335739136,
                    -4.579592227935791,
                    -27.624719619750977,
                    -22.07362174987793,
                    -48.453460693359375,
                    -10.64020824432373,
                    -22.54204559326172,
                    -21.163888931274414,
                    3.1218557357788086,
                    -16.75324821472168,
                    -45.418800354003906,
                    -26.679983139038086,
                    -8.322824478149414,
                    -1.79793119430542,
                    -12.529847145080566,
                    -8.876056671142578,
                    -18.586307525634766,
                    2.916672706604004,
                    -27.990652084350586,
                    5.724306106567383,
                    -49.554412841796875,
                    2.6781933307647705,
                    -54.628116607666016,
                    5.425442695617676,
                    -32.64115905761719,
                    -14.968781471252441,
                    -7.084889888763428,
                    -41.0161247253418,
                    -13.203702926635742,
                    -26.1689510345459,
                    -47.43607711791992,
                    -19.84899139404297,
                    -16.771060943603516,
                    -16.223562240600586,
                    -1.7965245246887207,
                    5.627100467681885,
                    1.062442660331726,
                    -46.48942565917969,
                    -36.35879135131836,
                    -50.301658630371094,
                    -42.12896728515625,
                    -19.998985290527344,
                    -1.178498387336731,
                    -26.91268539428711,
                    -39.642311096191406,
                    -17.085508346557617,
                    -21.092405319213867,
                    -51.43650817871094,
                    -21.96946144104004,
                    3.3929576873779297,
                    -24.646196365356445,
                    -25.89952850341797,
                    -22.94940757751465,
                    -11.856252670288086,
                    -33.00667953491211,
                    -2.761845588684082,
                    -7.198315143585205,
                    -21.21609878540039,
                    -43.8115348815918,
                    -23.595497131347656,
                    -33.69797134399414,
                    -34.62378692626953,
                    3.2590668201446533,
                    -25.037076950073242,
                    -52.96902847290039,
                    3.898393392562866,
                    1.1320459842681885,
                    -22.848888397216797,
                    -56.09363555908203,
                    -32.45058822631836,
                    -2.9924747943878174,
                    -12.02886962890625,
                    -3.865690231323242,
                    -1.111945629119873,
                    -57.356468200683594,
                    -12.824349403381348,
                    -27.606807708740234,
                    -22.407909393310547,
                    -25.676342010498047,
                    -13.409008979797363,
                    -32.82294845581055,
                    -20.847990036010742,
                    -27.356698989868164,
                    4.755083084106445,
                    -5.408461093902588,
                    3.7288291454315186,
                    2.353464126586914,
                    -18.094758987426758,
                    3.064800977706909,
                    -25.009157180786133,
                    -51.71805953979492,
                    -51.405887603759766,
                    -21.048128128051758,
                    -33.175201416015625,
                    -44.95121765136719,
                    5.6695780754089355,
                    -13.222515106201172,
                    -26.460342407226562,
                    -27.028039932250977,
                    -35.81352615356445,
                    -21.59893798828125,
                    -11.45799732208252,
                    -57.20973205566406,
                    -18.16937255859375,
                    -31.597028732299805,
                    -58.463417053222656,
                    -33.31927490234375,
                    -22.774337768554688,
                    -0.16954034566879272,
                    3.233515739440918,
                    -57.33368682861328,
                    -9.259799003601074,
                    -24.094974517822266,
                    -18.11408042907715,
                    -45.410301208496094,
                    -61.4493522644043,
                    -45.0149040222168,
                    -24.186664581298828,
                    -9.804941177368164,
                    -24.089874267578125,
                    -17.121925354003906,
                    -60.02864456176758,
                    -24.034534454345703,
                    -57.455989837646484,
                    -51.41348648071289,
                    -44.20246505737305,
                    -26.35558319091797,
                    -24.380407333374023,
                    -43.817325592041016,
                    -30.14337730407715,
                    8.55947494506836,
                    -44.32026290893555,
                    -7.699850082397461,
                    -55.313114166259766,
                    -46.90277099609375,
                    -60.61343002319336,
                    -22.800397872924805,
                    -18.442522048950195,
                    -59.02464294433594,
                    -16.45244598388672,
                    -28.338573455810547,
                    -4.701550483703613,
                    -20.53946304321289,
                    -59.932411193847656,
                    -44.786903381347656,
                    -41.10939025878906,
                    -54.686344146728516,
                    -1.71440851688385,
                    -21.82655906677246,
                    4.690995216369629,
                    -53.79663848876953,
                    -22.240474700927734,
                    -28.690826416015625,
                    -48.49375915527344,
                    -30.4007568359375,
                    -19.247663497924805,
                    -22.419822692871094,
                    -0.2901775538921356,
                    -56.85517883300781,
                    -45.76918411254883,
                    -57.301536560058594,
                    2.8866732120513916,
                    -44.303489685058594,
                    -57.277931213378906,
                    -22.71540069580078,
                    -56.59347152709961,
                    -24.116666793823242,
                    -53.04496765136719,
                    -39.08127975463867,
                    -51.827720642089844,
                    -24.83292579650879,
                    -26.94444465637207,
                    -43.02353286743164,
                    -12.063143730163574,
                    -19.192378997802734,
                    -22.91898536682129,
                    -39.33154296875,
                    -15.139181137084961,
                    -14.837150573730469,
                    -2.2244982719421387,
                    3.268500566482544,
                    -13.769678115844727,
                    -40.633888244628906,
                    -36.12190246582031,
                    -30.61150550842285,
                    -54.4194221496582,
                    -47.94019317626953,
                    -49.75210189819336,
                    -24.952556610107422,
                    -58.39536666870117,
                    -18.762004852294922,
                    -43.489131927490234,
                    -35.19084167480469,
                    -17.040666580200195,
                    -23.634075164794922,
                    -29.815664291381836,
                    -49.706398010253906,
                    -59.97148513793945,
                    -40.862022399902344,
                    -6.405794143676758,
                    -26.91015625,
                    -26.910972595214844,
                    -3.765705108642578,
                    -57.1937255859375,
                    -51.0638313293457,
                    -53.38794708251953,
                    -34.33482360839844,
                    -53.83200454711914,
                    -28.49983024597168,
                    -57.310325622558594,
                    -32.623374938964844,
                    -39.46856689453125,
                    -16.574262619018555,
                    -13.227238655090332,
                    -16.29647445678711,
                    -20.035499572753906,
                    -16.19779396057129,
                    -21.97016716003418,
                    -4.963286876678467,
                    -11.700189590454102,
                    -56.23871612548828,
                    -59.656429290771484,
                    -27.216228485107422,
                    -50.78239059448242,
                    -46.55712890625,
                    -53.749977111816406,
                    -52.30742263793945,
                    -36.71504211425781,
                    -25.622644424438477,
                    -59.74980163574219,
                    -57.94537353515625,
                    -14.57813549041748,
                    -28.366352081298828,
                    -10.105706214904785,
                    -22.624971389770508,
                    -31.421632766723633,
                    -55.69995880126953,
                    -32.51936340332031,
                    -20.79132843017578,
                    -20.28627586364746,
                    -34.580963134765625,
                    -57.02418899536133,
                    -53.08963394165039,
                    -34.652835845947266,
                    -21.09598731994629,
                    -61.79409408569336,
                    -4.797871112823486,
                    -59.41354751586914,
                    -43.74533462524414,
                    -17.480562210083008,
                    -58.81941604614258,
                    -24.830078125,
                    -7.575258255004883,
                    -47.137672424316406,
                    -42.03395080566406,
                    -42.451988220214844,
                    -45.48817443847656,
                    -60.72811508178711,
                    -42.33014678955078,
                    -49.58896255493164,
                    -58.39634323120117,
                    -17.35928726196289,
                    2.7269175052642822,
                    -23.371246337890625,
                    -45.77041244506836,
                    -23.427501678466797,
                    -40.478328704833984,
                    -32.27359390258789,
                    -12.749862670898438,
                    -44.7067756652832,
                    -53.458744049072266,
                    -26.090343475341797,
                    -24.70560646057129,
                    -1.957550287246704,
                    -20.873825073242188,
                    -20.741275787353516,
                    -37.45613479614258,
                    -37.83766555786133,
                    -25.776535034179688,
                    -19.053573608398438,
                    -54.4127197265625,
                    -52.1740837097168,
                    -34.71173858642578,
                    -30.272428512573242,
                    -57.53858947753906,
                    -33.59904098510742,
                    -33.148780822753906,
                    -27.637117385864258,
                    -14.98497486114502,
                    -53.526649475097656,
                    -31.87828254699707,
                    -49.3543586730957,
                    -28.46522331237793,
                    -16.82077980041504,
                    -48.02473068237305,
                    -21.325071334838867,
                    -12.851127624511719,
                    -37.13003158569336,
                    -20.835689544677734,
                    -48.15163803100586,
                    -22.8138370513916,
                    -8.500358581542969,
                    -17.427425384521484,
                    -39.89665222167969,
                    -27.56113052368164,
                    -56.29511260986328,
                    -47.759464263916016,
                    -11.401653289794922,
                    -22.134124755859375,
                    -14.689002990722656,
                    -38.514400482177734,
                    -48.25889205932617,
                    -22.29452133178711,
                    -28.258914947509766,
                    -50.73076629638672,
                    -50.89884567260742,
                    -58.991817474365234,
                    -17.26342010498047,
                    -56.45672607421875,
                    -60.45673751831055,
                    -55.45356369018555,
                    -26.15735626220703,
                    -55.71590805053711,
                    -58.6529655456543,
                    -11.199540138244629,
                    -43.244590759277344,
                    -19.05658721923828,
                    -48.84956741333008,
                    -19.862485885620117,
                    -34.92171859741211,
                    -18.350650787353516,
                    -25.166873931884766,
                    -48.52614974975586,
                    -42.835723876953125,
                    -48.5570182800293,
                    -55.960323333740234,
                    -36.00254821777344,
                    -31.766355514526367,
                    -27.001079559326172,
                    -57.680580139160156,
                    -53.65923309326172,
                    -21.964357376098633,
                    -54.761390686035156,
                    -19.562583923339844,
                    -45.08570861816406,
                    -56.43372344970703,
                    -54.12318420410156,
                    -24.891902923583984,
                    -23.51077651977539,
                    -54.43779754638672,
                    -37.37183380126953,
                    -46.368953704833984,
                    -58.813507080078125,
                    -53.221229553222656,
                    -22.20250701904297,
                    -54.47157669067383,
                    -31.1739559173584,
                    -23.386960983276367,
                    -31.83383560180664,
                    -22.514713287353516,
                    -55.4337158203125,
                    -37.68572998046875,
                    -55.763450622558594,
                    -61.65007400512695,
                    -18.55303192138672,
                    -57.13777160644531,
                    -49.199256896972656,
                    -48.946014404296875,
                    -24.203771591186523,
                    -45.21900939941406,
                    -58.772056579589844,
                    -61.56269836425781,
                    -54.72255325317383,
                    -39.67714309692383,
                    -20.526477813720703,
                    -60.55644989013672,
                    -52.00541687011719,
                    -26.783954620361328,
                    -41.21248245239258,
                    -45.088130950927734,
                    -38.742427825927734,
                    -57.08750534057617,
                    -61.05464172363281,
                    -24.111249923706055,
                    -33.996299743652344,
                    -52.88554382324219,
                    -61.16358947753906,
                    -19.814058303833008,
                    -60.403839111328125,
                    -61.06937026977539,
                    -55.790504455566406,
                    -23.154356002807617,
                    -60.221641540527344,
                    -51.01373291015625,
                    -18.3721866607666,
                    -19.365100860595703,
                    -33.674644470214844,
                    -51.919132232666016,
                    -57.392189025878906,
                    -56.021751403808594,
                    -30.531414031982422,
                    -60.470664978027344,
                    -20.210098266601562,
                    -57.29292678833008,
                    -23.083383560180664,
                    -36.003028869628906,
                    -44.913726806640625,
                    -53.26694869995117,
                    -49.76859664916992,
                    -54.9754524230957,
                    -26.085575103759766,
                    -52.77754592895508,
                    -46.906734466552734,
                    -59.78610610961914,
                    -22.611194610595703,
                    -19.054874420166016,
                    -47.016849517822266,
                    -47.95608139038086,
                    -55.92776870727539,
                    -40.849308013916016,
                    -25.431528091430664,
                    -10.87605094909668,
                    -19.730167388916016,
                    -25.279226303100586,
                    -25.655576705932617,
                    -53.70624923706055,
                    -11.411773681640625,
                    3.5665104389190674,
                    -25.802967071533203,
                    -26.459903717041016,
                    -20.662273406982422,
                    -59.39613342285156,
                    -39.243751525878906,
                    -44.50551986694336,
                    -49.55241012573242,
                    -34.3503303527832,
                    -28.62833595275879,
                    -55.472930908203125,
                    -9.186180114746094,
                    -30.390403747558594,
                    -55.86290740966797,
                    -24.864215850830078,
                    -56.62386703491211,
                    -54.113037109375,
                    -55.59767150878906,
                    -31.901817321777344,
                    -61.08401107788086,
                    -51.980499267578125,
                    -53.27214813232422,
                    -48.41675567626953,
                    -20.08293914794922,
                    -41.30412292480469,
                    -50.64435958862305,
                    -34.28408432006836,
                    -22.23907470703125,
                    -54.60043716430664,
                    -46.11091995239258,
                    -56.137996673583984,
                    -56.133060455322266,
                    -60.87278366088867,
                    -53.78764724731445,
                    -27.10938835144043,
                    -53.085601806640625,
                    -36.74690628051758,
                    -47.30632400512695,
                    -45.562137603759766,
                    -57.96723937988281,
                    -32.01554489135742,
                    -50.38372039794922,
                    -20.75240707397461,
                    -24.35267448425293,
                    -45.95016098022461,
                    -46.782135009765625,
                    -58.217987060546875,
                    -22.04914093017578,
                    -54.90667724609375,
                    -56.548606872558594,
                    -51.8326416015625,
                    -50.36680603027344,
                    -58.59808349609375,
                    -55.10395431518555,
                    -58.01259231567383,
                    -50.7581901550293,
                    -38.0921745300293,
                    -34.46891403198242,
                    -46.83733367919922,
                    -56.36453628540039,
                    -15.979307174682617,
                    -24.43639373779297,
                    -56.8753662109375,
                    -31.32452392578125,
                    -41.17765808105469,
                    -56.01616287231445,
                    -50.44327926635742,
                    -38.35037612915039,
                    -46.86168670654297,
                    -56.05404281616211,
                    -55.28717803955078,
                    -32.30078125,
                    -60.68704605102539,
                    -51.399959564208984,
                    -57.49631881713867,
                    -44.92483139038086,
                    -29.392942428588867,
                    -33.106502532958984,
                    -46.552059173583984,
                    -27.456174850463867,
                    -25.262187957763672,
                    -50.550540924072266,
                    -24.392318725585938,
                    -22.66566276550293,
                    -54.73454666137695,
                    -24.831300735473633,
                    -14.822196960449219,
                    -55.00148010253906,
                    -37.009429931640625,
                    -45.55710983276367,
                    -58.27265167236328,
                    -46.057865142822266,
                    -57.94807434082031,
                    -24.756271362304688,
                    -56.16872024536133,
                    -52.91830062866211,
                    -24.14725112915039,
                    -58.824092864990234,
                    -58.23480224609375,
                    -22.92374038696289,
                    -19.561616897583008,
                    -46.134483337402344,
                    -53.359127044677734,
                    -32.27759552001953,
                    -41.9996452331543,
                    -57.90174102783203,
                    -51.48514938354492,
                    -34.783321380615234,
                    -22.827219009399414,
                    -52.88411331176758,
                    -15.558460235595703,
                    -55.660308837890625,
                    -52.155555725097656,
                    -28.33424186706543,
                    -57.57941818237305,
                    -26.115779876708984,
                    -50.700599670410156,
                    -43.93034362792969,
                    4.795296669006348,
                    -60.3317985534668,
                    -46.19513702392578,
                    -23.56117820739746,
                    -42.301971435546875,
                    -57.717838287353516,
                    -58.554542541503906,
                    -61.60424041748047,
                    -29.976152420043945,
                    -46.55694580078125,
                    -49.078369140625,
                    -59.840858459472656,
                    -24.017560958862305,
                    -39.16007995605469,
                    -54.04932403564453,
                    -27.756282806396484,
                    -52.93355178833008,
                    -49.21672821044922,
                    -47.79195022583008,
                    -56.890586853027344,
                    -60.23253631591797,
                    -59.08201217651367,
                    -37.39566421508789,
                    -43.37019729614258,
                    -36.307395935058594,
                    -57.51348876953125,
                    -58.2271614074707,
                    -55.220733642578125,
                    -59.380313873291016,
                    -61.0298957824707,
                    -31.273229598999023,
                    -54.656097412109375,
                    -61.26728057861328,
                    -32.749794006347656,
                    -57.2989387512207,
                    -55.81544876098633,
                    -52.86172866821289,
                    -61.35641860961914,
                    -39.542694091796875,
                    -28.69170570373535,
                    -57.84256362915039,
                    -57.38410568237305,
                    -58.26475143432617,
                    -24.64010238647461,
                    -31.712820053100586,
                    -38.716712951660156,
                    -56.680809020996094,
                    -53.271568298339844,
                    -59.27810287475586,
                    -58.916893005371094,
                    -28.768247604370117,
                    -52.22929000854492,
                    -43.13962173461914,
                    -54.486778259277344,
                    -55.9432258605957,
                    -56.01138687133789,
                    -16.01112937927246
                  ],
                  "y": [
                    -16.361820220947266,
                    -15.779809951782227,
                    -16.335853576660156,
                    -16.253681182861328,
                    -16.254066467285156,
                    -14.550056457519531,
                    -15.907087326049805,
                    -16.08872413635254,
                    -16.308109283447266,
                    -15.759589195251465,
                    -15.6624755859375,
                    -16.2615909576416,
                    -16.225542068481445,
                    -15.337106704711914,
                    -15.844223976135254,
                    -15.41841983795166,
                    -15.132871627807617,
                    -15.8972806930542,
                    -15.98827838897705,
                    -15.94027042388916,
                    -15.83392333984375,
                    -15.746566772460938,
                    -15.803404808044434,
                    -16.009958267211914,
                    -15.645245552062988,
                    -14.883587837219238,
                    -15.937519073486328,
                    -16.1302490234375,
                    -15.671626091003418,
                    -16.05422019958496,
                    -16.03171730041504,
                    -15.7283296585083,
                    -16.084733963012695,
                    -16.02001953125,
                    -15.855358123779297,
                    -14.282817840576172,
                    -16.447328567504883,
                    -15.856416702270508,
                    -14.195028305053711,
                    -16.141023635864258,
                    -15.987025260925293,
                    -16.298168182373047,
                    -14.250309944152832,
                    -16.11623191833496,
                    -13.03703498840332,
                    -16.094940185546875,
                    -16.027807235717773,
                    -16.32186508178711,
                    -15.38267993927002,
                    -15.840178489685059,
                    -16.06751251220703,
                    -14.273711204528809,
                    -15.711495399475098,
                    -14.816160202026367,
                    -15.404013633728027,
                    -15.41186237335205,
                    -16.04846954345703,
                    -15.88448715209961,
                    -15.76944637298584,
                    -16.130168914794922,
                    -15.54837703704834,
                    -15.320830345153809,
                    -14.976142883300781,
                    -15.13509750366211,
                    -14.925743103027344,
                    -16.301477432250977,
                    -13.942815780639648,
                    -15.347522735595703,
                    -12.026328086853027,
                    -14.893272399902344,
                    -9.89132308959961,
                    -9.103365898132324,
                    -14.558870315551758,
                    -12.634275436401367,
                    -14.436299324035645,
                    -13.46353816986084,
                    -13.723312377929688,
                    -5.606637477874756,
                    -13.630328178405762,
                    -14.980585098266602,
                    -12.140780448913574,
                    -11.307210922241211,
                    -9.52402114868164,
                    -14.705153465270996,
                    -10.356988906860352,
                    -13.290812492370605,
                    -8.931989669799805,
                    -8.734233856201172,
                    -14.87905502319336,
                    -11.583441734313965,
                    -14.10488510131836,
                    -6.007915496826172,
                    -10.632467269897461,
                    -9.158368110656738,
                    -8.240277290344238,
                    -12.144639015197754,
                    -7.682247638702393,
                    -10.88007640838623,
                    -9.947218894958496,
                    -4.80606746673584,
                    -9.302791595458984,
                    -11.447587966918945,
                    -10.27541446685791,
                    6.295204162597656,
                    -11.608513832092285,
                    -8.548639297485352,
                    5.191487789154053,
                    -7.2415452003479,
                    -2.8531441688537598,
                    -8.827313423156738,
                    -8.819413185119629,
                    -8.733797073364258,
                    -4.430912494659424,
                    -1.1800084114074707,
                    -2.1654129028320312,
                    -3.031647205352783,
                    0.28122538328170776,
                    -7.129714012145996,
                    -1.6940820217132568,
                    -4.944301605224609,
                    -5.4776291847229,
                    -0.4566078186035156,
                    -6.129400253295898,
                    1.6796592473983765,
                    -12.062687873840332,
                    2.4539098739624023,
                    8.6730375289917,
                    -1.3960827589035034,
                    -4.679455280303955,
                    3.7687554359436035,
                    7.726240158081055,
                    -3.528660774230957,
                    -9.310403823852539,
                    9.270098686218262,
                    3.8394763469696045,
                    -4.734541416168213,
                    -2.0096726417541504,
                    -3.408454656600952,
                    2.639431953430176,
                    6.070566177368164,
                    -0.18799622356891632,
                    4.52279806137085,
                    -2.4575603008270264,
                    9.681605339050293,
                    0.33899953961372375,
                    -4.569856643676758,
                    3.8706157207489014,
                    1.785160779953003,
                    7.801499843597412,
                    1.722807765007019,
                    -0.6258862018585205,
                    -7.090283393859863,
                    4.148773193359375,
                    7.360567569732666,
                    -1.3442491292953491,
                    -6.180909633636475,
                    -3.546621084213257,
                    0.7166469097137451,
                    -3.024249315261841,
                    -5.348948001861572,
                    15.90164852142334,
                    14.678016662597656,
                    -5.102844715118408,
                    0.8748424053192139,
                    -5.587430477142334,
                    -6.48853874206543,
                    13.04361343383789,
                    8.460409164428711,
                    -6.680020809173584,
                    6.660907745361328,
                    9.145901679992676,
                    4.577164173126221,
                    0.5574796199798584,
                    23.262434005737305,
                    -0.21271641552448273,
                    5.126805782318115,
                    23.00533103942871,
                    8.65401554107666,
                    4.368174076080322,
                    4.122507572174072,
                    5.306412220001221,
                    3.255958318710327,
                    15.04498291015625,
                    7.875966548919678,
                    8.325067520141602,
                    1.6526601314544678,
                    14.871867179870605,
                    1.6188009977340698,
                    21.913148880004883,
                    16.919200897216797,
                    5.632134437561035,
                    17.93732261657715,
                    -1.769584059715271,
                    -0.3041037321090698,
                    9.365058898925781,
                    12.167717933654785,
                    22.557514190673828,
                    12.405478477478027,
                    15.730504989624023,
                    13.467552185058594,
                    -0.7011029124259949,
                    11.787775039672852,
                    5.1388421058654785,
                    1.1421014070510864,
                    13.996611595153809,
                    20.845108032226562,
                    22.136438369750977,
                    20.40630531311035,
                    4.121264457702637,
                    22.439197540283203,
                    9.747151374816895,
                    19.46346092224121,
                    17.901527404785156,
                    18.4517879486084,
                    5.442156791687012,
                    6.169061660766602,
                    7.344395637512207,
                    18.322479248046875,
                    17.959823608398438,
                    14.665425300598145,
                    13.388880729675293,
                    5.556291103363037,
                    12.459091186523438,
                    13.617143630981445,
                    19.019458770751953,
                    22.672502517700195,
                    8.238065719604492,
                    11.532354354858398,
                    15.434355735778809,
                    21.608842849731445,
                    15.315648078918457,
                    15.685540199279785,
                    17.23826789855957,
                    22.084321975708008,
                    21.489702224731445,
                    19.644943237304688,
                    21.500322341918945,
                    12.290305137634277,
                    16.94744300842285,
                    22.281085968017578,
                    22.049165725708008,
                    21.905466079711914,
                    15.978676795959473,
                    21.957502365112305,
                    22.631105422973633,
                    21.234161376953125,
                    12.276490211486816,
                    18.036808013916016,
                    21.102216720581055,
                    22.328542709350586,
                    9.763017654418945,
                    19.40060806274414,
                    15.942572593688965,
                    7.513823509216309,
                    1.6276730298995972,
                    17.298564910888672,
                    22.459802627563477,
                    19.299549102783203,
                    18.67534828186035,
                    14.244789123535156,
                    18.565271377563477,
                    20.681467056274414,
                    22.936595916748047,
                    20.688922882080078,
                    22.36746597290039,
                    8.451547622680664,
                    22.654457092285156,
                    12.21751880645752,
                    21.824769973754883,
                    21.440153121948242,
                    10.476880073547363,
                    12.93051528930664,
                    19.970487594604492,
                    20.919898986816406,
                    13.100805282592773,
                    16.14357566833496,
                    21.863590240478516,
                    16.60774803161621,
                    14.53182601928711,
                    21.703250885009766,
                    20.322996139526367,
                    17.433115005493164,
                    14.965351104736328,
                    22.43239402770996,
                    18.760120391845703,
                    6.91385555267334,
                    22.475189208984375,
                    19.075786590576172,
                    20.87996482849121,
                    22.347511291503906,
                    20.192440032958984,
                    20.292095184326172,
                    19.855852127075195,
                    19.923770904541016,
                    15.612548828125,
                    15.612326622009277,
                    12.001120567321777,
                    21.89857292175293,
                    3.023167848587036,
                    -21.417776107788086,
                    16.567337036132812,
                    21.398088455200195,
                    13.963879585266113,
                    -13.459354400634766,
                    21.56645393371582,
                    17.729642868041992,
                    21.161808013916016,
                    0.13198646903038025,
                    12.203413009643555,
                    21.595422744750977,
                    16.219640731811523,
                    14.897159576416016,
                    19.872188568115234,
                    20.38953971862793,
                    11.741121292114258,
                    22.231595993041992,
                    19.878318786621094,
                    19.301620483398438,
                    -1.0704823732376099,
                    17.69011878967285,
                    19.352642059326172,
                    20.929643630981445,
                    21.61830711364746,
                    -6.435362815856934,
                    15.227154731750488,
                    16.03811264038086,
                    14.601219177246094,
                    17.969728469848633,
                    16.451993942260742,
                    5.121517181396484,
                    22.44900894165039,
                    10.06734848022461,
                    16.275875091552734,
                    7.29561185836792,
                    4.9592604637146,
                    20.557310104370117,
                    13.331310272216797,
                    -21.76421356201172,
                    14.90243911743164,
                    21.076555252075195,
                    11.581657409667969,
                    20.284456253051758,
                    22.69734001159668,
                    14.994220733642578,
                    13.233633995056152,
                    21.246170043945312,
                    15.31728458404541,
                    16.12630271911621,
                    13.926749229431152,
                    18.807876586914062,
                    22.19721794128418,
                    3.0700910091400146,
                    16.911060333251953,
                    -18.509916305541992,
                    16.629060745239258,
                    19.457128524780273,
                    -19.389902114868164,
                    3.091679811477661,
                    11.160059928894043,
                    -8.899198532104492,
                    13.54362678527832,
                    -8.299174308776855,
                    -0.21261946856975555,
                    17.794315338134766,
                    -17.951980590820312,
                    -17.613962173461914,
                    -1.0093016624450684,
                    20.496217727661133,
                    22.33271598815918,
                    19.60747528076172,
                    11.853466987609863,
                    -13.986903190612793,
                    15.985269546508789,
                    21.60236167907715,
                    -21.528425216674805,
                    18.3967342376709,
                    -5.7601447105407715,
                    12.828973770141602,
                    4.6985182762146,
                    17.30315589904785,
                    15.023802757263184,
                    6.740036487579346,
                    7.120438098907471,
                    -5.480535984039307,
                    14.43471908569336,
                    15.384880065917969,
                    -11.790454864501953,
                    21.848979949951172,
                    22.684951782226562,
                    10.635125160217285,
                    1.9542204141616821,
                    21.622825622558594,
                    -15.631025314331055,
                    -6.916806697845459,
                    -7.4610772132873535,
                    -19.740015029907227,
                    -5.330191612243652,
                    14.63255500793457,
                    16.882278442382812,
                    -12.8677396774292,
                    18.859060287475586,
                    -3.686506748199463,
                    10.867026329040527,
                    19.91597557067871,
                    -21.991844177246094,
                    18.823591232299805,
                    12.871410369873047,
                    12.377861976623535,
                    -19.206151962280273,
                    -15.074134826660156,
                    -26.594709396362305,
                    6.1735944747924805,
                    7.93303918838501,
                    17.01129913330078,
                    21.749786376953125,
                    5.567785739898682,
                    1.2873491048812866,
                    -21.534503936767578,
                    -5.740913391113281,
                    -19.455490112304688,
                    5.205634117126465,
                    22.013221740722656,
                    -23.897380828857422,
                    -2.1068873405456543,
                    -11.040709495544434,
                    -0.4954468011856079,
                    0.2528064548969269,
                    -22.496339797973633,
                    -7.513643741607666,
                    -20.329837799072266,
                    5.975646018981934,
                    12.569978713989258,
                    -20.117733001708984,
                    11.95559024810791,
                    -20.573863983154297,
                    11.406901359558105,
                    -22.982358932495117,
                    -5.788583278656006,
                    6.415911674499512,
                    1.105329990386963,
                    2.504546880722046,
                    2.5706048011779785,
                    7.2475199699401855,
                    3.6657590866088867,
                    -9.73564624786377,
                    18.0886287689209,
                    -3.1128158569335938,
                    3.8864970207214355,
                    -21.321823120117188,
                    22.063201904296875,
                    21.698833465576172,
                    -24.07575225830078,
                    -13.293054580688477,
                    -10.02605152130127,
                    0.2790778875350952,
                    4.294855117797852,
                    -23.68456268310547,
                    -20.193500518798828,
                    -17.902545928955078,
                    -8.599343299865723,
                    6.825785160064697,
                    -7.213260650634766,
                    4.049033164978027,
                    -5.538397312164307,
                    -6.398801326751709,
                    -0.33786365389823914,
                    -23.25450325012207,
                    4.8202362060546875,
                    -6.12894868850708,
                    -16.621877670288086,
                    -6.113379955291748,
                    -26.587223052978516,
                    -19.264795303344727,
                    -11.762910842895508,
                    -24.532424926757812,
                    7.106715679168701,
                    0.8782857656478882,
                    21.055484771728516,
                    16.292083740234375,
                    8.52957820892334,
                    -24.017139434814453,
                    -6.54494571685791,
                    -21.350099563598633,
                    11.573002815246582,
                    -16.159025192260742,
                    -23.746530532836914,
                    -0.3788297474384308,
                    -16.083070755004883,
                    3.6467995643615723,
                    -15.518975257873535,
                    14.174698829650879,
                    -14.328801155090332,
                    -9.516327857971191,
                    -24.74104881286621,
                    -6.284468173980713,
                    -16.793764114379883,
                    -7.210733413696289,
                    -22.3868408203125,
                    -24.431543350219727,
                    -6.667901515960693,
                    6.455279350280762,
                    -11.039615631103516,
                    -16.865257263183594,
                    -8.998785972595215,
                    -16.668176651000977,
                    13.122124671936035,
                    -12.340916633605957,
                    -19.45224380493164,
                    -25.392263412475586,
                    -21.057641983032227,
                    -25.694507598876953,
                    -18.627471923828125,
                    5.276254177093506,
                    -6.637482643127441,
                    -22.204952239990234,
                    -0.2416606992483139,
                    -19.049596786499023,
                    -8.209649085998535,
                    -24.651588439941406,
                    -11.845846176147461,
                    -26.781333923339844,
                    -10.558191299438477,
                    -17.561853408813477,
                    3.8279848098754883,
                    15.569731712341309,
                    -26.69011116027832,
                    -16.159542083740234,
                    -9.94363784790039,
                    -23.84968376159668,
                    -16.375499725341797,
                    -15.766866683959961,
                    -18.375085830688477,
                    -9.617822647094727,
                    -8.262994766235352,
                    1.0874812602996826,
                    14.898786544799805,
                    -23.950801849365234,
                    -25.749513626098633,
                    -21.356678009033203,
                    -0.1266985684633255,
                    1.8040132522583008,
                    -6.726266860961914,
                    -10.641012191772461,
                    -25.497997283935547,
                    -10.864068031311035,
                    -24.392852783203125,
                    12.730657577514648,
                    -17.230449676513672,
                    -23.768434524536133,
                    0.701216459274292,
                    -21.161117553710938,
                    -14.089804649353027,
                    -24.210689544677734,
                    -24.441650390625,
                    -8.44989013671875,
                    -13.430133819580078,
                    -5.894180774688721,
                    8.614935874938965,
                    18.18461799621582,
                    0.43780890107154846,
                    -26.54043197631836,
                    -18.715877532958984,
                    -24.478670120239258,
                    -18.97724723815918,
                    -22.841022491455078,
                    -18.84634017944336,
                    -22.18474006652832,
                    -9.060624122619629,
                    -24.942750930786133,
                    -9.945076942443848,
                    -21.373035430908203,
                    -20.41623306274414,
                    -24.483776092529297,
                    22.340124130249023,
                    -21.391136169433594,
                    -17.071651458740234,
                    4.775672435760498,
                    -12.637828826904297,
                    -6.254934310913086,
                    -17.518400192260742,
                    0.27419257164001465,
                    -21.38163185119629,
                    -15.090910911560059,
                    20.34573745727539,
                    -7.945589065551758,
                    -13.265302658081055,
                    -5.74897575378418,
                    -8.426911354064941,
                    -15.321724891662598,
                    -23.21902847290039,
                    -6.361571788787842,
                    -21.902685165405273,
                    8.170926094055176,
                    5.053738594055176,
                    -25.478530883789062,
                    -25.27042579650879,
                    -23.519908905029297,
                    -11.559473037719727,
                    -22.66109275817871,
                    -26.33126449584961,
                    -16.63715934753418,
                    -4.640350341796875,
                    5.349389553070068,
                    -23.348161697387695,
                    -10.133673667907715,
                    -20.646272659301758,
                    -27.026897430419922,
                    -8.61355972290039,
                    -24.20094108581543,
                    -19.350387573242188,
                    -5.313682556152344,
                    -17.387836456298828,
                    -23.0960693359375,
                    -24.152097702026367,
                    -23.751806259155273,
                    -10.748003959655762,
                    -24.024673461914062,
                    -26.1068172454834,
                    -23.264629364013672,
                    -0.18423058092594147,
                    -0.6170899271965027,
                    -24.24873161315918,
                    -0.3567231297492981,
                    -22.18157386779785,
                    -11.065390586853027,
                    -18.765586853027344,
                    -15.63676643371582,
                    3.965858221054077,
                    -19.5849609375,
                    -20.339698791503906,
                    -26.188779830932617,
                    -23.967321395874023,
                    5.540804386138916,
                    -7.910872936248779,
                    -24.67304801940918,
                    3.1020679473876953,
                    -22.177806854248047,
                    -13.37882137298584,
                    17.123640060424805,
                    -18.265596389770508,
                    -23.470003128051758,
                    -25.57682228088379,
                    -7.410275459289551,
                    -23.020492553710938,
                    -24.834402084350586,
                    -23.37571907043457,
                    10.137938499450684,
                    -12.680060386657715,
                    -18.878860473632812,
                    14.433109283447266,
                    -7.020318984985352,
                    -25.086841583251953,
                    -17.466476440429688,
                    -24.40730857849121,
                    -9.54043960571289,
                    -24.476682662963867,
                    -20.18450355529785,
                    -24.787025451660156,
                    -23.104970932006836,
                    -22.198875427246094,
                    -15.632608413696289,
                    21.795005798339844,
                    -22.221460342407227,
                    2.7476375102996826,
                    4.363931179046631,
                    -20.697120666503906,
                    -23.2453556060791,
                    -15.231359481811523,
                    17.392148971557617,
                    -25.332096099853516,
                    -21.291629791259766,
                    -26.461578369140625,
                    -4.83599853515625,
                    0.7642396092414856,
                    -23.875865936279297,
                    -21.189451217651367,
                    4.651265621185303,
                    -1.2829662561416626,
                    4.2460103034973145,
                    1.1224416494369507,
                    -22.06045913696289,
                    -10.642581939697266,
                    -24.572834014892578,
                    1.9373729228973389,
                    11.306924819946289,
                    1.5894805192947388,
                    22.274765014648438,
                    9.24215316772461,
                    -23.695941925048828,
                    -11.628920555114746,
                    -11.921716690063477,
                    -10.944352149963379,
                    -25.570959091186523,
                    13.556899070739746,
                    16.007009506225586,
                    20.09746742248535,
                    -22.265501022338867,
                    -22.3950252532959,
                    -18.1123104095459,
                    12.390761375427246,
                    -25.131376266479492,
                    -19.64757537841797,
                    2.459778308868408,
                    -23.815906524658203,
                    18.48007583618164,
                    15.909793853759766,
                    -10.482934951782227,
                    -14.879375457763672,
                    4.799810886383057,
                    6.1731061935424805,
                    -25.94617462158203,
                    12.802408218383789,
                    12.303811073303223,
                    -12.32595157623291,
                    -15.47110652923584,
                    -3.1750450134277344,
                    18.773839950561523,
                    -23.60097312927246,
                    17.52791404724121,
                    12.353050231933594,
                    1.2193065881729126,
                    -13.385440826416016,
                    -20.6785888671875,
                    -13.838824272155762,
                    22.34728240966797,
                    13.878340721130371,
                    1.0788578987121582,
                    13.034238815307617,
                    -11.220276832580566,
                    -16.36496925354004,
                    -24.430309295654297,
                    -24.91350555419922,
                    -23.174724578857422,
                    -22.283349990844727,
                    4.402223110198975,
                    19.061283111572266,
                    -9.88301944732666,
                    16.266250610351562,
                    -6.721324920654297,
                    -19.311098098754883,
                    2.6984705924987793,
                    -11.905865669250488,
                    -10.490737915039062,
                    20.58790397644043,
                    -17.574600219726562,
                    14.381814002990723,
                    10.371413230895996,
                    -6.354256629943848,
                    7.588992118835449,
                    -17.31023597717285,
                    -20.03268051147461,
                    -5.267254829406738,
                    17.579452514648438,
                    -11.311470031738281,
                    -19.65898323059082,
                    -14.220335960388184,
                    -24.2360782623291,
                    -5.035735130310059,
                    -9.187877655029297,
                    -23.737092971801758,
                    6.6303935050964355,
                    -13.480283737182617,
                    -24.185558319091797,
                    -2.3886525630950928,
                    12.365144729614258,
                    22.852487564086914,
                    17.73923110961914,
                    0.29406896233558655,
                    -23.205907821655273,
                    17.657398223876953,
                    -15.44698715209961,
                    -24.748451232910156,
                    -13.068167686462402,
                    -24.687833786010742,
                    -17.64620590209961,
                    -13.342121124267578,
                    -24.877723693847656,
                    -5.627381324768066,
                    19.195287704467773,
                    -20.592294692993164,
                    -22.822628021240234,
                    -12.392745018005371,
                    -5.29685640335083,
                    18.3966064453125,
                    -5.612801551818848,
                    -12.70975399017334,
                    -1.9915599822998047,
                    -4.86052942276001,
                    5.028310775756836,
                    14.93230152130127,
                    -7.39061975479126,
                    12.742622375488281,
                    -20.197771072387695,
                    2.9305307865142822,
                    13.689915657043457,
                    8.440997123718262,
                    16.435916900634766,
                    -23.659034729003906,
                    -16.91691780090332,
                    -22.93045997619629,
                    -2.243581771850586,
                    8.212081909179688,
                    -20.480220794677734,
                    0.6934813261032104,
                    22.139511108398438,
                    -0.11230017989873886,
                    -2.2526848316192627,
                    7.993847370147705,
                    23.618736267089844,
                    -16.094560623168945,
                    10.416107177734375,
                    -8.018412590026855,
                    8.572928428649902,
                    16.689115524291992,
                    19.43795394897461,
                    -25.233823776245117,
                    16.62154769897461,
                    16.25724983215332,
                    -17.876220703125,
                    10.144597053527832,
                    -14.463724136352539,
                    3.522097110748291,
                    -12.005525588989258,
                    0.5210118293762207,
                    -22.10284423828125,
                    10.028314590454102,
                    12.109990119934082,
                    4.14377498626709,
                    18.98324966430664,
                    9.87398910522461,
                    -19.803491592407227,
                    -4.5534257888793945,
                    -26.494836807250977,
                    -20.69808578491211,
                    17.422630310058594,
                    17.75004768371582,
                    20.991455078125,
                    -18.99625587463379,
                    17.6561336517334,
                    19.310501098632812,
                    7.483354568481445,
                    16.069660186767578,
                    -25.288917541503906,
                    -13.234679222106934,
                    -9.675678253173828,
                    10.585102081298828,
                    3.060117721557617,
                    -13.552643775939941,
                    11.511651992797852,
                    2.7345316410064697,
                    17.246116638183594,
                    -7.868590354919434,
                    20.073246002197266,
                    -8.265401840209961,
                    8.523849487304688,
                    -9.67528247833252,
                    -0.15115319192409515,
                    8.12116813659668,
                    -21.01523780822754,
                    6.193255424499512,
                    -19.81108856201172,
                    3.3923308849334717,
                    21.675750732421875,
                    -22.212383270263672,
                    -1.2714791297912598,
                    -18.361547470092773,
                    -20.704593658447266,
                    -18.98511505126953,
                    20.112041473388672,
                    14.106383323669434,
                    -9.33575439453125,
                    1.880812644958496,
                    11.693781852722168,
                    -3.9543275833129883,
                    18.030925750732422,
                    -3.8947346210479736,
                    18.740171432495117,
                    -7.57155179977417,
                    21.20557975769043,
                    1.205051064491272,
                    -5.889257431030273,
                    10.484153747558594,
                    9.02269172668457,
                    4.793694972991943,
                    -22.84680938720703,
                    -7.950101852416992,
                    18.848642349243164,
                    21.412921905517578,
                    15.389752388000488,
                    14.383732795715332,
                    -9.089735984802246,
                    -8.356694221496582,
                    14.45185661315918,
                    20.323776245117188,
                    2.9695520401000977,
                    3.803563117980957,
                    11.51827335357666,
                    -0.9749449491500854,
                    -4.59013557434082,
                    0.09506220370531082,
                    17.704126358032227,
                    22.027965545654297,
                    9.611307144165039,
                    11.204870223999023,
                    -16.062496185302734,
                    -5.499895095825195,
                    0.3347187936306,
                    15.96102237701416,
                    -7.380648136138916,
                    -8.898591041564941,
                    21.32649040222168,
                    17.668628692626953,
                    13.01773452758789,
                    1.8311147689819336,
                    22.7720890045166,
                    -5.112704753875732,
                    23.763717651367188,
                    3.9195079803466797,
                    6.2355828285217285,
                    -23.3922176361084,
                    20.469112396240234,
                    -3.6726138591766357,
                    17.78407859802246,
                    13.255943298339844,
                    -0.23533353209495544,
                    14.40190315246582,
                    9.714678764343262,
                    9.513166427612305,
                    8.563947677612305,
                    22.171241760253906,
                    -25.096595764160156,
                    -5.614969253540039,
                    -11.389636039733887,
                    21.551244735717773,
                    -6.946238994598389,
                    7.685208320617676,
                    19.66855812072754,
                    -3.9942097663879395,
                    -6.311210632324219,
                    20.126012802124023,
                    10.611588478088379,
                    -16.67028045654297,
                    17.631526947021484,
                    18.842615127563477,
                    -11.729116439819336,
                    -8.77836799621582,
                    -2.8112080097198486,
                    17.759109497070312,
                    -6.199103832244873,
                    21.275033950805664,
                    -19.607542037963867,
                    12.156052589416504,
                    -7.947963237762451,
                    16.407989501953125,
                    -24.382097244262695,
                    10.339624404907227,
                    -4.819643020629883,
                    3.8255860805511475,
                    6.462032318115234,
                    -23.601463317871094,
                    21.284164428710938,
                    -25.485620498657227,
                    5.617008209228516,
                    -4.443841934204102,
                    20.325790405273438,
                    -19.806795120239258,
                    6.966818809509277,
                    -7.795088291168213,
                    17.605337142944336,
                    21.68478012084961,
                    20.413471221923828,
                    6.114513397216797,
                    -22.914888381958008,
                    -13.972010612487793,
                    8.504727363586426,
                    -8.131278991699219,
                    20.498403549194336,
                    14.731219291687012,
                    22.38031768798828,
                    14.03779125213623,
                    -7.082364082336426,
                    3.4424688816070557,
                    21.433364868164062,
                    5.409740924835205,
                    15.563382148742676,
                    15.148000717163086,
                    1.5295497179031372,
                    8.287322998046875,
                    -3.001478910446167,
                    -8.122086524963379,
                    3.2484824657440186,
                    7.7488884925842285,
                    6.358916759490967,
                    -8.54409122467041,
                    8.830432891845703,
                    18.244380950927734,
                    -14.366067886352539,
                    -10.71274185180664,
                    9.235980033874512,
                    20.545209884643555,
                    -12.745397567749023,
                    18.809255599975586,
                    14.045966148376465,
                    -13.983137130737305,
                    20.704166412353516,
                    -11.51880168914795,
                    -13.497330665588379,
                    -5.429079055786133,
                    -11.843555450439453,
                    -12.10684585571289,
                    18.28461456298828,
                    -9.029666900634766,
                    20.3558349609375,
                    19.521955490112305,
                    3.45194149017334,
                    22.822114944458008,
                    -2.0392203330993652,
                    17.045820236206055,
                    7.287178993225098,
                    7.155648708343506,
                    15.506455421447754,
                    12.305689811706543,
                    18.637821197509766,
                    -14.099349975585938,
                    -22.528852462768555,
                    19.646472930908203,
                    -3.4975409507751465,
                    20.355653762817383,
                    22.40224266052246,
                    2.2598378658294678,
                    19.243623733520508,
                    -19.07350730895996,
                    19.50234603881836,
                    -1.1930700540542603,
                    15.35898494720459,
                    11.577619552612305,
                    -6.561381816864014,
                    21.5494327545166,
                    -3.9937024116516113,
                    -0.30431458353996277,
                    -4.509463310241699,
                    -14.957520484924316,
                    -14.331184387207031,
                    22.75275421142578,
                    -6.709840774536133,
                    6.169892311096191,
                    17.97254753112793,
                    5.943795204162598,
                    20.1517391204834,
                    10.559906005859375,
                    13.762832641601562,
                    -13.4193115234375,
                    -1.6052665710449219,
                    6.338919162750244,
                    -10.158003807067871,
                    -1.8192254304885864,
                    14.443822860717773,
                    16.45521354675293,
                    5.6809587478637695,
                    7.826050758361816,
                    -11.155962944030762,
                    20.382030487060547,
                    14.782792091369629,
                    14.521286964416504,
                    21.566761016845703,
                    21.37339210510254,
                    7.613326549530029,
                    19.55816078186035,
                    2.837855815887451,
                    17.723384857177734,
                    -10.830771446228027,
                    11.234725952148438,
                    2.8394289016723633,
                    12.91047477722168,
                    21.26911163330078,
                    -12.647017478942871,
                    12.413564682006836,
                    19.547130584716797,
                    17.743925094604492,
                    -13.125130653381348,
                    -1.953630805015564,
                    11.400617599487305,
                    12.9284086227417,
                    6.568004608154297,
                    18.82811164855957,
                    13.905067443847656,
                    19.41634178161621,
                    19.930856704711914,
                    16.165124893188477,
                    -23.480091094970703,
                    5.43259334564209,
                    1.0517287254333496,
                    -7.033051490783691,
                    14.952067375183105,
                    9.488622665405273,
                    -14.678818702697754,
                    -6.552938938140869,
                    -2.2716662883758545,
                    21.44144058227539,
                    -7.5433878898620605,
                    18.793441772460938,
                    9.909852981567383,
                    20.89454460144043,
                    -13.865548133850098,
                    15.167969703674316,
                    6.1832122802734375,
                    -6.09041166305542,
                    20.935937881469727,
                    -13.210233688354492,
                    -0.883230984210968,
                    -14.381515502929688,
                    -10.541876792907715,
                    -12.636337280273438,
                    19.280109405517578,
                    4.014751434326172,
                    -11.289289474487305,
                    16.890417098999023,
                    13.128633499145508,
                    -11.245570182800293,
                    10.039063453674316,
                    9.905296325683594,
                    -7.5543212890625,
                    14.855836868286133,
                    -6.276841163635254,
                    12.990799903869629,
                    17.530290603637695,
                    -6.534223556518555,
                    5.053032398223877,
                    3.229681968688965,
                    -5.374629497528076,
                    -7.600494384765625,
                    18.726781845092773,
                    -0.4937228262424469,
                    17.90277671813965,
                    3.637967824935913,
                    16.60931396484375,
                    -9.835807800292969,
                    -20.37779426574707,
                    2.7313992977142334,
                    0.2616932988166809,
                    -8.95548152923584,
                    8.580424308776855,
                    4.380098819732666,
                    22.60605812072754,
                    -0.10458570718765259,
                    -10.144373893737793,
                    -15.417024612426758,
                    5.242471694946289,
                    15.99414348602295,
                    21.195341110229492,
                    -5.1999592781066895,
                    10.358951568603516,
                    -12.316866874694824,
                    21.04066276550293,
                    -5.914879322052002,
                    9.978952407836914,
                    8.642230033874512,
                    15.966367721557617,
                    -10.447552680969238,
                    -1.6757302284240723,
                    21.989442825317383,
                    21.173152923583984,
                    22.041696548461914,
                    21.911394119262695,
                    9.180853843688965,
                    -2.7829811573028564,
                    -2.8587212562561035,
                    6.282002925872803,
                    -1.781969428062439,
                    7.312793254852295,
                    2.3428268432617188,
                    8.001277923583984,
                    -14.773675918579102,
                    18.932809829711914,
                    22.125301361083984,
                    -11.824875831604004,
                    17.085384368896484,
                    0.851847231388092,
                    -11.815702438354492,
                    3.244391441345215,
                    16.989704132080078,
                    11.671980857849121,
                    21.628875732421875,
                    2.040372133255005,
                    16.10492706298828,
                    -14.984270095825195,
                    -11.854048728942871,
                    -6.4897685050964355,
                    -12.248101234436035,
                    9.768753051757812,
                    19.601234436035156,
                    -9.206646919250488,
                    -13.82865047454834,
                    18.23028564453125,
                    8.30234432220459,
                    -3.6231789588928223,
                    10.74884033203125,
                    7.079859256744385,
                    1.9703171253204346,
                    -7.499462604522705,
                    -7.27246618270874,
                    21.407285690307617,
                    -14.180407524108887,
                    22.565502166748047,
                    23.9764404296875,
                    10.92959213256836,
                    -10.86744213104248,
                    -4.184638500213623,
                    -11.481626510620117,
                    -12.529376983642578,
                    0.5464609861373901,
                    -5.248147964477539,
                    -3.3966503143310547,
                    1.125975251197815,
                    -14.986812591552734,
                    7.924600601196289,
                    -2.068311929702759,
                    -11.513053894042969,
                    19.570131301879883,
                    18.510791778564453,
                    21.609455108642578,
                    18.430519104003906,
                    6.765055179595947,
                    20.01709747314453,
                    -9.42308521270752,
                    5.369585990905762,
                    -6.2952470779418945,
                    19.429716110229492,
                    -11.779939651489258,
                    1.2744698524475098,
                    -0.24086879193782806,
                    0.5268919467926025,
                    23.33989906311035,
                    -21.022335052490234,
                    -14.611124992370605,
                    16.26538848876953,
                    17.921573638916016,
                    -3.5755646228790283,
                    -2.7431869506835938,
                    -4.4884724617004395,
                    6.760694980621338,
                    -9.979253768920898,
                    -11.702075004577637,
                    11.354188919067383,
                    22.753582000732422,
                    -0.16782920062541962,
                    -1.7152018547058105,
                    -13.606145858764648,
                    2.8134806156158447,
                    19.80970001220703,
                    0.8154899477958679,
                    8.562178611755371,
                    -11.810315132141113,
                    5.746310234069824,
                    -12.39228630065918,
                    7.7331085205078125,
                    21.289846420288086,
                    14.111175537109375,
                    8.754596710205078,
                    22.970849990844727,
                    -0.4292070269584656,
                    13.032544136047363,
                    -13.472942352294922,
                    16.79200553894043,
                    -14.086432456970215,
                    4.222893714904785,
                    19.284534454345703,
                    -3.001634359359741,
                    12.419776916503906,
                    13.25800609588623,
                    2.3527307510375977,
                    1.9019029140472412,
                    -3.678230047225952,
                    -13.246195793151855,
                    -1.0146194696426392,
                    -2.3824360370635986,
                    -8.658845901489258,
                    -7.032977104187012,
                    -24.06922149658203,
                    -8.566889762878418,
                    21.868724822998047,
                    6.726659297943115,
                    -7.193387031555176,
                    11.843376159667969,
                    5.586434841156006,
                    4.817849159240723,
                    10.534738540649414,
                    14.69034194946289,
                    -7.604460716247559,
                    22.635746002197266,
                    10.536055564880371,
                    9.929123878479004,
                    -11.155237197875977,
                    -11.703230857849121,
                    -2.4118542671203613,
                    19.944091796875,
                    6.708580493927002,
                    6.700311183929443,
                    0.04983266070485115,
                    -3.0368313789367676,
                    -6.539486408233643,
                    -13.195510864257812,
                    -12.830604553222656,
                    11.565191268920898,
                    -4.185317516326904,
                    22.1092472076416,
                    9.096640586853027,
                    -15.204891204833984,
                    -1.9167444705963135,
                    15.679112434387207,
                    -10.15832805633545,
                    3.248204231262207,
                    6.208036422729492,
                    11.249185562133789,
                    0.7131502032279968,
                    -1.4225894212722778,
                    -15.459264755249023,
                    -3.120950698852539,
                    -6.591434478759766,
                    -8.095684051513672,
                    -15.4867582321167,
                    20.309322357177734,
                    13.60807991027832,
                    -6.318173885345459,
                    -13.105290412902832,
                    13.231039047241211,
                    19.300764083862305,
                    21.947420120239258,
                    5.673885822296143,
                    20.533754348754883,
                    -13.349370002746582,
                    -13.61146068572998,
                    -10.559748649597168,
                    -3.027920961380005,
                    -7.554235935211182,
                    -3.923985719680786,
                    -3.0225400924682617,
                    5.8236002922058105,
                    17.647674560546875,
                    -9.901838302612305,
                    -11.771416664123535,
                    13.665940284729004,
                    5.222020149230957,
                    -11.236838340759277,
                    -3.7596113681793213,
                    12.709026336669922,
                    -12.025718688964844,
                    21.069427490234375,
                    -10.50370979309082,
                    -10.657649993896484,
                    21.641464233398438,
                    1.6712391376495361,
                    -0.9097478985786438,
                    -3.260537624359131,
                    -11.571418762207031,
                    -0.29467248916625977,
                    -5.447942733764648,
                    -1.8457261323928833,
                    -15.031452178955078,
                    -10.040432929992676,
                    19.426849365234375,
                    20.51652717590332,
                    20.44945526123047,
                    6.409351825714111,
                    11.093159675598145,
                    13.009805679321289,
                    19.485313415527344,
                    21.110946655273438,
                    3.691096305847168,
                    12.373003005981445,
                    -3.7094037532806396,
                    -3.199695348739624,
                    -13.273921012878418,
                    -1.0392038822174072,
                    0.3902454376220703,
                    -11.270880699157715,
                    -1.8870950937271118,
                    12.91628360748291,
                    8.852095603942871,
                    19.564783096313477,
                    -3.1600587368011475,
                    21.781858444213867,
                    6.505730628967285,
                    -10.416471481323242,
                    0.4049997627735138,
                    -14.09254264831543,
                    12.045136451721191,
                    6.934656143188477,
                    -12.979366302490234,
                    6.175320625305176,
                    1.3045463562011719,
                    -14.240691184997559,
                    5.553197860717773,
                    17.68959617614746,
                    19.706897735595703,
                    12.186582565307617,
                    -13.544921875,
                    20.26546859741211,
                    9.7880220413208,
                    -7.142251491546631,
                    19.88860321044922,
                    -9.552241325378418,
                    -9.149130821228027,
                    -9.56777286529541,
                    -12.708986282348633,
                    16.257532119750977,
                    -13.671602249145508,
                    -9.855566024780273,
                    9.367904663085938,
                    10.562403678894043,
                    -3.6885592937469482,
                    -4.514846324920654,
                    -8.211039543151855,
                    9.718523979187012,
                    -12.439446449279785,
                    -12.923904418945312,
                    18.584819793701172,
                    -9.445816040039062,
                    -1.1201952695846558,
                    -3.9806575775146484,
                    -0.070972740650177,
                    19.917434692382812,
                    0.14666394889354706,
                    6.781879901885986,
                    -12.822694778442383,
                    -12.425874710083008,
                    -4.250441551208496,
                    16.437952041625977,
                    -1.5441762208938599,
                    -4.089081287384033,
                    -10.714752197265625,
                    -8.91928768157959,
                    9.79450511932373,
                    -15.943302154541016,
                    -10.454365730285645,
                    -7.859012603759766,
                    13.752490997314453,
                    3.4235849380493164,
                    -15.000720977783203,
                    -3.412527561187744,
                    -8.098999977111816,
                    18.99851417541504,
                    -8.691821098327637,
                    7.996344566345215,
                    21.950288772583008,
                    -14.767765998840332,
                    11.408934593200684,
                    -5.213651657104492,
                    1.4260708093643188,
                    21.470365524291992,
                    14.951095581054688,
                    -12.24402141571045,
                    -7.015033721923828,
                    8.507949829101562,
                    -7.877979278564453,
                    18.845287322998047,
                    -2.154162883758545,
                    18.852340698242188,
                    -14.465312004089355,
                    -9.75643253326416,
                    8.504860877990723,
                    -8.855653762817383,
                    -4.458803176879883,
                    -0.6624255180358887,
                    11.060013771057129,
                    10.707582473754883,
                    5.741785049438477,
                    16.042606353759766,
                    -1.7061389684677124,
                    -9.495543479919434,
                    2.2007057666778564,
                    7.472551345825195,
                    20.199684143066406,
                    -14.582433700561523,
                    12.69171142578125,
                    -12.386262893676758,
                    6.111865997314453,
                    -12.566976547241211,
                    11.022858619689941,
                    -5.936446189880371,
                    -11.072701454162598,
                    -8.972908973693848,
                    -5.787076950073242,
                    4.187258243560791,
                    -15.215585708618164,
                    -8.622432708740234,
                    -7.389285087585449,
                    3.638347625732422,
                    0.9716703295707703,
                    -2.1141395568847656,
                    -2.095242977142334,
                    3.8894360065460205,
                    -15.03497314453125,
                    2.722926616668701,
                    -3.2231972217559814,
                    -3.7919762134552,
                    2.727621078491211,
                    3.477825164794922,
                    -14.04096508026123,
                    -12.20919132232666,
                    12.379068374633789,
                    -4.13271951675415,
                    -0.41529518365859985,
                    0.9902463555335999,
                    -9.533370971679688,
                    0.5361929535865784,
                    -8.820075988769531,
                    4.0498270988464355,
                    1.2355486154556274,
                    -13.780285835266113,
                    7.01854133605957,
                    17.440868377685547,
                    15.958868980407715,
                    10.423588752746582,
                    -7.535502910614014,
                    -6.5024261474609375,
                    4.3581862449646,
                    -7.04500675201416,
                    10.56771183013916,
                    17.28500747680664,
                    -0.6550417542457581,
                    -11.611498832702637,
                    4.544797897338867,
                    15.510427474975586,
                    -2.152574300765991,
                    -4.649944305419922,
                    -10.147054672241211,
                    -11.494463920593262,
                    -10.281111717224121,
                    9.21230411529541,
                    16.752363204956055,
                    8.746803283691406,
                    -12.555981636047363,
                    -0.6198359131813049,
                    15.767350196838379,
                    12.624340057373047,
                    14.623564720153809,
                    16.139617919921875,
                    3.1327364444732666,
                    1.3103952407836914,
                    14.892308235168457,
                    -2.0089643001556396,
                    4.709972381591797,
                    10.025113105773926,
                    -11.327698707580566,
                    -1.2168877124786377,
                    11.760139465332031,
                    4.004190921783447,
                    -7.407936096191406,
                    15.371138572692871,
                    5.747328758239746,
                    8.2091064453125,
                    0.4629446864128113,
                    -10.019865989685059,
                    -10.160176277160645,
                    -5.9614434242248535,
                    -9.90102481842041,
                    0.7639689445495605,
                    -2.828062057495117,
                    -2.222485303878784,
                    -8.578771591186523,
                    11.25007438659668,
                    2.2354538440704346,
                    7.104702949523926,
                    -14.279473304748535,
                    -9.337145805358887,
                    5.78794002532959,
                    -13.530756950378418,
                    -9.342924118041992,
                    23.769590377807617,
                    5.518649578094482,
                    -1.8025703430175781,
                    -0.5798506736755371,
                    -4.608101844787598,
                    22.443078994750977,
                    9.801407814025879,
                    -5.235445499420166,
                    -9.354181289672852,
                    8.613112449645996,
                    11.955693244934082,
                    -13.254037857055664,
                    -13.163729667663574,
                    -8.148402214050293,
                    -14.867571830749512,
                    -11.045458793640137,
                    5.794498920440674,
                    23.163074493408203,
                    -8.66607666015625,
                    -0.3506487011909485,
                    7.682983875274658,
                    6.784411430358887,
                    1.4338794946670532,
                    2.211148738861084,
                    -12.303529739379883,
                    15.561951637268066,
                    -2.095614194869995,
                    3.185767412185669,
                    -2.83915638923645,
                    6.806853771209717,
                    -12.234434127807617,
                    -4.979175090789795,
                    -9.7022705078125,
                    6.627697467803955,
                    5.900920391082764,
                    -11.157968521118164,
                    10.088171005249023,
                    1.5225305557250977,
                    16.53561782836914,
                    -4.492843151092529,
                    -4.753986835479736,
                    -0.2943904995918274,
                    -13.047304153442383,
                    -11.0203275680542,
                    -11.10206413269043,
                    7.863982200622559,
                    -13.210606575012207,
                    -7.823403358459473,
                    0.9611669778823853,
                    -2.1842427253723145,
                    -9.043630599975586,
                    -7.400576114654541,
                    10.852702140808105,
                    3.9336636066436768,
                    6.467677116394043,
                    -0.306613028049469,
                    0.1282801628112793,
                    -9.114396095275879,
                    13.215023040771484,
                    1.9088857173919678,
                    8.460549354553223,
                    -7.709114074707031,
                    -14.531729698181152,
                    -15.213926315307617,
                    -12.201081275939941,
                    9.92530632019043,
                    12.308694839477539,
                    0.8676149845123291,
                    9.476856231689453,
                    -7.720924377441406,
                    -15.124723434448242,
                    1.466719627380371,
                    -5.133219242095947,
                    -12.362922668457031,
                    -10.155989646911621,
                    7.302314281463623,
                    -0.35568395256996155,
                    -9.757529258728027,
                    15.374357223510742,
                    -6.654239654541016,
                    1.428045392036438,
                    -10.234034538269043,
                    -10.27267074584961,
                    -8.25692367553711,
                    -11.140127182006836,
                    -2.1122324466705322,
                    2.6727418899536133,
                    -2.056328535079956,
                    -5.350165367126465,
                    5.350173473358154,
                    5.093319892883301,
                    -4.0862507820129395,
                    14.72596263885498,
                    1.7720457315444946,
                    -12.525569915771484,
                    -12.603492736816406,
                    10.981529235839844,
                    -11.091752052307129,
                    5.029914855957031,
                    1.2831571102142334,
                    6.502098083496094,
                    2.117108106613159,
                    -0.6844358444213867,
                    6.613653659820557,
                    4.038082122802734,
                    -2.79156756401062,
                    14.855130195617676,
                    -11.717971801757812,
                    3.681328058242798,
                    -9.833528518676758,
                    -13.920151710510254,
                    8.730104446411133,
                    -0.8094528913497925,
                    -14.0264310836792,
                    5.774425029754639,
                    -5.056284427642822,
                    18.666147232055664,
                    8.42209529876709,
                    -6.029660701751709,
                    -5.42292594909668,
                    11.821359634399414,
                    -8.400716781616211,
                    -5.6645917892456055,
                    -7.706665515899658,
                    4.476292133331299,
                    14.513182640075684,
                    -10.517315864562988,
                    -2.0023090839385986,
                    -12.385029792785645,
                    3.4169833660125732,
                    12.715869903564453,
                    17.287670135498047,
                    -8.79357624053955,
                    -9.269026756286621,
                    -10.765015602111816,
                    14.279064178466797,
                    -7.914799690246582,
                    -11.568748474121094,
                    2.204512357711792,
                    -3.594696521759033,
                    -2.5926930904388428,
                    -11.144524574279785,
                    -10.020134925842285,
                    7.470849514007568,
                    11.346168518066406,
                    9.921348571777344,
                    -13.775146484375,
                    -8.700421333312988,
                    -11.746769905090332,
                    10.825735092163086,
                    6.409399032592773,
                    -1.4324039220809937,
                    12.53837776184082,
                    16.08773422241211,
                    -12.753643035888672,
                    3.410344362258911,
                    16.978107452392578,
                    -11.591595649719238,
                    0.07746360450983047,
                    8.783525466918945,
                    -3.7180349826812744,
                    16.642257690429688,
                    -14.071367263793945,
                    -5.273037433624268,
                    12.57420539855957,
                    12.666089057922363,
                    11.985808372497559,
                    6.081632137298584,
                    -14.068202018737793,
                    -15.802008628845215,
                    9.20288372039795,
                    -1.9858630895614624,
                    13.16842269897461,
                    13.044504165649414,
                    -7.124770641326904,
                    -6.003930568695068,
                    -10.858763694763184,
                    4.191589832305908,
                    4.529048919677734,
                    -2.0007872581481934,
                    13.260257720947266
                  ]
                }
              ],
              "layout": {
                "template": {
                  "data": {
                    "bar": [
                      {
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        },
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "bar"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "carpet": [
                      {
                        "aaxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "baxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "type": "carpet"
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "contour"
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmap"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2d"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2dcontour"
                      }
                    ],
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "scatter": [
                      {
                        "fillpattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        },
                        "type": "scatter"
                      }
                    ],
                    "scatter3d": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scattermap": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermap"
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "surface"
                      }
                    ],
                    "table": [
                      {
                        "cells": {
                          "fill": {
                            "color": "#EBF0F8"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "header": {
                          "fill": {
                            "color": "#C8D4E3"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "type": "table"
                      }
                    ]
                  },
                  "layout": {
                    "annotationdefaults": {
                      "arrowcolor": "#2a3f5f",
                      "arrowhead": 0,
                      "arrowwidth": 1
                    },
                    "autotypenumbers": "strict",
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "bgcolor": "white",
                      "lakecolor": "white",
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "showland": true,
                      "subunitcolor": "white"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "hovermode": "closest",
                    "mapbox": {
                      "style": "light"
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "polar": {
                      "angularaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "radialaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "scene": {
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      }
                    },
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "ternary": {
                      "aaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "baxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "caxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "title": {
                      "x": 0.05
                    },
                    "xaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    },
                    "yaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    }
                  }
                }
              }
            },
            "text/html": [
              "<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.1.1.min.js\" integrity=\"sha256-HUEFyfiTnZJxCxur99FjbKYTvKSzwDaD3/x5TqHpFu4=\" crossorigin=\"anonymous\"></script>                <div id=\"bba8e0ff-727f-49cf-9836-7f1944c64d3a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"bba8e0ff-727f-49cf-9836-7f1944c64d3a\")) {                    Plotly.newPlot(                        \"bba8e0ff-727f-49cf-9836-7f1944c64d3a\",                        [{\"mode\":\"text\",\"text\":[\"the\",\"to\",\"of\",\"in\",\"and\",\"he\",\"is\",\"for\",\"on\",\"said\",\"that\",\"has\",\"says\",\"was\",\"have\",\"it\",\"be\",\"are\",\"with\",\"will\",\"at\",\"mr\",\"from\",\"by\",\"we\",\"been\",\"as\",\"an\",\"not\",\"his\",\"but\",\"they\",\"after\",\"were\",\"had\",\"there\",\"new\",\"this\",\"australia\",\"australian\",\"who\",\"palestinian\",\"people\",\"their\",\"government\",\"two\",\"up\",\"south\",\"us\",\"which\",\"year\",\"one\",\"about\",\"out\",\"if\",\"also\",\"more\",\"when\",\"its\",\"into\",\"would\",\"first\",\"last\",\"against\",\"israeli\",\"minister\",\"arafat\",\"over\",\"all\",\"three\",\"afghanistan\",\"united\",\"world\",\"no\",\"or\",\"police\",\"than\",\"attacks\",\"fire\",\"before\",\"some\",\"security\",\"day\",\"you\",\"states\",\"could\",\"them\",\"say\",\"today\",\"now\",\"told\",\"time\",\"any\",\"laden\",\"very\",\"bin\",\"just\",\"can\",\"sydney\",\"what\",\"still\",\"company\",\"president\",\"man\",\"four\",\"taliban\",\"killed\",\"our\",\"forces\",\"al\",\"around\",\"being\",\"days\",\"west\",\"old\",\"other\",\"officials\",\"where\",\"so\",\"test\",\"qaeda\",\"israel\",\"think\",\"per\",\"general\",\"next\",\"federal\",\"force\",\"cent\",\"she\",\"leader\",\"yesterday\",\"workers\",\"take\",\"him\",\"hamas\",\"under\",\"state\",\"those\",\"years\",\"meeting\",\"bank\",\"suicide\",\"back\",\"action\",\"commission\",\"made\",\"down\",\"morning\",\"re\",\"pakistan\",\"international\",\"city\",\"attack\",\"centre\",\"group\",\"afghan\",\"members\",\"while\",\"military\",\"well\",\"number\",\"through\",\"qantas\",\"five\",\"local\",\"called\",\"area\",\"union\",\"gaza\",\"week\",\"national\",\"since\",\"wales\",\"including\",\"hours\",\"september\",\"another\",\"east\",\"night\",\"report\",\"off\",\"north\",\"should\",\"get\",\"second\",\"go\",\"earlier\",\"war\",\"staff\",\"six\",\"these\",\"between\",\"islamic\",\"months\",\"further\",\"end\",\"defence\",\"do\",\"sharon\",\"near\",\"team\",\"foreign\",\"power\",\"areas\",\"work\",\"going\",\"authority\",\"because\",\"way\",\"eight\",\"india\",\"only\",\"know\",\"month\",\"during\",\"died\",\"many\",\"match\",\"make\",\"air\",\"metres\",\"left\",\"claims\",\"spokesman\",\"ve\",\"former\",\"melbourne\",\"northern\",\"good\",\"authorities\",\"most\",\"osama\",\"support\",\"prime\",\"peace\",\"like\",\"set\",\"ago\",\"expected\",\"saying\",\"given\",\"am\",\"come\",\"looking\",\"militants\",\"bora\",\"tora\",\"put\",\"place\",\"several\",\"fighters\",\"children\",\"arrested\",\"injured\",\"found\",\"river\",\"royal\",\"groups\",\"africa\",\"unions\",\"christmas\",\"troops\",\"meanwhile\",\"indian\",\"child\",\"hospital\",\"terrorist\",\"interim\",\"part\",\"reports\",\"talks\",\"official\",\"whether\",\"then\",\"yasser\",\"statement\",\"leaders\",\"economy\",\"mountains\",\"how\",\"industrial\",\"third\",\"terrorism\",\"senior\",\"start\",\"don\",\"early\",\"radio\",\"john\",\"hit\",\"trying\",\"weather\",\"public\",\"both\",\"believe\",\"family\",\"pay\",\"million\",\"army\",\"court\",\"dr\",\"long\",\"best\",\"control\",\"help\",\"however\",\"lead\",\"adelaide\",\"asked\",\"following\",\"chief\",\"pressure\",\"agreement\",\"does\",\"service\",\"firefighters\",\"close\",\"few\",\"services\",\"labor\",\"play\",\"better\",\"community\",\"taken\",\"want\",\"arrest\",\"queensland\",\"house\",\"need\",\"overnight\",\"australians\",\"high\",\"confirmed\",\"process\",\"information\",\"came\",\"believed\",\"williams\",\"must\",\"opposition\",\"detainees\",\"won\",\"secretary\",\"did\",\"peter\",\"party\",\"held\",\"damage\",\"governor\",\"maintenance\",\"released\",\"win\",\"pentagon\",\"possible\",\"her\",\"brought\",\"hicks\",\"much\",\"shot\",\"took\",\"accused\",\"nations\",\"british\",\"weekend\",\"lot\",\"violence\",\"building\",\"despite\",\"council\",\"return\",\"got\",\"airline\",\"asylum\",\"york\",\"dead\",\"kandahar\",\"conditions\",\"across\",\"hill\",\"winds\",\"safety\",\"even\",\"such\",\"change\",\"cut\",\"eastern\",\"without\",\"director\",\"armed\",\"working\",\"aircraft\",\"call\",\"here\",\"see\",\"palestinians\",\"december\",\"economic\",\"news\",\"american\",\"too\",\"home\",\"men\",\"seekers\",\"strip\",\"lee\",\"waugh\",\"role\",\"country\",\"region\",\"trade\",\"emergency\",\"crew\",\"strong\",\"race\",\"captured\",\"david\",\"southern\",\"fighting\",\"continuing\",\"fires\",\"monday\",\"far\",\"anti\",\"board\",\"cricket\",\"training\",\"key\",\"plans\",\"bush\",\"bureau\",\"act\",\"industry\",\"george\",\"head\",\"past\",\"water\",\"charged\",\"used\",\"administration\",\"received\",\"offer\",\"alliance\",\"rate\",\"zinni\",\"health\",\"least\",\"leading\",\"person\",\"captain\",\"your\",\"town\",\"boat\",\"large\",\"decision\",\"stop\",\"known\",\"airport\",\"operations\",\"may\",\"line\",\"within\",\"risk\",\"use\",\"downer\",\"israelis\",\"soldiers\",\"major\",\"britain\",\"final\",\"parliament\",\"department\",\"zealand\",\"hundreds\",\"issue\",\"strikes\",\"hih\",\"station\",\"legal\",\"shane\",\"plane\",\"might\",\"series\",\"interest\",\"un\",\"laws\",\"policy\",\"right\",\"ahead\",\"hollingworth\",\"tomorrow\",\"network\",\"pm\",\"able\",\"due\",\"kabul\",\"latest\",\"death\",\"homes\",\"weapons\",\"behind\",\"great\",\"coast\",\"western\",\"position\",\"give\",\"later\",\"late\",\"half\",\"officers\",\"my\",\"taking\",\"every\",\"remain\",\"campaign\",\"seen\",\"thought\",\"bill\",\"timor\",\"special\",\"side\",\"failed\",\"same\",\"flight\",\"along\",\"jobs\",\"storm\",\"me\",\"forced\",\"life\",\"others\",\"continue\",\"hard\",\"event\",\"abuse\",\"cup\",\"victory\",\"jihad\",\"guilty\",\"point\",\"towards\",\"really\",\"concerned\",\"heard\",\"already\",\"territory\",\"washington\",\"deaths\",\"mcgrath\",\"helicopters\",\"envoy\",\"canyoning\",\"capital\",\"bus\",\"bichel\",\"november\",\"likely\",\"details\",\"case\",\"member\",\"launched\",\"innings\",\"according\",\"enough\",\"bombings\",\"weeks\",\"countries\",\"again\",\"detention\",\"move\",\"woomera\",\"seven\",\"cabinet\",\"bowler\",\"buildings\",\"hour\",\"mark\",\"matter\",\"middle\",\"bombing\",\"th\",\"sunday\",\"situation\",\"rates\",\"space\",\"important\",\"warne\",\"dispute\",\"caught\",\"jail\",\"claimed\",\"wants\",\"perth\",\"adventure\",\"targets\",\"run\",\"swiss\",\"asio\",\"added\",\"commonwealth\",\"raids\",\"office\",\"evidence\",\"deal\",\"guides\",\"disease\",\"show\",\"boy\",\"women\",\"own\",\"freeze\",\"opened\",\"human\",\"forward\",\"carried\",\"african\",\"mission\",\"movement\",\"based\",\"sure\",\"reported\",\"immediately\",\"political\",\"warplanes\",\"young\",\"rule\",\"ms\",\"blue\",\"top\",\"justice\",\"money\",\"aedt\",\"cancer\",\"crash\",\"march\",\"banks\",\"border\",\"using\",\"although\",\"access\",\"financial\",\"allegations\",\"certainly\",\"planning\",\"probably\",\"break\",\"find\",\"wicket\",\"ground\",\"beat\",\"prepared\",\"burning\",\"become\",\"always\",\"job\",\"proposed\",\"each\",\"full\",\"reached\",\"collapse\",\"growth\",\"order\",\"island\",\"sector\",\"flying\",\"carrying\",\"result\",\"face\",\"investigation\",\"times\",\"relations\",\"militant\",\"road\",\"sex\",\"needs\",\"organisation\",\"until\",\"serious\",\"program\",\"fight\",\"calls\",\"stage\",\"getting\",\"lives\",\"responsibility\",\"reserve\",\"thursday\",\"comes\",\"management\",\"sent\",\"drop\",\"surrender\",\"allow\",\"soon\",\"afp\",\"tried\",\"post\",\"killing\",\"radical\",\"hewitt\",\"himself\",\"senator\",\"executive\",\"outside\",\"believes\",\"inquiry\",\"short\",\"caves\",\"different\",\"flights\",\"immigration\",\"tourists\",\"future\",\"inside\",\"bid\",\"energy\",\"clear\",\"trees\",\"thousands\",\"argentina\",\"militia\",\"suspected\",\"making\",\"bowling\",\"ariel\",\"went\",\"alleged\",\"rejected\",\"howard\",\"quickly\",\"wave\",\"harrison\",\"travel\",\"opening\",\"ansett\",\"kilometres\",\"declared\",\"running\",\"measures\",\"biggest\",\"list\",\"figures\",\"rise\",\"residents\",\"sea\",\"form\",\"annual\",\"anything\",\"attempt\",\"open\",\"parties\",\"available\",\"announced\",\"shortly\",\"among\",\"currently\",\"bombers\",\"circumstances\",\"accident\",\"donald\",\"ministers\",\"look\",\"brisbane\",\"decided\",\"ruddock\",\"changes\",\"yet\",\"issues\",\"address\",\"destroyed\",\"actually\",\"rights\",\"increase\",\"terms\",\"school\",\"rural\",\"fighter\",\"quite\",\"happened\",\"wounded\",\"victoria\",\"television\",\"nine\",\"something\",\"try\",\"parts\",\"white\",\"response\",\"done\",\"wickets\",\"witnesses\",\"refused\",\"karzai\",\"sentence\",\"ended\",\"tanks\",\"gunmen\",\"sources\",\"kallis\",\"agency\",\"july\",\"jewish\",\"warned\",\"directors\",\"understand\",\"meet\",\"means\",\"returned\",\"offices\",\"yacht\",\"source\",\"alexander\",\"ll\",\"fact\",\"difficult\",\"though\",\"period\",\"confidence\",\"wage\",\"airlines\",\"virus\",\"advice\",\"caused\",\"musharraf\",\"allan\",\"recession\",\"less\",\"ensure\",\"strike\",\"appeared\",\"islands\",\"crowd\",\"suharto\",\"highway\",\"afternoon\",\"step\",\"commanders\",\"began\",\"gave\",\"worst\",\"glenn\",\"bomb\",\"commissioner\",\"powell\",\"having\",\"beginning\",\"intelligence\",\"rafter\",\"prevent\",\"gives\",\"expressed\",\"huge\",\"ever\",\"big\",\"business\",\"ses\",\"media\",\"friday\",\"pacific\",\"robert\",\"expect\",\"blake\",\"runs\",\"involved\",\"followed\",\"deputy\",\"hobart\",\"whose\",\"market\",\"tour\",\"rather\",\"attorney\",\"elected\",\"beyond\",\"arrived\",\"away\",\"facility\",\"commander\",\"total\",\"law\",\"field\",\"supporters\",\"struck\",\"car\",\"cost\",\"sir\",\"negotiations\",\"nauru\",\"tennis\",\"massive\",\"entered\",\"threat\",\"plan\",\"explosives\",\"debt\",\"entitlements\",\"criticism\",\"decide\",\"quarter\",\"saturday\",\"assistance\",\"labour\",\"geoff\",\"together\",\"finished\",\"chance\",\"endeavour\",\"chairman\",\"main\",\"heavy\",\"base\",\"places\",\"tragedy\",\"sort\",\"vote\",\"giving\",\"jenin\",\"front\",\"powers\",\"anglican\",\"son\",\"zimbabwe\",\"themselves\",\"conflict\",\"yes\",\"muslim\",\"lockett\",\"daryl\",\"helicopter\",\"current\",\"fast\",\"complex\",\"terror\",\"smoke\",\"france\",\"anthony\",\"calling\",\"hearings\",\"population\",\"tasmania\",\"game\",\"jacques\",\"placed\",\"denied\",\"reid\",\"pakistani\",\"indonesia\",\"bring\",\"ballot\",\"played\",\"protect\",\"level\",\"conference\",\"organisations\",\"martin\",\"employees\",\"feel\",\"costs\",\"changed\",\"study\",\"survey\",\"brett\",\"potential\",\"macgill\",\"cannot\",\"crean\",\"lost\",\"storms\",\"round\",\"russian\",\"trip\",\"crisis\",\"nearly\",\"americans\",\"speaking\",\"ambush\",\"never\",\"significant\",\"boxing\",\"longer\",\"low\",\"tribal\",\"deadly\",\"record\",\"problem\",\"professor\",\"hayden\",\"fleeing\",\"absolutely\",\"continues\",\"fired\",\"rumsfeld\",\"claim\",\"ramallah\",\"hold\",\"anyone\",\"election\",\"construction\",\"technology\",\"doubles\",\"cities\",\"companies\",\"research\",\"whole\",\"efforts\",\"needed\",\"small\",\"moved\",\"confident\",\"land\",\"proposals\",\"sign\",\"little\",\"affected\",\"tape\",\"ruled\",\"environment\",\"everything\",\"severe\",\"led\",\"closed\",\"forecast\",\"pilot\",\"overall\",\"gillespie\",\"signed\",\"coming\",\"receive\",\"rival\",\"provide\",\"representation\",\"simon\",\"accept\",\"sides\",\"mountain\",\"receiving\",\"mean\",\"secret\",\"injuries\",\"dozens\",\"steve\",\"payment\",\"hope\",\"battle\",\"shuttle\",\"gun\",\"central\",\"bomber\",\"starting\",\"activity\",\"damaged\",\"bonn\",\"disaster\",\"problems\",\"verdict\",\"flames\",\"condition\",\"french\",\"tony\",\"resolution\",\"rest\",\"coalition\",\"richard\",\"treatment\",\"recorded\",\"grant\",\"stopped\",\"hotel\",\"insurance\",\"carry\",\"rain\",\"almost\",\"ice\",\"continued\",\"greater\",\"global\",\"share\",\"direct\",\"nation\",\"paid\",\"vaughan\",\"statistics\",\"fellow\",\"winner\",\"civil\",\"review\",\"private\",\"gas\",\"twice\",\"interlaken\",\"concern\",\"cars\",\"started\",\"red\",\"fell\",\"disappointed\",\"debate\",\"determined\",\"michael\",\"seles\",\"begin\",\"krishna\",\"didn\",\"refugees\",\"remaining\",\"tough\",\"ceremony\",\"property\",\"january\",\"qc\",\"stand\",\"operation\",\"territories\",\"above\",\"lower\",\"respond\",\"reduce\",\"resolve\",\"victims\",\"strategic\",\"asic\",\"alongside\",\"include\",\"revealed\",\"august\",\"season\",\"charge\",\"completed\",\"seeking\",\"bit\",\"park\",\"lines\",\"heritage\",\"traditional\",\"enter\",\"tuesday\",\"guard\",\"ray\",\"avoid\",\"markets\",\"visit\",\"europe\",\"winning\",\"playing\",\"self\",\"yachts\",\"met\",\"charges\",\"vice\",\"cease\",\"roads\",\"factory\",\"america\",\"itself\",\"created\",\"wake\",\"levels\",\"fall\",\"related\",\"outlook\",\"ministry\",\"lung\",\"hearing\",\"non\",\"volunteers\",\"civilians\",\"voted\",\"liquidation\",\"search\",\"provisional\",\"rescue\",\"victorian\",\"table\",\"successful\",\"track\",\"conducted\",\"heading\",\"spread\",\"accompanied\",\"delhi\",\"operating\",\"wanted\",\"expects\",\"leg\",\"ponting\",\"pulled\",\"knew\",\"heart\",\"coach\",\"confirm\",\"ball\",\"virgin\",\"press\",\"suffered\",\"illawarra\",\"approach\",\"manslaughter\",\"costello\",\"showed\",\"threatened\",\"warning\",\"helped\",\"resume\",\"japan\",\"individuals\",\"mayor\",\"giuliani\",\"friedli\",\"wind\",\"served\",\"andy\",\"range\",\"responsible\",\"unemployment\",\"mckenzie\",\"initial\",\"keep\",\"families\",\"lord\",\"incident\",\"october\",\"finance\",\"treated\",\"ian\",\"why\",\"solution\",\"apparently\",\"body\",\"club\",\"crackdown\",\"reach\",\"officer\",\"institute\",\"shaun\",\"pollock\",\"hopes\",\"structure\",\"data\",\"nice\",\"food\",\"seriously\",\"suspended\",\"attacked\",\"jason\",\"elections\",\"edge\",\"affairs\",\"nothing\",\"questions\",\"mid\",\"built\",\"negotiating\",\"peacekeepers\",\"saw\",\"issued\",\"spokeswoman\",\"assisting\",\"remains\",\"finding\",\"recovery\",\"woman\",\"gang\",\"kashmir\",\"farmers\",\"oil\",\"networks\",\"sheikh\",\"adequate\",\"doubt\",\"products\",\"secure\",\"beatle\",\"single\",\"options\",\"clearly\",\"blaze\",\"present\",\"ford\",\"cfmeu\",\"tailenders\",\"fatah\",\"scene\",\"co\",\"lording\",\"factions\",\"st\",\"raid\",\"career\",\"streets\",\"butterfly\",\"amin\",\"outcome\",\"traveland\",\"peres\",\"inappropriate\",\"austar\",\"scored\",\"champion\",\"races\",\"cave\",\"scheduled\",\"clean\",\"nearby\",\"philip\",\"shows\",\"invasion\",\"aboard\",\"coup\",\"senate\",\"doug\",\"solomon\",\"eve\",\"sarah\",\"holiday\",\"mohammad\",\"university\",\"murder\",\"whiting\",\"gorge\",\"tensions\",\"manufacturing\",\"wayne\",\"yallourn\",\"diplomatic\",\"drug\",\"promised\",\"cause\",\"natural\",\"afroz\",\"ethnic\",\"singles\",\"crews\",\"meetings\",\"toll\",\"apra\",\"administrators\",\"corporation\",\"leadership\",\"canberra\",\"exchange\",\"nuclear\",\"germany\",\"numbers\",\"attacking\",\"largest\",\"petrol\",\"customers\",\"prior\",\"internet\",\"awards\",\"extremists\",\"attempting\",\"personnel\",\"hand\",\"criminal\",\"mandate\",\"things\",\"deployed\",\"follows\",\"unrest\",\"dropped\",\"manager\",\"injury\",\"settlement\",\"roof\",\"honours\",\"appears\",\"metre\",\"boats\",\"often\",\"speech\",\"squad\",\"fair\",\"budget\",\"ready\",\"ask\",\"band\",\"proteas\",\"king\",\"grand\",\"recent\",\"happens\",\"classic\",\"suburbs\",\"resign\",\"swept\",\"collapsed\",\"true\",\"agreed\",\"batsmen\",\"presence\",\"felt\",\"billion\",\"resistance\",\"giant\",\"increased\",\"described\",\"unit\",\"create\",\"concerns\",\"protection\",\"targeted\",\"boys\",\"saudi\",\"leave\",\"unity\",\"planes\",\"halt\",\"read\",\"marine\",\"neil\",\"walk\",\"crossed\",\"fleet\",\"knowledge\",\"minute\",\"greatest\",\"extensive\",\"backed\",\"ocean\",\"assa\",\"ricky\",\"abloy\",\"light\",\"premier\",\"names\",\"explanation\",\"wall\",\"possibility\",\"real\",\"live\",\"switzerland\",\"japanese\",\"shopping\",\"reveal\",\"fierce\",\"tree\",\"elders\",\"blame\",\"tension\",\"employment\",\"detain\",\"positive\",\"income\",\"haifa\",\"jerusalem\",\"pre\",\"programs\",\"jets\",\"transport\",\"regional\",\"save\",\"hunt\",\"advance\",\"gone\",\"battling\",\"suspect\",\"representing\",\"investigating\",\"reduced\",\"acting\",\"projects\",\"investment\",\"spencer\",\"findings\",\"students\",\"nablus\",\"actions\",\"trial\",\"declaration\",\"handed\",\"custody\",\"growing\",\"system\",\"prisoners\",\"domestic\",\"education\",\"society\",\"summit\",\"assault\",\"langer\",\"matthew\",\"requested\",\"westpac\",\"doctor\",\"wing\",\"republic\",\"searching\",\"eliminated\",\"approval\",\"anz\",\"term\",\"bargaining\",\"various\",\"balls\",\"klusener\",\"boucher\",\"humanity\",\"suggested\",\"adding\",\"history\",\"normal\",\"cuts\",\"signs\",\"gunships\",\"blasted\",\"turn\",\"hare\",\"smaller\",\"guess\",\"benares\",\"ashes\",\"path\",\"terrorists\",\"blazes\",\"hijacked\",\"adam\",\"follow\",\"comment\",\"aware\",\"connection\",\"underway\",\"kieren\",\"rabbani\",\"completely\",\"tonight\",\"understanding\",\"infected\",\"masood\",\"treasurer\",\"crime\",\"gambier\",\"henderson\",\"returning\",\"results\",\"kingham\",\"question\",\"kissinger\",\"gerber\",\"stuart\",\"launceston\",\"sergeant\",\"flood\",\"committee\",\"hundred\",\"goshen\",\"handling\",\"church\",\"thing\",\"escaped\",\"injuring\",\"slightly\",\"francs\",\"hunter\",\"ahmed\",\"actor\",\"wednesday\",\"aged\",\"centrelink\",\"threatening\",\"sultan\",\"improve\",\"passed\",\"stability\",\"project\",\"dollars\",\"decades\",\"course\",\"ill\",\"faces\",\"chosen\",\"bob\",\"hamid\",\"passengers\",\"davis\",\"neville\",\"ways\",\"pace\",\"whatever\",\"headed\",\"launch\",\"replied\",\"hopefully\",\"determine\",\"archbishop\",\"unable\",\"throughout\",\"average\",\"unidentified\",\"survived\",\"approached\",\"convicted\",\"cooperation\",\"redundancy\",\"waiting\",\"request\",\"paying\",\"observers\",\"aboriginal\",\"procedures\",\"reject\",\"document\",\"improved\",\"holding\",\"mass\",\"unfortunately\",\"welcomed\",\"whereabouts\",\"appropriate\",\"lack\",\"delay\",\"trapped\",\"facilities\",\"decisions\",\"prepare\",\"medical\",\"necessary\",\"spinner\",\"examination\",\"losing\",\"channel\",\"occupation\",\"title\",\"consumers\",\"firm\",\"creditors\",\"fine\",\"vehicle\",\"staying\",\"relationship\",\"delivered\",\"begun\",\"hot\",\"coroner\",\"temperatures\",\"containment\",\"cross\",\"contested\",\"strongly\",\"experts\",\"celebrations\",\"focus\",\"named\",\"sometimes\",\"marines\",\"player\",\"jalalabad\",\"games\",\"breaking\",\"contained\",\"counts\",\"stay\",\"allowed\",\"temporary\",\"assembly\",\"draft\",\"understood\",\"toowoomba\",\"voice\",\"twenty\",\"strachan\",\"harris\",\"discussions\",\"hopman\",\"crashed\",\"farm\",\"violent\",\"communities\",\"kilometre\",\"doctors\",\"hoping\",\"ban\",\"colin\",\"effective\",\"success\",\"offered\",\"positions\",\"abu\",\"worked\",\"documents\",\"tell\",\"phillips\",\"retired\",\"choosing\",\"responding\",\"allegedly\",\"indonesian\",\"detail\",\"free\",\"bringing\",\"hiv\",\"proposal\",\"doesn\",\"mining\",\"embassy\",\"heights\",\"mt\",\"trading\",\"room\",\"fund\",\"impact\",\"male\",\"mohammed\",\"interests\",\"effort\",\"antarctic\",\"previous\",\"target\",\"words\",\"publicly\",\"walked\",\"credit\",\"provided\",\"investigate\",\"telephone\",\"eventually\",\"leaving\",\"banking\",\"interview\",\"headquarters\",\"clashes\",\"doing\",\"fear\",\"predicted\",\"picked\",\"happy\",\"visa\",\"tie\",\"putting\",\"escalating\",\"hoped\",\"landed\",\"sharing\",\"mind\",\"skipper\",\"gary\",\"soft\",\"became\",\"sending\",\"shoes\",\"paris\",\"required\",\"seemed\",\"cameron\",\"ability\",\"locked\",\"travelled\",\"finally\",\"separate\",\"owen\"],\"x\":[66.3838882446289,70.03727722167969,69.80760192871094,69.58170318603516,70.17229461669922,59.32894515991211,67.90863037109375,68.62193298339844,70.41272735595703,66.06658172607422,64.53382110595703,70.55182647705078,69.12042236328125,62.315887451171875,69.25253295898438,63.64190673828125,62.95341873168945,68.9612045288086,70.59469604492188,67.53556060791016,70.44652557373047,62.15291976928711,70.43708038330078,70.86611938476562,67.21956634521484,60.01408767700195,69.8149185180664,69.13595581054688,64.30243682861328,66.39900970458984,66.85686492919922,66.91075897216797,70.90409851074219,68.50463104248047,66.11347198486328,59.614356994628906,67.72840881347656,65.19670867919922,60.0964469909668,66.10891723632812,67.81172943115234,63.38094711303711,60.10725784301758,66.7314224243164,58.13697052001953,69.31446838378906,66.17745208740234,64.22927856445312,62.257164001464844,63.980464935302734,64.89633178710938,60.08439254760742,63.56083297729492,60.83586120605469,63.003475189208984,62.15724182128906,62.65536117553711,66.37185668945312,63.28640365600586,67.29171752929688,63.66503143310547,62.39086151123047,61.182151794433594,61.4775390625,60.093868255615234,67.90023040771484,58.82817840576172,62.39879608154297,57.200950622558594,60.76457595825195,55.72461700439453,54.35950469970703,60.38949966430664,57.73248291015625,59.83946990966797,58.65781021118164,59.50355529785156,54.15410232543945,58.81045913696289,61.133907318115234,57.30542755126953,56.60201644897461,55.558658599853516,59.82984161376953,55.419227600097656,58.43398666381836,55.31121826171875,55.26466369628906,60.903953552246094,56.804283142089844,59.37510299682617,54.440608978271484,56.14787673950195,54.661624908447266,55.03977966308594,56.84988784790039,54.820030212402344,56.31010437011719,55.765106201171875,54.554107666015625,55.43418884277344,56.72661590576172,55.956687927246094,53.285396575927734,56.841407775878906,55.161075592041016,53.02909469604492,54.721866607666016,54.134498596191406,54.88554763793945,55.339927673339844,55.19314193725586,54.11680603027344,53.56168746948242,53.360511779785156,54.034854888916016,54.017269134521484,54.655364990234375,54.09534454345703,54.29253005981445,53.67941665649414,54.16633224487305,54.5442008972168,53.127403259277344,57.2662353515625,53.73740005493164,52.40613555908203,54.06179428100586,53.729801177978516,53.800357818603516,52.503387451171875,54.10138702392578,55.37040710449219,52.68584060668945,53.719356536865234,54.23197937011719,54.02332305908203,54.10688018798828,53.67628479003906,53.37466812133789,53.98497772216797,52.87056350708008,54.08102798461914,52.55867004394531,53.70336151123047,54.316097259521484,53.50819778442383,53.758487701416016,53.31901931762695,53.71604537963867,53.86891174316406,54.650535583496094,53.27031326293945,53.041839599609375,53.96886444091797,54.414337158203125,54.115055084228516,53.80965042114258,54.065277099609375,54.372833251953125,50.06894302368164,51.054786682128906,54.288639068603516,53.83853530883789,54.4201545715332,54.5213737487793,51.709869384765625,52.91292953491211,54.55896759033203,53.05800247192383,52.86412811279297,53.64644241333008,53.85977554321289,40.47349548339844,53.873573303222656,53.63609313964844,36.0756950378418,52.88836669921875,53.49000549316406,53.2493782043457,53.262725830078125,53.58715057373047,49.68132400512695,53.08605194091797,53.09371566772461,53.8241081237793,50.05182647705078,53.91777038574219,37.591026306152344,49.0499153137207,53.391963958740234,48.0398063659668,53.992149353027344,53.69087600708008,52.60426330566406,51.56808090209961,41.640464782714844,51.74321746826172,50.17192840576172,51.83246994018555,53.96410369873047,51.724037170410156,53.78411865234375,53.83463668823242,50.8928337097168,44.05918502807617,38.417869567871094,44.571929931640625,53.825286865234375,36.513580322265625,52.49834442138672,46.794559478759766,47.91973114013672,48.08796691894531,53.49114227294922,53.2877082824707,53.157196044921875,47.70852279663086,47.407535552978516,50.273353576660156,51.19487762451172,53.341556549072266,51.643043518066406,51.053802490234375,46.569175720214844,37.53000259399414,52.813194274902344,52.00128936767578,50.55269241333008,42.78109359741211,29.90513801574707,49.378116607666016,49.949005126953125,36.50753402709961,44.88375473022461,46.44334030151367,42.68423080444336,51.5224494934082,48.86923599243164,40.26498794555664,41.85823059082031,36.430519104003906,49.35749053955078,42.560604095458984,36.716556549072266,43.69744110107422,52.1346435546875,48.82204055786133,44.11542892456055,38.52657699584961,52.51730728149414,45.97466278076172,49.795623779296875,28.130367279052734,28.4909725189209,48.658897399902344,42.02653884887695,46.25815200805664,47.30986785888672,27.424694061279297,47.30442428588867,33.9825325012207,37.03540802001953,44.77467727661133,37.69233322143555,52.77803421020508,37.02115249633789,26.992536544799805,37.175662994384766,34.88822555541992,27.795921325683594,51.38943862915039,45.855403900146484,44.14128875732422,51.30164337158203,27.535877227783203,41.75183868408203,30.078048706054688,29.55145263671875,37.35546112060547,45.10432434082031,48.227962493896484,50.46978759765625,41.34557342529297,46.987972259521484,27.034664154052734,41.62954330444336,31.56039810180664,43.875858306884766,38.95124435424805,45.385948181152344,32.70208740234375,46.06510925292969,45.482994079589844,49.97248840332031,29.53412437438965,28.692340850830078,41.07302474975586,28.44556999206543,22.719282150268555,49.137088775634766,43.1151008605957,50.829917907714844,25.982969284057617,35.18517303466797,30.655784606933594,34.39967727661133,28.45579719543457,27.971054077148438,36.28323745727539,49.804813385009766,50.689937591552734,32.230995178222656,45.08646774291992,29.103534698486328,39.71873092651367,32.343048095703125,32.12438201904297,27.312349319458008,48.08551788330078,32.2723388671875,33.789363861083984,42.23685073852539,26.239280700683594,28.930429458618164,29.582103729248047,29.13603973388672,47.94231414794922,28.89218521118164,27.138090133666992,38.826927185058594,28.25901222229004,49.40243148803711,28.493783950805664,28.16080093383789,32.80537033081055,28.508392333984375,19.424339294433594,29.571794509887695,34.21099853515625,28.905757904052734,33.12464904785156,37.721675872802734,50.3474235534668,28.69346809387207,43.64868927001953,28.394800186157227,29.970102310180664,28.54316520690918,30.733495712280273,40.748783111572266,28.1602783203125,29.289623260498047,24.16069984436035,29.946365356445312,31.425460815429688,23.3623046875,28.565235137939453,27.703975677490234,27.232297897338867,28.487375259399414,25.805986404418945,26.704317092895508,30.26924705505371,24.538042068481445,23.734569549560547,28.14321517944336,33.3218994140625,36.96942138671875,32.398380279541016,27.37734031677246,25.279827117919922,29.00090217590332,36.221229553222656,19.30573844909668,31.406984329223633,26.24303436279297,27.8632755279541,27.556854248046875,29.22592544555664,28.083593368530273,28.315881729125977,27.70564079284668,27.27078628540039,28.04432487487793,28.286840438842773,26.912601470947266,41.85068893432617,37.03776931762695,28.111236572265625,27.683799743652344,35.26738739013672,25.249191284179688,27.803810119628906,27.8295841217041,21.506635665893555,27.122053146362305,28.516603469848633,30.348329544067383,25.966522216796875,31.631914138793945,27.560211181640625,27.72195816040039,31.960433959960938,22.032522201538086,31.485153198242188,51.308349609375,28.013986587524414,23.513071060180664,24.866796493530273,13.46588134765625,28.202234268188477,27.891908645629883,29.771093368530273,41.76749801635742,27.32257843017578,27.038747787475586,5.165525436401367,26.4559326171875,24.270267486572266,27.693864822387695,36.14649200439453,6.40425968170166,27.243051528930664,2.1814167499542236,28.409141540527344,27.09673309326172,16.923433303833008,27.147850036621094,21.7128849029541,28.469451904296875,27.9125919342041,22.30403709411621,28.454240798950195,23.106712341308594,28.009443283081055,7.598287105560303,27.305587768554688,27.75697135925293,27.839189529418945,27.54913902282715,26.914962768554688,27.428525924682617,28.25181007385254,27.517976760864258,30.542692184448242,2.1803441047668457,27.897581100463867,22.080963134765625,41.11630630493164,41.16336441040039,12.49476432800293,25.744155883789062,26.4406795501709,27.389041900634766,27.744342803955078,18.759206771850586,2.3007452487945557,23.09798812866211,26.452186584472656,27.469505310058594,26.979598999023438,27.553050994873047,28.225378036499023,25.989089965820312,28.90704345703125,18.28961181640625,27.608003616333008,27.03302001953125,25.48565101623535,26.691898345947266,12.573896408081055,22.512466430664062,0.8397967219352722,11.868453025817871,27.983911514282227,27.475404739379883,33.758750915527344,29.380020141601562,27.91515350341797,7.097864627838135,27.690401077270508,3.7515642642974854,27.6750545501709,25.608840942382812,5.808915138244629,27.439363479614258,24.614315032958984,27.01204490661621,24.193885803222656,28.899864196777344,26.27663230895996,26.855979919433594,13.999480247497559,27.522512435913086,24.16776466369629,26.913480758666992,4.08782958984375,15.51730728149414,1.1541047096252441,28.02761459350586,25.971439361572266,25.529569625854492,27.344867706298828,24.5886173248291,28.946125030517578,25.901979446411133,23.354774475097656,15.970992088317871,22.075679779052734,11.912951469421387,24.472997665405273,28.654918670654297,27.824657440185547,18.8824405670166,4.894108772277832,1.5086770057678223,26.476163864135742,7.328629493713379,25.692373275756836,11.628061294555664,26.214431762695312,3.3513801097869873,28.660533905029297,28.476247787475586,14.726469993591309,25.534502029418945,1.2554723024368286,18.26544761657715,25.045711517333984,23.83209991455078,2.0431032180786133,26.608129501342773,26.91000747680664,27.992433547973633,28.792884826660156,17.272539138793945,16.319602966308594,20.974990844726562,27.645204544067383,27.86296272277832,28.4322566986084,0.8844791054725647,11.569622039794922,26.622886657714844,5.456864833831787,4.4964399337768555,24.14083480834961,16.280582427978516,28.515932083129883,22.48073387145996,25.410104751586914,16.24913787841797,6.100963592529297,28.01543426513672,1.5293060541152954,2.237093448638916,6.232179164886475,2.5287437438964844,27.473634719848633,14.832275390625,23.640398025512695,11.998573303222656,2.999349594116211,2.9549834728240967,22.090473175048828,18.259281158447266,25.833908081054688,9.03244400024414,26.463098526000977,1.7597659826278687,21.416236877441406,12.743522644042969,37.328514099121094,21.284135818481445,23.576753616333008,28.27518081665039,2.5570313930511475,26.818845748901367,24.210681915283203,28.041954040527344,20.791751861572266,24.484399795532227,32.80032730102539,26.3421573638916,0.09230969101190567,28.08559799194336,25.81791877746582,1.6279246807098389,4.607223987579346,28.515363693237305,7.194377899169922,2.851445436477661,2.600938081741333,14.93978500366211,14.269180297851562,4.863690376281738,26.457374572753906,6.581759452819824,13.60167121887207,1.6223018169403076,3.824173927307129,2.8084356784820557,14.751022338867188,2.614793300628662,4.2886810302734375,14.321739196777344,2.0924649238586426,7.112168312072754,1.6337255239486694,27.16103744506836,1.8644171953201294,20.2393856048584,17.702089309692383,17.15961265563965,27.38419532775879,10.352058410644531,15.574044227600098,19.40694236755371,4.251465797424316,2.3676388263702393,15.1710786819458,27.688793182373047,19.895923614501953,1.8956060409545898,22.77171516418457,25.743404388427734,26.99852752685547,2.7900521755218506,23.383150100708008,14.355363845825195,15.371696472167969,27.548171997070312,3.981768846511841,15.672492027282715,3.93953275680542,5.641819953918457,0.8990104794502258,-0.09478674083948135,3.438084602355957,16.134737014770508,17.66996192932129,1.2829066514968872,5.714400768280029,10.948832511901855,14.798127174377441,-22.019123077392578,25.674428939819336,2.831275701522827,3.8538360595703125,-26.2596435546875,13.488704681396484,2.2827463150024414,9.65277099609375,26.63282585144043,18.127553939819336,22.458404541015625,14.668371200561523,17.094236373901367,20.168529510498047,1.9926667213439941,-9.958386421203613,4.522586345672607,2.254894495010376,5.95841646194458,20.59882164001465,9.842700004577637,25.07673454284668,3.673198699951172,8.466893196105957,3.1400318145751953,13.41666316986084,4.848123550415039,28.3035945892334,13.79228687286377,5.7416253089904785,3.5767228603363037,3.538796901702881,3.111776113510132,4.934834957122803,3.1435060501098633,-48.395301818847656,10.921762466430664,5.539677143096924,3.6126296520233154,6.315653324127197,-5.4607038497924805,3.549403667449951,16.818178176879883,2.6422510147094727,0.39706671237945557,2.6191697120666504,12.997390747070312,-18.988994598388672,-0.07471291720867157,-3.4602224826812744,6.497378826141357,5.610983848571777,1.9137448072433472,4.636561393737793,16.041826248168945,21.743528366088867,27.536575317382812,13.9342679977417,0.11018408834934235,-14.232396125793457,26.652263641357422,-34.05174255371094,5.946363925933838,4.025294303894043,11.091974258422852,2.8148176670074463,-18.614063262939453,0.22319979965686798,1.0443254709243774,27.687788009643555,-10.905630111694336,5.751302719116211,-11.886563301086426,2.7913568019866943,1.9477788209915161,1.829982042312622,2.8389782905578613,2.4332804679870605,-11.808037757873535,-60.07505798339844,2.882476806640625,1.7558921575546265,1.8366779088974,1.834519624710083,7.671560764312744,16.485618591308594,6.357071399688721,3.6160244941711426,6.082180500030518,1.194061517715454,2.521075963973999,2.766570568084717,3.224632978439331,4.364656448364258,-20.37794303894043,1.2343870401382446,2.9400410652160645,-2.3610451221466064,3.250816822052002,-17.589054107666016,4.831930637359619,2.593717336654663,3.3238797187805176,25.134132385253906,3.1181726455688477,2.4069879055023193,0.7780314683914185,2.6356284618377686,22.76288604736328,2.2611138820648193,9.093058586120605,1.5544226169586182,1.5777761936187744,14.20523452758789,2.467942714691162,-42.299415588378906,13.038534164428711,2.442448854446411,3.6444506645202637,-6.982672214508057,-17.241376876831055,2.399423599243164,18.893871307373047,-3.528175115585327,1.3329962491989136,18.14756202697754,0.9159736633300781,15.308525085449219,23.17407989501953,3.2088451385498047,8.396228790283203,2.3511064052581787,-4.477787017822266,2.3173553943634033,6.50845193862915,1.1969385147094727,-48.99125289916992,-2.6219642162323,5.134729862213135,-29.83340072631836,2.431367874145508,3.7803544998168945,4.501986980438232,1.63693368434906,1.437120795249939,1.9982972145080566,1.3213491439819336,4.87343692779541,1.0840729475021362,4.832571983337402,-15.761204719543457,12.859333992004395,25.258529663085938,8.02536678314209,1.5325126647949219,5.047880172729492,20.963428497314453,3.7389769554138184,-7.994770050048828,2.7070529460906982,3.570358991622925,-21.63253402709961,-5.488454341888428,0.8224937915802002,3.697751522064209,2.540470600128174,-20.30014419555664,-16.84880256652832,-4.175836563110352,13.18851089477539,-0.5631857514381409,-18.377901077270508,23.092647552490234,3.1351914405822754,0.8130359053611755,4.264707088470459,3.043280601501465,-25.32920265197754,2.771852970123291,-18.359804153442383,5.287017822265625,3.8873836994171143,-12.417696952819824,-22.245325088500977,3.376875400543213,3.464177370071411,10.409667015075684,1.5362823009490967,1.3611325025558472,1.0643528699874878,-2.477494239807129,2.341675043106079,-12.356472969055176,2.378873586654663,4.857940196990967,1.6040449142456055,11.21350383758545,-46.606048583984375,3.8406639099121094,-19.943696975708008,5.567195892333984,2.031053066253662,3.7208189964294434,5.100648403167725,-14.414823532104492,4.1344380378723145,-6.836599349975586,1.1928707361221313,-22.28136444091797,1.4056849479675293,2.846891164779663,2.6196415424346924,5.166737079620361,6.643126010894775,1.148411512374878,4.516327857971191,-14.3594388961792,5.336153984069824,2.2935032844543457,3.202310085296631,4.435025691986084,4.021119594573975,-7.223355770111084,3.9001832008361816,0.6658019423484802,3.4002110958099365,2.9167354106903076,2.757338285446167,-15.542720794677734,3.5402467250823975,2.2656409740448,0.8967165350914001,-12.18293285369873,4.942546367645264,-27.9440860748291,3.373436212539673,1.549004077911377,-20.332815170288086,3.242124557495117,1.9936981201171875,0.6630045175552368,-1.664788007736206,3.9958033561706543,1.2124913930892944,-28.44381332397461,1.8240278959274292,-17.44756507873535,-10.051637649536133,-23.48295783996582,2.0383782386779785,-19.38912582397461,4.025132179260254,1.942064642906189,4.913013458251953,-18.379262924194336,-6.302505970001221,5.523695468902588,4.018278121948242,-34.81912612915039,-26.088909149169922,4.449319362640381,3.675814151763916,2.533050775527954,-27.28433609008789,-0.248331218957901,-0.8774054050445557,-20.510284423828125,-21.685083389282227,-5.700313091278076,3.6414055824279785,-7.773971080780029,4.2360920906066895,3.799140214920044,12.086825370788574,-8.120000839233398,1.8660506010055542,-14.366547584533691,3.606553077697754,3.431110143661499,2.015012502670288,2.5313122272491455,4.473135471343994,4.07548189163208,-0.5527340173721313,6.191308975219727,1.1621131896972656,-30.99024772644043,-4.519214153289795,-28.08330726623535,4.074141025543213,0.6053165197372437,3.7250614166259766,1.0981745719909668,-2.457209825515747,2.450261116027832,3.0515642166137695,0.8563485145568848,-0.9771766662597656,-30.481975555419922,3.2942306995391846,4.334439754486084,-14.438127517700195,2.425342321395874,-9.540046691894531,4.373692512512207,-18.32874870300293,0.3210470676422119,2.9571516513824463,10.46633529663086,2.5930979251861572,-27.380290985107422,5.199720859527588,-18.553890228271484,7.5512471199035645,-4.564673900604248,15.156754493713379,2.1769862174987793,5.087087154388428,-16.09311866760254,4.492702960968018,-23.846223831176758,2.800111770629883,-2.701289415359497,-15.649179458618164,-3.317441701889038,5.668086051940918,7.644052505493164,-34.203670501708984,6.239250659942627,2.6600539684295654,-13.70698356628418,1.9300743341445923,-10.072098731994629,3.840024948120117,-27.063217163085938,-19.868886947631836,-7.076589107513428,-21.347675323486328,2.7123124599456787,-0.01598326489329338,3.044312000274658,-18.811569213867188,-26.701990127563477,-26.344995498657227,-21.137710571289062,-20.03350830078125,-17.660688400268555,-25.763391494750977,-23.91257095336914,-14.138050079345703,-43.253787994384766,1.1253706216812134,-20.571752548217773,-5.5707926750183105,-47.07274627685547,-14.075246810913086,3.1600935459136963,-39.5892448425293,-11.735111236572266,-41.542476654052734,-33.081146240234375,-22.169227600097656,-43.85806655883789,-43.305213928222656,-15.854799270629883,-45.547569274902344,-4.298437595367432,-1.568683385848999,2.7418479919433594,-11.507096290588379,-50.04982376098633,2.3007915019989014,5.939353942871094,-18.981597900390625,-16.463409423828125,-18.580720901489258,-3.7836220264434814,0.7240985035896301,17.69293212890625,-9.487251281738281,-50.6611328125,-15.453362464904785,-2.3199105262756348,-25.51790428161621,-3.313126802444458,1.4291775226593018,2.500075340270996,-25.711105346679688,1.882426381111145,-16.16091537475586,-23.373123168945312,-3.5132968425750732,-24.732072830200195,-27.700281143188477,4.726027965545654,-37.39472579956055,-41.72591018676758,-4.081757545471191,3.7436976432800293,-56.92491149902344,-12.976746559143066,3.614142417907715,-0.09466857463121414,4.458282947540283,-16.475284576416016,-33.28458786010742,4.659688949584961,-21.457752227783203,-39.22913360595703,4.724618911743164,-20.88513946533203,-13.096759796142578,5.140501499176025,-19.489940643310547,-47.14329147338867,-14.799365997314453,-18.765140533447266,-20.087934494018555,-14.420880317687988,-6.6205596923828125,-17.61183738708496,-16.22751808166504,3.504249334335327,-15.9464750289917,-42.503936767578125,-22.88743782043457,6.342711448669434,4.005825042724609,-9.006139755249023,1.1691282987594604,-17.22394371032715,-7.435420513153076,1.315794825553894,-37.77734375,-22.76690101623535,-22.979736328125,0.8209572434425354,2.682723045349121,-1.2028775215148926,-14.484460830688477,-10.851127624511719,-9.339320182800293,-17.509929656982422,4.438039302825928,3.35709285736084,-23.07425880432129,-46.24554443359375,0.959308385848999,-19.38573455810547,-35.5521240234375,3.6439669132232666,-51.30702209472656,-9.378650665283203,-28.46576690673828,-13.113876342773438,5.800864219665527,-7.219264984130859,-33.74116897583008,-16.171091079711914,-24.285675048828125,-30.125667572021484,-5.310688018798828,-44.047218322753906,3.5112650394439697,-35.81993103027344,-32.983001708984375,-31.34107780456543,1.2104607820510864,2.928075075149536,-38.83811950683594,-13.343935012817383,2.1761529445648193,-42.89789962768555,5.5079498291015625,-20.107940673828125,-24.75234031677246,2.358909845352173,-48.06236267089844,-20.399887084960938,-2.9242992401123047,-27.02735710144043,-55.80746841430664,3.6558837890625,-24.56902313232422,-29.17045783996582,-6.068840026855469,-22.184261322021484,-17.401113510131836,-25.346715927124023,-15.458365440368652,-45.90984344482422,3.1354241371154785,-23.16278839111328,2.5992767810821533,-28.567533493041992,-23.966445922851562,-22.537946701049805,-4.179891586303711,-27.094905853271484,-46.93598556518555,-45.66786575317383,-25.203704833984375,1.6003302335739136,-4.579592227935791,-27.624719619750977,-22.07362174987793,-48.453460693359375,-10.64020824432373,-22.54204559326172,-21.163888931274414,3.1218557357788086,-16.75324821472168,-45.418800354003906,-26.679983139038086,-8.322824478149414,-1.79793119430542,-12.529847145080566,-8.876056671142578,-18.586307525634766,2.916672706604004,-27.990652084350586,5.724306106567383,-49.554412841796875,2.6781933307647705,-54.628116607666016,5.425442695617676,-32.64115905761719,-14.968781471252441,-7.084889888763428,-41.0161247253418,-13.203702926635742,-26.1689510345459,-47.43607711791992,-19.84899139404297,-16.771060943603516,-16.223562240600586,-1.7965245246887207,5.627100467681885,1.062442660331726,-46.48942565917969,-36.35879135131836,-50.301658630371094,-42.12896728515625,-19.998985290527344,-1.178498387336731,-26.91268539428711,-39.642311096191406,-17.085508346557617,-21.092405319213867,-51.43650817871094,-21.96946144104004,3.3929576873779297,-24.646196365356445,-25.89952850341797,-22.94940757751465,-11.856252670288086,-33.00667953491211,-2.761845588684082,-7.198315143585205,-21.21609878540039,-43.8115348815918,-23.595497131347656,-33.69797134399414,-34.62378692626953,3.2590668201446533,-25.037076950073242,-52.96902847290039,3.898393392562866,1.1320459842681885,-22.848888397216797,-56.09363555908203,-32.45058822631836,-2.9924747943878174,-12.02886962890625,-3.865690231323242,-1.111945629119873,-57.356468200683594,-12.824349403381348,-27.606807708740234,-22.407909393310547,-25.676342010498047,-13.409008979797363,-32.82294845581055,-20.847990036010742,-27.356698989868164,4.755083084106445,-5.408461093902588,3.7288291454315186,2.353464126586914,-18.094758987426758,3.064800977706909,-25.009157180786133,-51.71805953979492,-51.405887603759766,-21.048128128051758,-33.175201416015625,-44.95121765136719,5.6695780754089355,-13.222515106201172,-26.460342407226562,-27.028039932250977,-35.81352615356445,-21.59893798828125,-11.45799732208252,-57.20973205566406,-18.16937255859375,-31.597028732299805,-58.463417053222656,-33.31927490234375,-22.774337768554688,-0.16954034566879272,3.233515739440918,-57.33368682861328,-9.259799003601074,-24.094974517822266,-18.11408042907715,-45.410301208496094,-61.4493522644043,-45.0149040222168,-24.186664581298828,-9.804941177368164,-24.089874267578125,-17.121925354003906,-60.02864456176758,-24.034534454345703,-57.455989837646484,-51.41348648071289,-44.20246505737305,-26.35558319091797,-24.380407333374023,-43.817325592041016,-30.14337730407715,8.55947494506836,-44.32026290893555,-7.699850082397461,-55.313114166259766,-46.90277099609375,-60.61343002319336,-22.800397872924805,-18.442522048950195,-59.02464294433594,-16.45244598388672,-28.338573455810547,-4.701550483703613,-20.53946304321289,-59.932411193847656,-44.786903381347656,-41.10939025878906,-54.686344146728516,-1.71440851688385,-21.82655906677246,4.690995216369629,-53.79663848876953,-22.240474700927734,-28.690826416015625,-48.49375915527344,-30.4007568359375,-19.247663497924805,-22.419822692871094,-0.2901775538921356,-56.85517883300781,-45.76918411254883,-57.301536560058594,2.8866732120513916,-44.303489685058594,-57.277931213378906,-22.71540069580078,-56.59347152709961,-24.116666793823242,-53.04496765136719,-39.08127975463867,-51.827720642089844,-24.83292579650879,-26.94444465637207,-43.02353286743164,-12.063143730163574,-19.192378997802734,-22.91898536682129,-39.33154296875,-15.139181137084961,-14.837150573730469,-2.2244982719421387,3.268500566482544,-13.769678115844727,-40.633888244628906,-36.12190246582031,-30.61150550842285,-54.4194221496582,-47.94019317626953,-49.75210189819336,-24.952556610107422,-58.39536666870117,-18.762004852294922,-43.489131927490234,-35.19084167480469,-17.040666580200195,-23.634075164794922,-29.815664291381836,-49.706398010253906,-59.97148513793945,-40.862022399902344,-6.405794143676758,-26.91015625,-26.910972595214844,-3.765705108642578,-57.1937255859375,-51.0638313293457,-53.38794708251953,-34.33482360839844,-53.83200454711914,-28.49983024597168,-57.310325622558594,-32.623374938964844,-39.46856689453125,-16.574262619018555,-13.227238655090332,-16.29647445678711,-20.035499572753906,-16.19779396057129,-21.97016716003418,-4.963286876678467,-11.700189590454102,-56.23871612548828,-59.656429290771484,-27.216228485107422,-50.78239059448242,-46.55712890625,-53.749977111816406,-52.30742263793945,-36.71504211425781,-25.622644424438477,-59.74980163574219,-57.94537353515625,-14.57813549041748,-28.366352081298828,-10.105706214904785,-22.624971389770508,-31.421632766723633,-55.69995880126953,-32.51936340332031,-20.79132843017578,-20.28627586364746,-34.580963134765625,-57.02418899536133,-53.08963394165039,-34.652835845947266,-21.09598731994629,-61.79409408569336,-4.797871112823486,-59.41354751586914,-43.74533462524414,-17.480562210083008,-58.81941604614258,-24.830078125,-7.575258255004883,-47.137672424316406,-42.03395080566406,-42.451988220214844,-45.48817443847656,-60.72811508178711,-42.33014678955078,-49.58896255493164,-58.39634323120117,-17.35928726196289,2.7269175052642822,-23.371246337890625,-45.77041244506836,-23.427501678466797,-40.478328704833984,-32.27359390258789,-12.749862670898438,-44.7067756652832,-53.458744049072266,-26.090343475341797,-24.70560646057129,-1.957550287246704,-20.873825073242188,-20.741275787353516,-37.45613479614258,-37.83766555786133,-25.776535034179688,-19.053573608398438,-54.4127197265625,-52.1740837097168,-34.71173858642578,-30.272428512573242,-57.53858947753906,-33.59904098510742,-33.148780822753906,-27.637117385864258,-14.98497486114502,-53.526649475097656,-31.87828254699707,-49.3543586730957,-28.46522331237793,-16.82077980041504,-48.02473068237305,-21.325071334838867,-12.851127624511719,-37.13003158569336,-20.835689544677734,-48.15163803100586,-22.8138370513916,-8.500358581542969,-17.427425384521484,-39.89665222167969,-27.56113052368164,-56.29511260986328,-47.759464263916016,-11.401653289794922,-22.134124755859375,-14.689002990722656,-38.514400482177734,-48.25889205932617,-22.29452133178711,-28.258914947509766,-50.73076629638672,-50.89884567260742,-58.991817474365234,-17.26342010498047,-56.45672607421875,-60.45673751831055,-55.45356369018555,-26.15735626220703,-55.71590805053711,-58.6529655456543,-11.199540138244629,-43.244590759277344,-19.05658721923828,-48.84956741333008,-19.862485885620117,-34.92171859741211,-18.350650787353516,-25.166873931884766,-48.52614974975586,-42.835723876953125,-48.5570182800293,-55.960323333740234,-36.00254821777344,-31.766355514526367,-27.001079559326172,-57.680580139160156,-53.65923309326172,-21.964357376098633,-54.761390686035156,-19.562583923339844,-45.08570861816406,-56.43372344970703,-54.12318420410156,-24.891902923583984,-23.51077651977539,-54.43779754638672,-37.37183380126953,-46.368953704833984,-58.813507080078125,-53.221229553222656,-22.20250701904297,-54.47157669067383,-31.1739559173584,-23.386960983276367,-31.83383560180664,-22.514713287353516,-55.4337158203125,-37.68572998046875,-55.763450622558594,-61.65007400512695,-18.55303192138672,-57.13777160644531,-49.199256896972656,-48.946014404296875,-24.203771591186523,-45.21900939941406,-58.772056579589844,-61.56269836425781,-54.72255325317383,-39.67714309692383,-20.526477813720703,-60.55644989013672,-52.00541687011719,-26.783954620361328,-41.21248245239258,-45.088130950927734,-38.742427825927734,-57.08750534057617,-61.05464172363281,-24.111249923706055,-33.996299743652344,-52.88554382324219,-61.16358947753906,-19.814058303833008,-60.403839111328125,-61.06937026977539,-55.790504455566406,-23.154356002807617,-60.221641540527344,-51.01373291015625,-18.3721866607666,-19.365100860595703,-33.674644470214844,-51.919132232666016,-57.392189025878906,-56.021751403808594,-30.531414031982422,-60.470664978027344,-20.210098266601562,-57.29292678833008,-23.083383560180664,-36.003028869628906,-44.913726806640625,-53.26694869995117,-49.76859664916992,-54.9754524230957,-26.085575103759766,-52.77754592895508,-46.906734466552734,-59.78610610961914,-22.611194610595703,-19.054874420166016,-47.016849517822266,-47.95608139038086,-55.92776870727539,-40.849308013916016,-25.431528091430664,-10.87605094909668,-19.730167388916016,-25.279226303100586,-25.655576705932617,-53.70624923706055,-11.411773681640625,3.5665104389190674,-25.802967071533203,-26.459903717041016,-20.662273406982422,-59.39613342285156,-39.243751525878906,-44.50551986694336,-49.55241012573242,-34.3503303527832,-28.62833595275879,-55.472930908203125,-9.186180114746094,-30.390403747558594,-55.86290740966797,-24.864215850830078,-56.62386703491211,-54.113037109375,-55.59767150878906,-31.901817321777344,-61.08401107788086,-51.980499267578125,-53.27214813232422,-48.41675567626953,-20.08293914794922,-41.30412292480469,-50.64435958862305,-34.28408432006836,-22.23907470703125,-54.60043716430664,-46.11091995239258,-56.137996673583984,-56.133060455322266,-60.87278366088867,-53.78764724731445,-27.10938835144043,-53.085601806640625,-36.74690628051758,-47.30632400512695,-45.562137603759766,-57.96723937988281,-32.01554489135742,-50.38372039794922,-20.75240707397461,-24.35267448425293,-45.95016098022461,-46.782135009765625,-58.217987060546875,-22.04914093017578,-54.90667724609375,-56.548606872558594,-51.8326416015625,-50.36680603027344,-58.59808349609375,-55.10395431518555,-58.01259231567383,-50.7581901550293,-38.0921745300293,-34.46891403198242,-46.83733367919922,-56.36453628540039,-15.979307174682617,-24.43639373779297,-56.8753662109375,-31.32452392578125,-41.17765808105469,-56.01616287231445,-50.44327926635742,-38.35037612915039,-46.86168670654297,-56.05404281616211,-55.28717803955078,-32.30078125,-60.68704605102539,-51.399959564208984,-57.49631881713867,-44.92483139038086,-29.392942428588867,-33.106502532958984,-46.552059173583984,-27.456174850463867,-25.262187957763672,-50.550540924072266,-24.392318725585938,-22.66566276550293,-54.73454666137695,-24.831300735473633,-14.822196960449219,-55.00148010253906,-37.009429931640625,-45.55710983276367,-58.27265167236328,-46.057865142822266,-57.94807434082031,-24.756271362304688,-56.16872024536133,-52.91830062866211,-24.14725112915039,-58.824092864990234,-58.23480224609375,-22.92374038696289,-19.561616897583008,-46.134483337402344,-53.359127044677734,-32.27759552001953,-41.9996452331543,-57.90174102783203,-51.48514938354492,-34.783321380615234,-22.827219009399414,-52.88411331176758,-15.558460235595703,-55.660308837890625,-52.155555725097656,-28.33424186706543,-57.57941818237305,-26.115779876708984,-50.700599670410156,-43.93034362792969,4.795296669006348,-60.3317985534668,-46.19513702392578,-23.56117820739746,-42.301971435546875,-57.717838287353516,-58.554542541503906,-61.60424041748047,-29.976152420043945,-46.55694580078125,-49.078369140625,-59.840858459472656,-24.017560958862305,-39.16007995605469,-54.04932403564453,-27.756282806396484,-52.93355178833008,-49.21672821044922,-47.79195022583008,-56.890586853027344,-60.23253631591797,-59.08201217651367,-37.39566421508789,-43.37019729614258,-36.307395935058594,-57.51348876953125,-58.2271614074707,-55.220733642578125,-59.380313873291016,-61.0298957824707,-31.273229598999023,-54.656097412109375,-61.26728057861328,-32.749794006347656,-57.2989387512207,-55.81544876098633,-52.86172866821289,-61.35641860961914,-39.542694091796875,-28.69170570373535,-57.84256362915039,-57.38410568237305,-58.26475143432617,-24.64010238647461,-31.712820053100586,-38.716712951660156,-56.680809020996094,-53.271568298339844,-59.27810287475586,-58.916893005371094,-28.768247604370117,-52.22929000854492,-43.13962173461914,-54.486778259277344,-55.9432258605957,-56.01138687133789,-16.01112937927246],\"y\":[-16.361820220947266,-15.779809951782227,-16.335853576660156,-16.253681182861328,-16.254066467285156,-14.550056457519531,-15.907087326049805,-16.08872413635254,-16.308109283447266,-15.759589195251465,-15.6624755859375,-16.2615909576416,-16.225542068481445,-15.337106704711914,-15.844223976135254,-15.41841983795166,-15.132871627807617,-15.8972806930542,-15.98827838897705,-15.94027042388916,-15.83392333984375,-15.746566772460938,-15.803404808044434,-16.009958267211914,-15.645245552062988,-14.883587837219238,-15.937519073486328,-16.1302490234375,-15.671626091003418,-16.05422019958496,-16.03171730041504,-15.7283296585083,-16.084733963012695,-16.02001953125,-15.855358123779297,-14.282817840576172,-16.447328567504883,-15.856416702270508,-14.195028305053711,-16.141023635864258,-15.987025260925293,-16.298168182373047,-14.250309944152832,-16.11623191833496,-13.03703498840332,-16.094940185546875,-16.027807235717773,-16.32186508178711,-15.38267993927002,-15.840178489685059,-16.06751251220703,-14.273711204528809,-15.711495399475098,-14.816160202026367,-15.404013633728027,-15.41186237335205,-16.04846954345703,-15.88448715209961,-15.76944637298584,-16.130168914794922,-15.54837703704834,-15.320830345153809,-14.976142883300781,-15.13509750366211,-14.925743103027344,-16.301477432250977,-13.942815780639648,-15.347522735595703,-12.026328086853027,-14.893272399902344,-9.89132308959961,-9.103365898132324,-14.558870315551758,-12.634275436401367,-14.436299324035645,-13.46353816986084,-13.723312377929688,-5.606637477874756,-13.630328178405762,-14.980585098266602,-12.140780448913574,-11.307210922241211,-9.52402114868164,-14.705153465270996,-10.356988906860352,-13.290812492370605,-8.931989669799805,-8.734233856201172,-14.87905502319336,-11.583441734313965,-14.10488510131836,-6.007915496826172,-10.632467269897461,-9.158368110656738,-8.240277290344238,-12.144639015197754,-7.682247638702393,-10.88007640838623,-9.947218894958496,-4.80606746673584,-9.302791595458984,-11.447587966918945,-10.27541446685791,6.295204162597656,-11.608513832092285,-8.548639297485352,5.191487789154053,-7.2415452003479,-2.8531441688537598,-8.827313423156738,-8.819413185119629,-8.733797073364258,-4.430912494659424,-1.1800084114074707,-2.1654129028320312,-3.031647205352783,0.28122538328170776,-7.129714012145996,-1.6940820217132568,-4.944301605224609,-5.4776291847229,-0.4566078186035156,-6.129400253295898,1.6796592473983765,-12.062687873840332,2.4539098739624023,8.6730375289917,-1.3960827589035034,-4.679455280303955,3.7687554359436035,7.726240158081055,-3.528660774230957,-9.310403823852539,9.270098686218262,3.8394763469696045,-4.734541416168213,-2.0096726417541504,-3.408454656600952,2.639431953430176,6.070566177368164,-0.18799622356891632,4.52279806137085,-2.4575603008270264,9.681605339050293,0.33899953961372375,-4.569856643676758,3.8706157207489014,1.785160779953003,7.801499843597412,1.722807765007019,-0.6258862018585205,-7.090283393859863,4.148773193359375,7.360567569732666,-1.3442491292953491,-6.180909633636475,-3.546621084213257,0.7166469097137451,-3.024249315261841,-5.348948001861572,15.90164852142334,14.678016662597656,-5.102844715118408,0.8748424053192139,-5.587430477142334,-6.48853874206543,13.04361343383789,8.460409164428711,-6.680020809173584,6.660907745361328,9.145901679992676,4.577164173126221,0.5574796199798584,23.262434005737305,-0.21271641552448273,5.126805782318115,23.00533103942871,8.65401554107666,4.368174076080322,4.122507572174072,5.306412220001221,3.255958318710327,15.04498291015625,7.875966548919678,8.325067520141602,1.6526601314544678,14.871867179870605,1.6188009977340698,21.913148880004883,16.919200897216797,5.632134437561035,17.93732261657715,-1.769584059715271,-0.3041037321090698,9.365058898925781,12.167717933654785,22.557514190673828,12.405478477478027,15.730504989624023,13.467552185058594,-0.7011029124259949,11.787775039672852,5.1388421058654785,1.1421014070510864,13.996611595153809,20.845108032226562,22.136438369750977,20.40630531311035,4.121264457702637,22.439197540283203,9.747151374816895,19.46346092224121,17.901527404785156,18.4517879486084,5.442156791687012,6.169061660766602,7.344395637512207,18.322479248046875,17.959823608398438,14.665425300598145,13.388880729675293,5.556291103363037,12.459091186523438,13.617143630981445,19.019458770751953,22.672502517700195,8.238065719604492,11.532354354858398,15.434355735778809,21.608842849731445,15.315648078918457,15.685540199279785,17.23826789855957,22.084321975708008,21.489702224731445,19.644943237304688,21.500322341918945,12.290305137634277,16.94744300842285,22.281085968017578,22.049165725708008,21.905466079711914,15.978676795959473,21.957502365112305,22.631105422973633,21.234161376953125,12.276490211486816,18.036808013916016,21.102216720581055,22.328542709350586,9.763017654418945,19.40060806274414,15.942572593688965,7.513823509216309,1.6276730298995972,17.298564910888672,22.459802627563477,19.299549102783203,18.67534828186035,14.244789123535156,18.565271377563477,20.681467056274414,22.936595916748047,20.688922882080078,22.36746597290039,8.451547622680664,22.654457092285156,12.21751880645752,21.824769973754883,21.440153121948242,10.476880073547363,12.93051528930664,19.970487594604492,20.919898986816406,13.100805282592773,16.14357566833496,21.863590240478516,16.60774803161621,14.53182601928711,21.703250885009766,20.322996139526367,17.433115005493164,14.965351104736328,22.43239402770996,18.760120391845703,6.91385555267334,22.475189208984375,19.075786590576172,20.87996482849121,22.347511291503906,20.192440032958984,20.292095184326172,19.855852127075195,19.923770904541016,15.612548828125,15.612326622009277,12.001120567321777,21.89857292175293,3.023167848587036,-21.417776107788086,16.567337036132812,21.398088455200195,13.963879585266113,-13.459354400634766,21.56645393371582,17.729642868041992,21.161808013916016,0.13198646903038025,12.203413009643555,21.595422744750977,16.219640731811523,14.897159576416016,19.872188568115234,20.38953971862793,11.741121292114258,22.231595993041992,19.878318786621094,19.301620483398438,-1.0704823732376099,17.69011878967285,19.352642059326172,20.929643630981445,21.61830711364746,-6.435362815856934,15.227154731750488,16.03811264038086,14.601219177246094,17.969728469848633,16.451993942260742,5.121517181396484,22.44900894165039,10.06734848022461,16.275875091552734,7.29561185836792,4.9592604637146,20.557310104370117,13.331310272216797,-21.76421356201172,14.90243911743164,21.076555252075195,11.581657409667969,20.284456253051758,22.69734001159668,14.994220733642578,13.233633995056152,21.246170043945312,15.31728458404541,16.12630271911621,13.926749229431152,18.807876586914062,22.19721794128418,3.0700910091400146,16.911060333251953,-18.509916305541992,16.629060745239258,19.457128524780273,-19.389902114868164,3.091679811477661,11.160059928894043,-8.899198532104492,13.54362678527832,-8.299174308776855,-0.21261946856975555,17.794315338134766,-17.951980590820312,-17.613962173461914,-1.0093016624450684,20.496217727661133,22.33271598815918,19.60747528076172,11.853466987609863,-13.986903190612793,15.985269546508789,21.60236167907715,-21.528425216674805,18.3967342376709,-5.7601447105407715,12.828973770141602,4.6985182762146,17.30315589904785,15.023802757263184,6.740036487579346,7.120438098907471,-5.480535984039307,14.43471908569336,15.384880065917969,-11.790454864501953,21.848979949951172,22.684951782226562,10.635125160217285,1.9542204141616821,21.622825622558594,-15.631025314331055,-6.916806697845459,-7.4610772132873535,-19.740015029907227,-5.330191612243652,14.63255500793457,16.882278442382812,-12.8677396774292,18.859060287475586,-3.686506748199463,10.867026329040527,19.91597557067871,-21.991844177246094,18.823591232299805,12.871410369873047,12.377861976623535,-19.206151962280273,-15.074134826660156,-26.594709396362305,6.1735944747924805,7.93303918838501,17.01129913330078,21.749786376953125,5.567785739898682,1.2873491048812866,-21.534503936767578,-5.740913391113281,-19.455490112304688,5.205634117126465,22.013221740722656,-23.897380828857422,-2.1068873405456543,-11.040709495544434,-0.4954468011856079,0.2528064548969269,-22.496339797973633,-7.513643741607666,-20.329837799072266,5.975646018981934,12.569978713989258,-20.117733001708984,11.95559024810791,-20.573863983154297,11.406901359558105,-22.982358932495117,-5.788583278656006,6.415911674499512,1.105329990386963,2.504546880722046,2.5706048011779785,7.2475199699401855,3.6657590866088867,-9.73564624786377,18.0886287689209,-3.1128158569335938,3.8864970207214355,-21.321823120117188,22.063201904296875,21.698833465576172,-24.07575225830078,-13.293054580688477,-10.02605152130127,0.2790778875350952,4.294855117797852,-23.68456268310547,-20.193500518798828,-17.902545928955078,-8.599343299865723,6.825785160064697,-7.213260650634766,4.049033164978027,-5.538397312164307,-6.398801326751709,-0.33786365389823914,-23.25450325012207,4.8202362060546875,-6.12894868850708,-16.621877670288086,-6.113379955291748,-26.587223052978516,-19.264795303344727,-11.762910842895508,-24.532424926757812,7.106715679168701,0.8782857656478882,21.055484771728516,16.292083740234375,8.52957820892334,-24.017139434814453,-6.54494571685791,-21.350099563598633,11.573002815246582,-16.159025192260742,-23.746530532836914,-0.3788297474384308,-16.083070755004883,3.6467995643615723,-15.518975257873535,14.174698829650879,-14.328801155090332,-9.516327857971191,-24.74104881286621,-6.284468173980713,-16.793764114379883,-7.210733413696289,-22.3868408203125,-24.431543350219727,-6.667901515960693,6.455279350280762,-11.039615631103516,-16.865257263183594,-8.998785972595215,-16.668176651000977,13.122124671936035,-12.340916633605957,-19.45224380493164,-25.392263412475586,-21.057641983032227,-25.694507598876953,-18.627471923828125,5.276254177093506,-6.637482643127441,-22.204952239990234,-0.2416606992483139,-19.049596786499023,-8.209649085998535,-24.651588439941406,-11.845846176147461,-26.781333923339844,-10.558191299438477,-17.561853408813477,3.8279848098754883,15.569731712341309,-26.69011116027832,-16.159542083740234,-9.94363784790039,-23.84968376159668,-16.375499725341797,-15.766866683959961,-18.375085830688477,-9.617822647094727,-8.262994766235352,1.0874812602996826,14.898786544799805,-23.950801849365234,-25.749513626098633,-21.356678009033203,-0.1266985684633255,1.8040132522583008,-6.726266860961914,-10.641012191772461,-25.497997283935547,-10.864068031311035,-24.392852783203125,12.730657577514648,-17.230449676513672,-23.768434524536133,0.701216459274292,-21.161117553710938,-14.089804649353027,-24.210689544677734,-24.441650390625,-8.44989013671875,-13.430133819580078,-5.894180774688721,8.614935874938965,18.18461799621582,0.43780890107154846,-26.54043197631836,-18.715877532958984,-24.478670120239258,-18.97724723815918,-22.841022491455078,-18.84634017944336,-22.18474006652832,-9.060624122619629,-24.942750930786133,-9.945076942443848,-21.373035430908203,-20.41623306274414,-24.483776092529297,22.340124130249023,-21.391136169433594,-17.071651458740234,4.775672435760498,-12.637828826904297,-6.254934310913086,-17.518400192260742,0.27419257164001465,-21.38163185119629,-15.090910911560059,20.34573745727539,-7.945589065551758,-13.265302658081055,-5.74897575378418,-8.426911354064941,-15.321724891662598,-23.21902847290039,-6.361571788787842,-21.902685165405273,8.170926094055176,5.053738594055176,-25.478530883789062,-25.27042579650879,-23.519908905029297,-11.559473037719727,-22.66109275817871,-26.33126449584961,-16.63715934753418,-4.640350341796875,5.349389553070068,-23.348161697387695,-10.133673667907715,-20.646272659301758,-27.026897430419922,-8.61355972290039,-24.20094108581543,-19.350387573242188,-5.313682556152344,-17.387836456298828,-23.0960693359375,-24.152097702026367,-23.751806259155273,-10.748003959655762,-24.024673461914062,-26.1068172454834,-23.264629364013672,-0.18423058092594147,-0.6170899271965027,-24.24873161315918,-0.3567231297492981,-22.18157386779785,-11.065390586853027,-18.765586853027344,-15.63676643371582,3.965858221054077,-19.5849609375,-20.339698791503906,-26.188779830932617,-23.967321395874023,5.540804386138916,-7.910872936248779,-24.67304801940918,3.1020679473876953,-22.177806854248047,-13.37882137298584,17.123640060424805,-18.265596389770508,-23.470003128051758,-25.57682228088379,-7.410275459289551,-23.020492553710938,-24.834402084350586,-23.37571907043457,10.137938499450684,-12.680060386657715,-18.878860473632812,14.433109283447266,-7.020318984985352,-25.086841583251953,-17.466476440429688,-24.40730857849121,-9.54043960571289,-24.476682662963867,-20.18450355529785,-24.787025451660156,-23.104970932006836,-22.198875427246094,-15.632608413696289,21.795005798339844,-22.221460342407227,2.7476375102996826,4.363931179046631,-20.697120666503906,-23.2453556060791,-15.231359481811523,17.392148971557617,-25.332096099853516,-21.291629791259766,-26.461578369140625,-4.83599853515625,0.7642396092414856,-23.875865936279297,-21.189451217651367,4.651265621185303,-1.2829662561416626,4.2460103034973145,1.1224416494369507,-22.06045913696289,-10.642581939697266,-24.572834014892578,1.9373729228973389,11.306924819946289,1.5894805192947388,22.274765014648438,9.24215316772461,-23.695941925048828,-11.628920555114746,-11.921716690063477,-10.944352149963379,-25.570959091186523,13.556899070739746,16.007009506225586,20.09746742248535,-22.265501022338867,-22.3950252532959,-18.1123104095459,12.390761375427246,-25.131376266479492,-19.64757537841797,2.459778308868408,-23.815906524658203,18.48007583618164,15.909793853759766,-10.482934951782227,-14.879375457763672,4.799810886383057,6.1731061935424805,-25.94617462158203,12.802408218383789,12.303811073303223,-12.32595157623291,-15.47110652923584,-3.1750450134277344,18.773839950561523,-23.60097312927246,17.52791404724121,12.353050231933594,1.2193065881729126,-13.385440826416016,-20.6785888671875,-13.838824272155762,22.34728240966797,13.878340721130371,1.0788578987121582,13.034238815307617,-11.220276832580566,-16.36496925354004,-24.430309295654297,-24.91350555419922,-23.174724578857422,-22.283349990844727,4.402223110198975,19.061283111572266,-9.88301944732666,16.266250610351562,-6.721324920654297,-19.311098098754883,2.6984705924987793,-11.905865669250488,-10.490737915039062,20.58790397644043,-17.574600219726562,14.381814002990723,10.371413230895996,-6.354256629943848,7.588992118835449,-17.31023597717285,-20.03268051147461,-5.267254829406738,17.579452514648438,-11.311470031738281,-19.65898323059082,-14.220335960388184,-24.2360782623291,-5.035735130310059,-9.187877655029297,-23.737092971801758,6.6303935050964355,-13.480283737182617,-24.185558319091797,-2.3886525630950928,12.365144729614258,22.852487564086914,17.73923110961914,0.29406896233558655,-23.205907821655273,17.657398223876953,-15.44698715209961,-24.748451232910156,-13.068167686462402,-24.687833786010742,-17.64620590209961,-13.342121124267578,-24.877723693847656,-5.627381324768066,19.195287704467773,-20.592294692993164,-22.822628021240234,-12.392745018005371,-5.29685640335083,18.3966064453125,-5.612801551818848,-12.70975399017334,-1.9915599822998047,-4.86052942276001,5.028310775756836,14.93230152130127,-7.39061975479126,12.742622375488281,-20.197771072387695,2.9305307865142822,13.689915657043457,8.440997123718262,16.435916900634766,-23.659034729003906,-16.91691780090332,-22.93045997619629,-2.243581771850586,8.212081909179688,-20.480220794677734,0.6934813261032104,22.139511108398438,-0.11230017989873886,-2.2526848316192627,7.993847370147705,23.618736267089844,-16.094560623168945,10.416107177734375,-8.018412590026855,8.572928428649902,16.689115524291992,19.43795394897461,-25.233823776245117,16.62154769897461,16.25724983215332,-17.876220703125,10.144597053527832,-14.463724136352539,3.522097110748291,-12.005525588989258,0.5210118293762207,-22.10284423828125,10.028314590454102,12.109990119934082,4.14377498626709,18.98324966430664,9.87398910522461,-19.803491592407227,-4.5534257888793945,-26.494836807250977,-20.69808578491211,17.422630310058594,17.75004768371582,20.991455078125,-18.99625587463379,17.6561336517334,19.310501098632812,7.483354568481445,16.069660186767578,-25.288917541503906,-13.234679222106934,-9.675678253173828,10.585102081298828,3.060117721557617,-13.552643775939941,11.511651992797852,2.7345316410064697,17.246116638183594,-7.868590354919434,20.073246002197266,-8.265401840209961,8.523849487304688,-9.67528247833252,-0.15115319192409515,8.12116813659668,-21.01523780822754,6.193255424499512,-19.81108856201172,3.3923308849334717,21.675750732421875,-22.212383270263672,-1.2714791297912598,-18.361547470092773,-20.704593658447266,-18.98511505126953,20.112041473388672,14.106383323669434,-9.33575439453125,1.880812644958496,11.693781852722168,-3.9543275833129883,18.030925750732422,-3.8947346210479736,18.740171432495117,-7.57155179977417,21.20557975769043,1.205051064491272,-5.889257431030273,10.484153747558594,9.02269172668457,4.793694972991943,-22.84680938720703,-7.950101852416992,18.848642349243164,21.412921905517578,15.389752388000488,14.383732795715332,-9.089735984802246,-8.356694221496582,14.45185661315918,20.323776245117188,2.9695520401000977,3.803563117980957,11.51827335357666,-0.9749449491500854,-4.59013557434082,0.09506220370531082,17.704126358032227,22.027965545654297,9.611307144165039,11.204870223999023,-16.062496185302734,-5.499895095825195,0.3347187936306,15.96102237701416,-7.380648136138916,-8.898591041564941,21.32649040222168,17.668628692626953,13.01773452758789,1.8311147689819336,22.7720890045166,-5.112704753875732,23.763717651367188,3.9195079803466797,6.2355828285217285,-23.3922176361084,20.469112396240234,-3.6726138591766357,17.78407859802246,13.255943298339844,-0.23533353209495544,14.40190315246582,9.714678764343262,9.513166427612305,8.563947677612305,22.171241760253906,-25.096595764160156,-5.614969253540039,-11.389636039733887,21.551244735717773,-6.946238994598389,7.685208320617676,19.66855812072754,-3.9942097663879395,-6.311210632324219,20.126012802124023,10.611588478088379,-16.67028045654297,17.631526947021484,18.842615127563477,-11.729116439819336,-8.77836799621582,-2.8112080097198486,17.759109497070312,-6.199103832244873,21.275033950805664,-19.607542037963867,12.156052589416504,-7.947963237762451,16.407989501953125,-24.382097244262695,10.339624404907227,-4.819643020629883,3.8255860805511475,6.462032318115234,-23.601463317871094,21.284164428710938,-25.485620498657227,5.617008209228516,-4.443841934204102,20.325790405273438,-19.806795120239258,6.966818809509277,-7.795088291168213,17.605337142944336,21.68478012084961,20.413471221923828,6.114513397216797,-22.914888381958008,-13.972010612487793,8.504727363586426,-8.131278991699219,20.498403549194336,14.731219291687012,22.38031768798828,14.03779125213623,-7.082364082336426,3.4424688816070557,21.433364868164062,5.409740924835205,15.563382148742676,15.148000717163086,1.5295497179031372,8.287322998046875,-3.001478910446167,-8.122086524963379,3.2484824657440186,7.7488884925842285,6.358916759490967,-8.54409122467041,8.830432891845703,18.244380950927734,-14.366067886352539,-10.71274185180664,9.235980033874512,20.545209884643555,-12.745397567749023,18.809255599975586,14.045966148376465,-13.983137130737305,20.704166412353516,-11.51880168914795,-13.497330665588379,-5.429079055786133,-11.843555450439453,-12.10684585571289,18.28461456298828,-9.029666900634766,20.3558349609375,19.521955490112305,3.45194149017334,22.822114944458008,-2.0392203330993652,17.045820236206055,7.287178993225098,7.155648708343506,15.506455421447754,12.305689811706543,18.637821197509766,-14.099349975585938,-22.528852462768555,19.646472930908203,-3.4975409507751465,20.355653762817383,22.40224266052246,2.2598378658294678,19.243623733520508,-19.07350730895996,19.50234603881836,-1.1930700540542603,15.35898494720459,11.577619552612305,-6.561381816864014,21.5494327545166,-3.9937024116516113,-0.30431458353996277,-4.509463310241699,-14.957520484924316,-14.331184387207031,22.75275421142578,-6.709840774536133,6.169892311096191,17.97254753112793,5.943795204162598,20.1517391204834,10.559906005859375,13.762832641601562,-13.4193115234375,-1.6052665710449219,6.338919162750244,-10.158003807067871,-1.8192254304885864,14.443822860717773,16.45521354675293,5.6809587478637695,7.826050758361816,-11.155962944030762,20.382030487060547,14.782792091369629,14.521286964416504,21.566761016845703,21.37339210510254,7.613326549530029,19.55816078186035,2.837855815887451,17.723384857177734,-10.830771446228027,11.234725952148438,2.8394289016723633,12.91047477722168,21.26911163330078,-12.647017478942871,12.413564682006836,19.547130584716797,17.743925094604492,-13.125130653381348,-1.953630805015564,11.400617599487305,12.9284086227417,6.568004608154297,18.82811164855957,13.905067443847656,19.41634178161621,19.930856704711914,16.165124893188477,-23.480091094970703,5.43259334564209,1.0517287254333496,-7.033051490783691,14.952067375183105,9.488622665405273,-14.678818702697754,-6.552938938140869,-2.2716662883758545,21.44144058227539,-7.5433878898620605,18.793441772460938,9.909852981567383,20.89454460144043,-13.865548133850098,15.167969703674316,6.1832122802734375,-6.09041166305542,20.935937881469727,-13.210233688354492,-0.883230984210968,-14.381515502929688,-10.541876792907715,-12.636337280273438,19.280109405517578,4.014751434326172,-11.289289474487305,16.890417098999023,13.128633499145508,-11.245570182800293,10.039063453674316,9.905296325683594,-7.5543212890625,14.855836868286133,-6.276841163635254,12.990799903869629,17.530290603637695,-6.534223556518555,5.053032398223877,3.229681968688965,-5.374629497528076,-7.600494384765625,18.726781845092773,-0.4937228262424469,17.90277671813965,3.637967824935913,16.60931396484375,-9.835807800292969,-20.37779426574707,2.7313992977142334,0.2616932988166809,-8.95548152923584,8.580424308776855,4.380098819732666,22.60605812072754,-0.10458570718765259,-10.144373893737793,-15.417024612426758,5.242471694946289,15.99414348602295,21.195341110229492,-5.1999592781066895,10.358951568603516,-12.316866874694824,21.04066276550293,-5.914879322052002,9.978952407836914,8.642230033874512,15.966367721557617,-10.447552680969238,-1.6757302284240723,21.989442825317383,21.173152923583984,22.041696548461914,21.911394119262695,9.180853843688965,-2.7829811573028564,-2.8587212562561035,6.282002925872803,-1.781969428062439,7.312793254852295,2.3428268432617188,8.001277923583984,-14.773675918579102,18.932809829711914,22.125301361083984,-11.824875831604004,17.085384368896484,0.851847231388092,-11.815702438354492,3.244391441345215,16.989704132080078,11.671980857849121,21.628875732421875,2.040372133255005,16.10492706298828,-14.984270095825195,-11.854048728942871,-6.4897685050964355,-12.248101234436035,9.768753051757812,19.601234436035156,-9.206646919250488,-13.82865047454834,18.23028564453125,8.30234432220459,-3.6231789588928223,10.74884033203125,7.079859256744385,1.9703171253204346,-7.499462604522705,-7.27246618270874,21.407285690307617,-14.180407524108887,22.565502166748047,23.9764404296875,10.92959213256836,-10.86744213104248,-4.184638500213623,-11.481626510620117,-12.529376983642578,0.5464609861373901,-5.248147964477539,-3.3966503143310547,1.125975251197815,-14.986812591552734,7.924600601196289,-2.068311929702759,-11.513053894042969,19.570131301879883,18.510791778564453,21.609455108642578,18.430519104003906,6.765055179595947,20.01709747314453,-9.42308521270752,5.369585990905762,-6.2952470779418945,19.429716110229492,-11.779939651489258,1.2744698524475098,-0.24086879193782806,0.5268919467926025,23.33989906311035,-21.022335052490234,-14.611124992370605,16.26538848876953,17.921573638916016,-3.5755646228790283,-2.7431869506835938,-4.4884724617004395,6.760694980621338,-9.979253768920898,-11.702075004577637,11.354188919067383,22.753582000732422,-0.16782920062541962,-1.7152018547058105,-13.606145858764648,2.8134806156158447,19.80970001220703,0.8154899477958679,8.562178611755371,-11.810315132141113,5.746310234069824,-12.39228630065918,7.7331085205078125,21.289846420288086,14.111175537109375,8.754596710205078,22.970849990844727,-0.4292070269584656,13.032544136047363,-13.472942352294922,16.79200553894043,-14.086432456970215,4.222893714904785,19.284534454345703,-3.001634359359741,12.419776916503906,13.25800609588623,2.3527307510375977,1.9019029140472412,-3.678230047225952,-13.246195793151855,-1.0146194696426392,-2.3824360370635986,-8.658845901489258,-7.032977104187012,-24.06922149658203,-8.566889762878418,21.868724822998047,6.726659297943115,-7.193387031555176,11.843376159667969,5.586434841156006,4.817849159240723,10.534738540649414,14.69034194946289,-7.604460716247559,22.635746002197266,10.536055564880371,9.929123878479004,-11.155237197875977,-11.703230857849121,-2.4118542671203613,19.944091796875,6.708580493927002,6.700311183929443,0.04983266070485115,-3.0368313789367676,-6.539486408233643,-13.195510864257812,-12.830604553222656,11.565191268920898,-4.185317516326904,22.1092472076416,9.096640586853027,-15.204891204833984,-1.9167444705963135,15.679112434387207,-10.15832805633545,3.248204231262207,6.208036422729492,11.249185562133789,0.7131502032279968,-1.4225894212722778,-15.459264755249023,-3.120950698852539,-6.591434478759766,-8.095684051513672,-15.4867582321167,20.309322357177734,13.60807991027832,-6.318173885345459,-13.105290412902832,13.231039047241211,19.300764083862305,21.947420120239258,5.673885822296143,20.533754348754883,-13.349370002746582,-13.61146068572998,-10.559748649597168,-3.027920961380005,-7.554235935211182,-3.923985719680786,-3.0225400924682617,5.8236002922058105,17.647674560546875,-9.901838302612305,-11.771416664123535,13.665940284729004,5.222020149230957,-11.236838340759277,-3.7596113681793213,12.709026336669922,-12.025718688964844,21.069427490234375,-10.50370979309082,-10.657649993896484,21.641464233398438,1.6712391376495361,-0.9097478985786438,-3.260537624359131,-11.571418762207031,-0.29467248916625977,-5.447942733764648,-1.8457261323928833,-15.031452178955078,-10.040432929992676,19.426849365234375,20.51652717590332,20.44945526123047,6.409351825714111,11.093159675598145,13.009805679321289,19.485313415527344,21.110946655273438,3.691096305847168,12.373003005981445,-3.7094037532806396,-3.199695348739624,-13.273921012878418,-1.0392038822174072,0.3902454376220703,-11.270880699157715,-1.8870950937271118,12.91628360748291,8.852095603942871,19.564783096313477,-3.1600587368011475,21.781858444213867,6.505730628967285,-10.416471481323242,0.4049997627735138,-14.09254264831543,12.045136451721191,6.934656143188477,-12.979366302490234,6.175320625305176,1.3045463562011719,-14.240691184997559,5.553197860717773,17.68959617614746,19.706897735595703,12.186582565307617,-13.544921875,20.26546859741211,9.7880220413208,-7.142251491546631,19.88860321044922,-9.552241325378418,-9.149130821228027,-9.56777286529541,-12.708986282348633,16.257532119750977,-13.671602249145508,-9.855566024780273,9.367904663085938,10.562403678894043,-3.6885592937469482,-4.514846324920654,-8.211039543151855,9.718523979187012,-12.439446449279785,-12.923904418945312,18.584819793701172,-9.445816040039062,-1.1201952695846558,-3.9806575775146484,-0.070972740650177,19.917434692382812,0.14666394889354706,6.781879901885986,-12.822694778442383,-12.425874710083008,-4.250441551208496,16.437952041625977,-1.5441762208938599,-4.089081287384033,-10.714752197265625,-8.91928768157959,9.79450511932373,-15.943302154541016,-10.454365730285645,-7.859012603759766,13.752490997314453,3.4235849380493164,-15.000720977783203,-3.412527561187744,-8.098999977111816,18.99851417541504,-8.691821098327637,7.996344566345215,21.950288772583008,-14.767765998840332,11.408934593200684,-5.213651657104492,1.4260708093643188,21.470365524291992,14.951095581054688,-12.24402141571045,-7.015033721923828,8.507949829101562,-7.877979278564453,18.845287322998047,-2.154162883758545,18.852340698242188,-14.465312004089355,-9.75643253326416,8.504860877990723,-8.855653762817383,-4.458803176879883,-0.6624255180358887,11.060013771057129,10.707582473754883,5.741785049438477,16.042606353759766,-1.7061389684677124,-9.495543479919434,2.2007057666778564,7.472551345825195,20.199684143066406,-14.582433700561523,12.69171142578125,-12.386262893676758,6.111865997314453,-12.566976547241211,11.022858619689941,-5.936446189880371,-11.072701454162598,-8.972908973693848,-5.787076950073242,4.187258243560791,-15.215585708618164,-8.622432708740234,-7.389285087585449,3.638347625732422,0.9716703295707703,-2.1141395568847656,-2.095242977142334,3.8894360065460205,-15.03497314453125,2.722926616668701,-3.2231972217559814,-3.7919762134552,2.727621078491211,3.477825164794922,-14.04096508026123,-12.20919132232666,12.379068374633789,-4.13271951675415,-0.41529518365859985,0.9902463555335999,-9.533370971679688,0.5361929535865784,-8.820075988769531,4.0498270988464355,1.2355486154556274,-13.780285835266113,7.01854133605957,17.440868377685547,15.958868980407715,10.423588752746582,-7.535502910614014,-6.5024261474609375,4.3581862449646,-7.04500675201416,10.56771183013916,17.28500747680664,-0.6550417542457581,-11.611498832702637,4.544797897338867,15.510427474975586,-2.152574300765991,-4.649944305419922,-10.147054672241211,-11.494463920593262,-10.281111717224121,9.21230411529541,16.752363204956055,8.746803283691406,-12.555981636047363,-0.6198359131813049,15.767350196838379,12.624340057373047,14.623564720153809,16.139617919921875,3.1327364444732666,1.3103952407836914,14.892308235168457,-2.0089643001556396,4.709972381591797,10.025113105773926,-11.327698707580566,-1.2168877124786377,11.760139465332031,4.004190921783447,-7.407936096191406,15.371138572692871,5.747328758239746,8.2091064453125,0.4629446864128113,-10.019865989685059,-10.160176277160645,-5.9614434242248535,-9.90102481842041,0.7639689445495605,-2.828062057495117,-2.222485303878784,-8.578771591186523,11.25007438659668,2.2354538440704346,7.104702949523926,-14.279473304748535,-9.337145805358887,5.78794002532959,-13.530756950378418,-9.342924118041992,23.769590377807617,5.518649578094482,-1.8025703430175781,-0.5798506736755371,-4.608101844787598,22.443078994750977,9.801407814025879,-5.235445499420166,-9.354181289672852,8.613112449645996,11.955693244934082,-13.254037857055664,-13.163729667663574,-8.148402214050293,-14.867571830749512,-11.045458793640137,5.794498920440674,23.163074493408203,-8.66607666015625,-0.3506487011909485,7.682983875274658,6.784411430358887,1.4338794946670532,2.211148738861084,-12.303529739379883,15.561951637268066,-2.095614194869995,3.185767412185669,-2.83915638923645,6.806853771209717,-12.234434127807617,-4.979175090789795,-9.7022705078125,6.627697467803955,5.900920391082764,-11.157968521118164,10.088171005249023,1.5225305557250977,16.53561782836914,-4.492843151092529,-4.753986835479736,-0.2943904995918274,-13.047304153442383,-11.0203275680542,-11.10206413269043,7.863982200622559,-13.210606575012207,-7.823403358459473,0.9611669778823853,-2.1842427253723145,-9.043630599975586,-7.400576114654541,10.852702140808105,3.9336636066436768,6.467677116394043,-0.306613028049469,0.1282801628112793,-9.114396095275879,13.215023040771484,1.9088857173919678,8.460549354553223,-7.709114074707031,-14.531729698181152,-15.213926315307617,-12.201081275939941,9.92530632019043,12.308694839477539,0.8676149845123291,9.476856231689453,-7.720924377441406,-15.124723434448242,1.466719627380371,-5.133219242095947,-12.362922668457031,-10.155989646911621,7.302314281463623,-0.35568395256996155,-9.757529258728027,15.374357223510742,-6.654239654541016,1.428045392036438,-10.234034538269043,-10.27267074584961,-8.25692367553711,-11.140127182006836,-2.1122324466705322,2.6727418899536133,-2.056328535079956,-5.350165367126465,5.350173473358154,5.093319892883301,-4.0862507820129395,14.72596263885498,1.7720457315444946,-12.525569915771484,-12.603492736816406,10.981529235839844,-11.091752052307129,5.029914855957031,1.2831571102142334,6.502098083496094,2.117108106613159,-0.6844358444213867,6.613653659820557,4.038082122802734,-2.79156756401062,14.855130195617676,-11.717971801757812,3.681328058242798,-9.833528518676758,-13.920151710510254,8.730104446411133,-0.8094528913497925,-14.0264310836792,5.774425029754639,-5.056284427642822,18.666147232055664,8.42209529876709,-6.029660701751709,-5.42292594909668,11.821359634399414,-8.400716781616211,-5.6645917892456055,-7.706665515899658,4.476292133331299,14.513182640075684,-10.517315864562988,-2.0023090839385986,-12.385029792785645,3.4169833660125732,12.715869903564453,17.287670135498047,-8.79357624053955,-9.269026756286621,-10.765015602111816,14.279064178466797,-7.914799690246582,-11.568748474121094,2.204512357711792,-3.594696521759033,-2.5926930904388428,-11.144524574279785,-10.020134925842285,7.470849514007568,11.346168518066406,9.921348571777344,-13.775146484375,-8.700421333312988,-11.746769905090332,10.825735092163086,6.409399032592773,-1.4324039220809937,12.53837776184082,16.08773422241211,-12.753643035888672,3.410344362258911,16.978107452392578,-11.591595649719238,0.07746360450983047,8.783525466918945,-3.7180349826812744,16.642257690429688,-14.071367263793945,-5.273037433624268,12.57420539855957,12.666089057922363,11.985808372497559,6.081632137298584,-14.068202018737793,-15.802008628845215,9.20288372039795,-1.9858630895614624,13.16842269897461,13.044504165649414,-7.124770641326904,-6.003930568695068,-10.858763694763184,4.191589832305908,4.529048919677734,-2.0007872581481934,13.260257720947266],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scattermap\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermap\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bba8e0ff-727f-49cf-9836-7f1944c64d3a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
        "from sklearn.manifold import TSNE                   # final reduction\n",
        "import numpy as np                                  # array handling\n",
        "\n",
        "\n",
        "def reduce_dimensions(model):\n",
        "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
        "\n",
        "    # extract the words & their vectors, as numpy arrays\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
        "\n",
        "    # reduce using t-SNE\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    x_vals = [v[0] for v in vectors]\n",
        "    y_vals = [v[1] for v in vectors]\n",
        "    return x_vals, y_vals, labels\n",
        "\n",
        "\n",
        "x_vals, y_vals, labels = reduce_dimensions(model)\n",
        "\n",
        "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
        "    from plotly.offline import init_notebook_mode, iplot, plot\n",
        "    import plotly.graph_objs as go\n",
        "\n",
        "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
        "    data = [trace]\n",
        "\n",
        "    if plot_in_notebook:\n",
        "        init_notebook_mode(connected=True)\n",
        "        iplot(data, filename='word-embedding-plot')\n",
        "    else:\n",
        "        plot(data, filename='word-embedding-plot.html')\n",
        "\n",
        "\n",
        "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import random\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.scatter(x_vals, y_vals)\n",
        "\n",
        "    #\n",
        "    # Label randomly subsampled 25 data points\n",
        "    #\n",
        "    indices = list(range(len(labels)))\n",
        "    selected_indices = random.sample(indices, 25)\n",
        "    for i in selected_indices:\n",
        "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
        "\n",
        "try:\n",
        "    get_ipython()\n",
        "except Exception:\n",
        "    plot_function = plot_with_matplotlib\n",
        "else:\n",
        "    plot_function = plot_with_plotly\n",
        "\n",
        "plot_function(x_vals, y_vals, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n",
        "----------\n",
        "\n",
        "In this tutorial we learned how to train word2vec models on your custom data\n",
        "and also how to evaluate it. Hope that you too will find this popular tool\n",
        "useful in your Machine Learning tasks!\n",
        "\n",
        "Links\n",
        "-----\n",
        "\n",
        "- API docs: :py:mod:`gensim.models.word2vec`\n",
        "- `Original C toolkit and word2vec papers by Google <https://code.google.com/archive/p/word2vec/>`_.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 311",
      "language": "python",
      "name": "env311"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
