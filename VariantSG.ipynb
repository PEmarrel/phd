{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccaf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataSet import SGNS_store_DataSet\n",
    "\n",
    "from typing import Sequence, Optional, Callable, List, Dict\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "from visuEmbedding import components_to_fig_3D, components_to_fig_3D_animation\n",
    "import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size:int, embedding_dimension:int=15, context_dimension:int|None=None, init_range:float|None=None, sparse:bool=True, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.emb_size:int = emb_size\n",
    "        self.emb_dim:int = embedding_dimension\n",
    "        self.word_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.emb_dim, device=device, sparse=sparse)\n",
    "        self.con_size = embedding_dimension if context_dimension is None else context_dimension\n",
    "        self.con_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.con_size, device=device,sparse=sparse)\n",
    "\n",
    "        if init_range is None:\n",
    "            init_range = 0.5 / self.emb_dim\n",
    "        self.word_emb.weight.data.uniform_(-init_range, init_range)\n",
    "        self.con_emb.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, centrals_words:list|torch.Tensor, pos_context:list|torch.Tensor, neg_context:list|torch.Tensor):\n",
    "        words_emb:torch.Tensor = self.word_emb(centrals_words)\n",
    "        context_emb:torch.Tensor = self.con_emb(pos_context) # [B, D]\n",
    "        neg_emb:torch.Tensor = self.con_emb(neg_context) # [B, K, D]\n",
    "\n",
    "        pos_score = torch.sum(words_emb * context_emb, dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        neg_score = torch.bmm(neg_emb, words_emb.unsqueeze(-1)).squeeze(2)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "\n",
    "        loss = - (pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "    \n",
    "    def save_weight(self, path:str=\"SGNS_weights/\"):\n",
    "        word_weights = self.word_emb.weight.detach().cpu()\n",
    "        con_weight = self.con_emb.weight.detach().cpu()\n",
    "        torch.save(word_weights, path+'word_embedding.pt')\n",
    "        torch.save(con_weight, path+'con_embedding.pt')\n",
    "\n",
    "    def load_weight(self, path:str=\"SGNS_weights/\", name_word_weights:str=\"word_embedding.pt\", name_con_weights:str=\"con_embedding.pt\"):\n",
    "        word_weights = torch.load(path + name_word_weights)\n",
    "        con_weight = torch.load(path + name_con_weights)\n",
    "        self.word_emb:nn.Embedding = nn.Embedding.from_pretrained(word_weights)\n",
    "        self.con_emb:nn.Embedding = nn.Embedding.from_pretrained(con_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92fd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlyOneEmb(nn.Module):\n",
    "    def __init__(self, emb_size:int, embedding_dimension:int=15, context_dimension:int|None=None, init_range:float|None=None, sparse:bool=True, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.emb_size:int = emb_size\n",
    "        self.emb_dim:int = embedding_dimension\n",
    "        self.word_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.emb_dim, device=device, sparse=sparse)\n",
    "\n",
    "        if init_range is None:\n",
    "            init_range = 0.5 / self.emb_dim\n",
    "        self.word_emb.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, centrals_words:list|torch.Tensor, pos_context:list|torch.Tensor, neg_context:list|torch.Tensor):\n",
    "        words_emb:torch.Tensor = self.word_emb(centrals_words)\n",
    "        context_emb:torch.Tensor = self.word_emb(pos_context) # [B, D]\n",
    "        neg_emb:torch.Tensor = self.word_emb(neg_context) # [B, K, D]\n",
    "\n",
    "        pos_score = torch.sum(words_emb * context_emb, dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        neg_score = torch.bmm(neg_emb, words_emb.unsqueeze(-1)).squeeze(2)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "    \n",
    "    def save_weight(self, path:str=\"SGNS_weights/\"):\n",
    "        word_weights = self.word_emb.weight.detach().cpu()\n",
    "        torch.save(word_weights, path+'word_embedding.pt')\n",
    "\n",
    "    def load_weight(self, path:str=\"SGNS_weights/\", name_word_weights:str=\"word_embedding.pt\", name_con_weights:str=\"con_embedding.pt\"):\n",
    "        word_weights = torch.load(path + name_word_weights)\n",
    "        self.word_emb:nn.Embedding = nn.Embedding.from_pretrained(word_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Word2Vec(modelW2V:nn.Module, dataLoader:Dataset, optimizer:optim.Optimizer, epochs:int, verbal:bool=True, log_interval=100, device=\"cpu\"):\n",
    "    \"\"\"Fonction d’entraînement pour un modèle Word2Vec\n",
    "    \"\"\"\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        batches = 0\n",
    "        loss_history = []\n",
    "        global_step = 0\n",
    "        \n",
    "        modelW2V.train()\n",
    "\n",
    "        for batch in dataLoader:\n",
    "            # centers: [B], pos: [B], negs: [B, K]\n",
    "            centers, pos, negs = batch\n",
    "            centers = centers.to(device)\n",
    "            pos = pos.to(device)\n",
    "            negs = negs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = modelW2V(centers, pos, negs)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            loss_history.append(batch_loss)\n",
    "            batches += 1\n",
    "            global_step += 1\n",
    "\n",
    "            if verbal and log_interval and (global_step % log_interval == 0):\n",
    "                print(f\"Epoch {epoch} Step {global_step} AvgLoss {epoch_loss / batches:.6f}\")\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / max(1, batches)\n",
    "        if verbal : print(f\"Epoch {epoch} finished. Avg loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "    return {\"loss_history\": loss_history, \"final_epoch_loss\": avg_epoch_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGwithNorm(nn.Module):\n",
    "    def __init__(self, emb_size:int, embedding_dimension:int=15, context_dimension:int|None=None, init_range:float|None=None, sparse:bool=True, device=\"cpu\"):\n",
    "        \"\"\"Initialisation du modèle SkipGram\n",
    "        Args:\n",
    "            emb_size: La taille de l'embedding, ce nombre devrais être déterminé après le process sur les data, et dépend de la taille de la fenêtre glissante.\n",
    "            embedding_dimension: La taille souhaité de l'embedding. Pour notre cas d'utilisation nous préférons une taille très petit\n",
    "            context_dimension: Il n'est pas recommandé de mettre un entier mais de laisser a None.\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emb_size:int = emb_size\n",
    "        self.emb_dim:int = embedding_dimension\n",
    "        self.word_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.emb_dim, device=device, sparse=sparse)\n",
    "\n",
    "        self.con_size = embedding_dimension if context_dimension is None else context_dimension\n",
    "        self.con_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.con_size, device=device,sparse=sparse)\n",
    "\n",
    "        if init_range is None:\n",
    "            init_range = 0.5 / self.emb_dim\n",
    "        self.word_emb.weight.data.uniform_(-init_range, init_range)\n",
    "        self.con_emb.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "        self.scale = nn.Parameter(torch.tensor(10.0, device=device))\n",
    "\n",
    "    def forward(self, centrals_words:list|torch.Tensor, pos_context:list|torch.Tensor, neg_context:list|torch.Tensor):\n",
    "        \"\"\"Fonction du forward pour le modèle SkipGramModel\n",
    "        Args:\n",
    "            centrals_words: Liste des ids des tokens des mots centraux [B]\n",
    "            pos_context: Liste des ids des tokens des mots dans le contexte [B]\n",
    "            neg_context: Liste des ids des tokens des mots non présent dans le contexte [B, K]\n",
    "        \"\"\"\n",
    "        words_emb:torch.Tensor = self.word_emb(centrals_words) # [B, D]\n",
    "        context_emb:torch.Tensor = self.con_emb(pos_context)   # [B, D]\n",
    "        neg_emb:torch.Tensor = self.con_emb(neg_context)       # [B, K, D]\n",
    "        words_norm = F.normalize(words_emb, p=2, dim=1)\n",
    "        context_norm = F.normalize(context_emb, p=2, dim=1)\n",
    "        neg_norm = F.normalize(neg_emb, p=2, dim=2)\n",
    "\n",
    "\n",
    "        pos_dot = torch.sum(words_norm * context_norm, dim=1)\n",
    "        pos_score = pos_dot * self.scale # Scale up\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        neg_dot = torch.bmm(neg_norm, words_norm.unsqueeze(-1)).squeeze(2)\n",
    "        neg_score = neg_dot * self.scale # Scale up\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "\n",
    "\n",
    "        loss = - (pos_loss + neg_loss).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_data(\n",
    "    language: str,\n",
    "    dataseteur: Callable[..., object],\n",
    "    window_size: int = 3,\n",
    "    nb_neg: int = 3,\n",
    "    subsample_thresh: float = 1.0,\n",
    "    vocab_size_limit: Optional[int] = None,\n",
    "    power:float =0.75,\n",
    "    file: Optional[str] = None,\n",
    "    files: Optional[list[str]] = None,\n",
    "    sentences: Optional[List[List[str]]] = None,\n",
    "    remove_accent: bool = True,\n",
    "    remove_ponct: bool = True,\n",
    "    keep_accent: bool = True,\n",
    "    contraction_map: Optional[dict] = None,\n",
    "    stop_words:List[str] = []\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if contraction_map is None:\n",
    "        contraction_map = {\n",
    "            \"n't\": \" n't\", \"'re\": \" 're\", \"'ve\": \" 've\", \"'ll\": \" 'll\",\n",
    "            \"'d\": \" 'd\", \"'s\": \" 's\", \"'m\": \" 'm\"\n",
    "        }\n",
    "\n",
    "    if all([(files is None), (file is None), (sentences is None)],):\n",
    "        raise AssertionError(\"One of files, file or sentence must not be None\")\n",
    "\n",
    "    if language not in {\"english\", \"french\", None}:\n",
    "        raise ValueError(\"language must be 'english' or 'french' or None\")\n",
    "\n",
    "    def remove_accents(text: str) -> str:\n",
    "        nk = unicodedata.normalize(\"NFKD\", text)\n",
    "        return \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "\n",
    "    keep = {\"'\", \"’\"} if keep_accent else set()\n",
    "    base_punct = set(string.punctuation)\n",
    "    extra_punct = set('“”‘’—–…«»')\n",
    "    punct_to_remove = (base_punct | extra_punct) - keep\n",
    "    TRANSL_TABLE = str.maketrans('', '', ''.join(sorted(punct_to_remove)))\n",
    "\n",
    "    tokens_by_sentence: List[List[str]] = []\n",
    "\n",
    "    if files is not None:\n",
    "        for name_file in files:\n",
    "            with open(name_file, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    s = line.strip().lower()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    if remove_accent:\n",
    "                        s = remove_accents(s)\n",
    "                    for k, v in contraction_map.items():\n",
    "                        s = s.replace(k, v)\n",
    "                    if remove_ponct:\n",
    "                        s = s.translate(TRANSL_TABLE)\n",
    "                    s2 = [word for word in s.split() if word not in stop_words]\n",
    "                    s = \" \".join(s2)\n",
    "                    toks = word_tokenize(s, language=language)\n",
    "                    if toks:\n",
    "                        tokens_by_sentence.append(toks)\n",
    "\n",
    "    elif file is not None:\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip().lower()\n",
    "                if not s:\n",
    "                    continue\n",
    "                if remove_accent:\n",
    "                    s = remove_accents(s)\n",
    "                for k, v in contraction_map.items():\n",
    "                    s = s.replace(k, v)\n",
    "                if remove_ponct:\n",
    "                    s = s.translate(TRANSL_TABLE)\n",
    "                s2 = [word for word in s.split() if word not in stop_words]\n",
    "                s = \" \".join(s2)                \n",
    "                if not s:\n",
    "                    continue\n",
    "                toks = word_tokenize(s, language=language)\n",
    "                if toks:\n",
    "                    tokens_by_sentence.append(toks)\n",
    "    else:\n",
    "        tokens_by_sentence = [list(s) for s in sentences if s]\n",
    "\n",
    "    return dataseteur(\n",
    "        sentences=tokens_by_sentence,\n",
    "        window_size=window_size,\n",
    "        nb_neg=nb_neg,\n",
    "        subsample_thresh=subsample_thresh,\n",
    "        power=power,\n",
    "        vocab_size_limit=vocab_size_limit\n",
    "    )\n",
    "\n",
    "def cosine_similarity_matrix(embeddings:nn.Embedding) -> torch.Tensor:\n",
    "    emb = embeddings.weight.detach()\n",
    "    emb_norm = F.normalize(emb, p=2, dim=1)\n",
    "    similarity_matrix = emb_norm @ emb_norm.t()\n",
    "    return similarity_matrix\n",
    "\n",
    "def update_sim_history(words: list[str], idx: List[int], cos_sim_history:Dict, similarity_matrix):\n",
    "    num_words = len(words)\n",
    "\n",
    "    for i in range(num_words):\n",
    "        for j in range(num_words):\n",
    "            similarity = ((similarity_matrix[idx[i], idx[j]] + 1) / 2) * 100\n",
    "            cos_sim_history[words[i]][words[j]].append(round(float(similarity), 2))\n",
    "\n",
    "def heat_map(words:List[str], similarity_matrix, figsize=(10, 8), save_file='tmp.png'):\n",
    "    plt.close('all')\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt=\".2f\", cmap=\"magma\",\n",
    "                xticklabels=words, yticklabels=words, cbar=True, robust=False,\n",
    "                vmin=0, vmax=100,\n",
    "                square=False, linewidths=0.)\n",
    "\n",
    "    plt.title(\"Matrix de Similarité Cosinus\")\n",
    "    plt.xlabel(\"Mots\", fontstyle=\"italic\")\n",
    "    plt.ylabel(\"Mots\", fontstyle=\"italic\")\n",
    "    plt.savefig(save_file)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d56788",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset:SGNS_store_DataSet = pipe_data(\n",
    "    language=\"french\",\n",
    "    dataseteur=SGNS_store_DataSet,\n",
    "    window_size = 3,\n",
    "    nb_neg=5,\n",
    "    subsample_thresh= 1,\n",
    "    vocab_size_limit=None,\n",
    "    file=\"data/GPT5v2.txt\",\n",
    "    remove_accent=True,\n",
    "    remove_ponct=True,\n",
    "    keep_accent= False,\n",
    "    contraction_map=None,\n",
    "    stop_words=[\"le\", \"les\", \"sur\", \"fait\", \"de\", \"et\", \"la\", \"des\", \"sont\"] + \\\n",
    "    [\"the\", \"your\", \"a\", \"rubber\"]\n",
    "\n",
    ")\n",
    "data = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "k = [\"chat\", \"chien\", \"animal\", \"train\", \"balle\", \"jouer\"]\n",
    "\n",
    "modelW2V:SkipGramModel = SkipGramModel(dataset.vocab_size, embedding_dimension=3, init_range=None, sparse=False)\n",
    "# optimizer = torch.optim.SparseAdam(modelW2V.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(modelW2V.parameters(), lr=0.01)\n",
    "\n",
    "nb_epoch = 30\n",
    "for _ in range(nb_epoch):\n",
    "    for sentence_nb, (centers, pos, negs) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        loss = modelW2V(centers, pos, negs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "similarity = cosine_similarity_matrix(modelW2V.word_emb)\n",
    "m_to_h = similarity\n",
    "m_to_h = ((m_to_h + 1) / 2) * 100\n",
    "m_to_h_2 = m_to_h[dataset.encode(k),:]\n",
    "m_to_h_2 = m_to_h_2[:, dataset.encode(k)]\n",
    "plt = heat_map(words=k, similarity_matrix=m_to_h_2)\n",
    "plt.show()\n",
    "\n",
    "components_to_fig_3D(components=modelW2V.word_emb.weight.detach().cpu().numpy(),\n",
    "    encoder=dataset.encoder,\n",
    "    words_display=list(dataset.encoder.keys()),\n",
    "    highlight_words=k,\n",
    "    nb_neighbors=2, _min=-5, _max=5, base_color={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V:OnlyOneEmb = OnlyOneEmb(dataset.vocab_size, embedding_dimension=3, init_range=None, sparse=True)\n",
    "# optimizer = torch.optim.Adam(modelW2V.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SparseAdam(modelW2V.parameters(), lr=0.005)\n",
    "\n",
    "emb_hist = []\n",
    "nb_epoch = 5\n",
    "for _ in range(nb_epoch):\n",
    "    for sentence_nb, (centers, pos, negs) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        loss = modelW2V(centers, pos, negs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    w = deepcopy(modelW2V.word_emb.weight.detach().cpu().numpy())\n",
    "    emb_hist.append(w)\n",
    "\n",
    "similarity = cosine_similarity_matrix(modelW2V.word_emb)\n",
    "m_to_h = similarity\n",
    "m_to_h = ((m_to_h + 1) / 2) * 100\n",
    "m_to_h_2 = m_to_h[dataset.encode(k),:]\n",
    "m_to_h_2 = m_to_h_2[:, dataset.encode(k)]\n",
    "plt = heat_map(words=k, similarity_matrix=m_to_h_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0964cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_colors = {\n",
    "    'chat': (\"blue\",  \"cyan\"),\n",
    "    'chien': (\"goldenrod\", \"yellow\"),\n",
    "    'balle': (\"green\", \"lightgreen\"),\n",
    "    \"jouer\": (\"magenta\", \"pink\")\n",
    "}\n",
    "\n",
    "\n",
    "fig = components_to_fig_3D_animation(\n",
    "    history_components=emb_hist,\n",
    "    encoder=dataset.encoder,\n",
    "    highlight_words=[\"chat\", \"chien\", \"balle\", \"jouer\"],\n",
    "    nb_neighbors=6, base_color=base_colors\n",
    ")\n",
    "\n",
    "\n",
    "modelW2V.save_weight()\n",
    "tool.DicToJson(dataset.encoder, \"data/encoder\")\n",
    "tool.DicToJson(dataset.decoder, \"data/decoder\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
