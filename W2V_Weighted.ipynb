{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ad063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cosine_similarity\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "\n",
    "from dataSet import SGNS_store_DataSet\n",
    "\n",
    "from typing import Sequence, Optional, Callable, List, Dict, Set\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "from visuEmbedding import components_to_fig_3D, components_to_fig_3D_animation\n",
    "import tool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text: str) -> str:\n",
    "    \"\"\"Normalizes text to remove accents (e.g., 'café' -> 'cafe').\"\"\"\n",
    "    nk = unicodedata.normalize(\"NFKD\", text)\n",
    "    return \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "\n",
    "def prepare_data(\n",
    "    file_path: str,\n",
    "    language: str,\n",
    "    remove_accent: bool = True,\n",
    "    remove_punct: bool = True,\n",
    "    keep_apostrophes: bool = True,\n",
    "    contraction_map: Optional[Dict[str, str]] = None,\n",
    "    stop_words: Optional[List[str]] = None,\n",
    "    break_line: bool = True,\n",
    "    expand_is_contraction: bool = True\n",
    "    ) -> List[List[str]]:\n",
    "\n",
    "    sentence_split_re = re.compile(r'[\\.!\\?]+')\n",
    "    \n",
    "    contraction_re = None\n",
    "    if contraction_map:\n",
    "        pattern = \"|\".join(re.escape(k) for k in sorted(contraction_map.keys(), reverse=True))\n",
    "        contraction_re = re.compile(f\"({pattern})\")\n",
    "\n",
    "    punctuation_chars = set(string.punctuation)\n",
    "    if keep_apostrophes or expand_is_contraction:\n",
    "        punctuation_chars -= {\"'\", \"’\"}\n",
    "    \n",
    "    punct_trans_table = str.maketrans({c: \" \" for c in punctuation_chars})\n",
    "    stop_words_set: Set[str] = set(stop_words) if stop_words else set()\n",
    "    tokens_by_sentence: List[List[str]] = []\n",
    "    \n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            sub_lines = sentence_split_re.split(line.strip().lower()) if break_line else [line.strip().lower()]\n",
    "            \n",
    "            for s in sub_lines:\n",
    "                if not s: continue\n",
    "                \n",
    "                if contraction_re:\n",
    "                    s = contraction_re.sub(lambda m: contraction_map[m.group(0)], s)\n",
    "                    \n",
    "                s = s.replace(\"-\", \"\")\n",
    "                s = s.replace(\"—\", \" \")\n",
    "                \n",
    "                if remove_accent:\n",
    "                    s = remove_accents(s) \n",
    "\n",
    "                if remove_punct:\n",
    "                    s = s.translate(punct_trans_table)\n",
    "\n",
    "                toks = word_tokenize(s, language=language)\n",
    "\n",
    "                if expand_is_contraction and language == 'english':\n",
    "                    tagged = nltk.pos_tag(toks)\n",
    "                    new_toks = []\n",
    "                    for word, tag in tagged:\n",
    "                        if tag == 'POS': continue # Remove possession\n",
    "                        elif word in [\"'s\", \"’s\"] and tag == 'VBZ':\n",
    "                            new_toks.append(\"is\")\n",
    "                        else:\n",
    "                            new_toks.append(word)\n",
    "                    toks = new_toks\n",
    "\n",
    "                clean_toks = []\n",
    "                for t in toks:\n",
    "                    t_stripped = t.strip(\"'’\")\n",
    "                    if t_stripped and t_stripped not in stop_words_set:\n",
    "                        clean_toks.append(t_stripped)\n",
    "                \n",
    "                if clean_toks:\n",
    "                    tokens_by_sentence.append(clean_toks)\n",
    "\n",
    "    return tokens_by_sentence\n",
    "\n",
    "def separate_text_intonation(data:List[List[str]]):\n",
    "    texts = []\n",
    "    intonations = []\n",
    "    for sentence in data:\n",
    "        intonation = sentence[1::2]\n",
    "        text = sentence[::2]\n",
    "        if all(t.isalpha() for t in text) and all(t.isdigit() for t in intonation):\n",
    "            texts.append(text)\n",
    "            intonations.append(list(map(int, intonation)))\n",
    "        else:\n",
    "            print(\"Warning: Mismatched text and intonation in sentence:\", sentence)\n",
    "            print(\"Extracted text:\", text)\n",
    "            print(\"Extracted intonation:\", intonation)\n",
    "            for t in text:\n",
    "                if not t.isalpha():\n",
    "                    print(\" Non-alpha text token:\", t)\n",
    "            for i in intonation:\n",
    "                if not i.isdigit():\n",
    "                    print(\" Non-digit intonation token:\", i)\n",
    "            \n",
    "    return texts, intonations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data(\n",
    "    file_path=\"./data/GoodNightGorilla_Intonation.txt\",\n",
    "    language='english',\n",
    "    remove_accent=True,\n",
    "    remove_punct=True,\n",
    "    keep_apostrophes=False,\n",
    "    contraction_map={\n",
    "        \"that's\" : \"thatis\",\n",
    "        \"it's\" : \"itis\",\n",
    "        \"don't\": \"donot\",\n",
    "        \"doesn't\": \"doesnot\",},\n",
    "    stop_words=[\"s\", \"n't\"],\n",
    "    break_line=False\n",
    ")\n",
    "for s in data:\n",
    "    print(s)\n",
    "texts, intonations = separate_text_intonation(data)\n",
    "for t, i in zip(texts, intonations):\n",
    "    print(\"Text:\", t)\n",
    "    print(\"Intonation:\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First analyse of frequency of words\n",
    "word_counter = Counter()\n",
    "for sentence in texts:\n",
    "    word_counter.update(sentence)\n",
    "most_common_words = word_counter.most_common()\n",
    "print(\"Most common words:\", most_common_words)\n",
    "\n",
    "occ_m, word_list, word_to_index = tool.compute_co_occurrence_matrix(texts, window_size=2)\n",
    "ooc_df = pd.DataFrame(\n",
    "    data=occ_m,\n",
    "    index=word_list,\n",
    "    columns=word_list\n",
    ")\n",
    "\n",
    "bad_word, score_series = tool.get_parasite_word(ooc_df, percentile_threshold=95)\n",
    "print(f\"Identified parasite word: {bad_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_weighted_DataSet(Dataset):\n",
    "    def compute_importance(self, words, intonations):\n",
    "        dict_list_importance = {}\n",
    "        for sentence, intonation in zip(words, intonations) :\n",
    "            for index, inton in enumerate(intonation):\n",
    "                if sentence[index] not in dict_list_importance :\n",
    "                    dict_list_importance[sentence[index]] = [float(inton)]\n",
    "                else :\n",
    "                    dict_list_importance[sentence[index]].append(float(inton))\n",
    "\n",
    "        dict_importance = {}\n",
    "        for word in dict_list_importance :\n",
    "            print(f\"Pour le mots : {word} Somme des intonation : {sum(dict_list_importance[word])} \")\n",
    "            print(f\"Nombre de fois ou le mots apparaît{len(dict_list_importance[word])}\")\n",
    "            print(f\"résultat : { sum(dict_list_importance[word]) / len(dict_list_importance[word])}\")\n",
    "            dict_importance[word] = sum(dict_list_importance[word]) / len(dict_list_importance[word])\n",
    "\n",
    "        return dict_importance\n",
    "    \n",
    "    def _get_unigram_dist(self):\n",
    "        \"\"\"Compute unigram distribution depending on word importance\"\"\"\n",
    "        weight_list = [self.word_importance[token] for token in range(len(self.encoder))]\n",
    "        unigram = torch.tensor([weight for weight in weight_list], dtype=torch.float)\n",
    "        return unigram / unigram.sum()\n",
    "\n",
    "    \n",
    "    def _make_pairs_positif(self):\n",
    "        pairs = []\n",
    "        for sent, intonation in zip(self.sentences, self.intonations):\n",
    "            ids = self.encode(sent)\n",
    "            L = len(ids)\n",
    "            for i, center in enumerate(ids):\n",
    "                cur_window = self.context_size\n",
    "                start = max(0, i - cur_window)\n",
    "                end = min(L, i + cur_window + 1)\n",
    "                for j in range(start, end):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    context = ids[j]\n",
    "                    pairs.append((center, context, intonation[i]))\n",
    "        return pairs\n",
    "    \n",
    "    def __init__(self, sentences:list[list[str]], intonations:List[List[float]] , window_size:int=2, nb_neg:int=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(sentences) == len(intonations), f\"Error: Sentences and intonations must have the same length.\"\n",
    "\n",
    "        all_tokens = [t for sentence in sentences for t in sentence if t.isalpha()]\n",
    "        self.vocab = list(set(all_tokens))\n",
    "        self.encoder:dict = {w:i for i,w in enumerate(self.vocab)}\n",
    "        self.decoder:dict = {i:w for i,w in enumerate(self.vocab)}\n",
    "        self.context_size:int = window_size\n",
    "        self.sentences = sentences\n",
    "        self.intonations = intonations\n",
    "        self.K = nb_neg\n",
    "        \n",
    "        self.tokens = []\n",
    "        for s in sentences :\n",
    "            self.tokens.append([])\n",
    "            for w in s :\n",
    "                self.tokens[-1] .append(self.encoder[w])\n",
    "\n",
    "        self.word_importance:dict = self.compute_importance(self.tokens, intonations)\n",
    "        self.unigram_dist = self._get_unigram_dist()\n",
    "        self.pairs:List = self._make_pairs_positif()\n",
    "\n",
    "    def encode(self, words:list|str) -> list|int:\n",
    "        if isinstance(words, str) : return self.encoder[words]\n",
    "        ids = []\n",
    "        for w in words :\n",
    "            ids.append(self.encoder[w])\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids:list|int) -> list|int:\n",
    "        if isinstance(ids, int) : return self.decoder[ids]\n",
    "        words = []\n",
    "        for i in ids :\n",
    "            words.append(self.decoder[i])\n",
    "        return words\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        center, pos, intonation = self.pairs[idx]\n",
    "        neg = torch.multinomial(self.unigram_dist, self.K, replacement=True)\n",
    "        return center, pos, neg, intonation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intonation(intonations:List[List[int]], range_normalize:float=1.0, center_intonation:float=1.0) -> List[List[float]]:\n",
    "    all_intonations = [inton for sublist in intonations for inton in sublist]\n",
    "    min_inton = min(all_intonations) # Find the minimum intonation value\n",
    "    max_inton = max(all_intonations) # Find the maximum intonation value\n",
    "    assert max_inton > min_inton, \"Error: All intonation values are the same.\"\n",
    "\n",
    "    normalized_intonations = []\n",
    "    for sentence in intonations:\n",
    "        normalized_sentence = [\n",
    "            (inton - min_inton) / (max_inton - min_inton) for inton in sentence\n",
    "        ] # Normalize to [0, 1]\n",
    "        normalized_sentence = [\n",
    "            range_normalize * intonation + (center_intonation - range_normalize / 2)\n",
    "            for intonation in normalized_sentence\n",
    "        ]\n",
    "        normalized_intonations.append(normalized_sentence)\n",
    "\n",
    "    return normalized_intonations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_intonation([[1, 2, 3], [4, 5]], range_normalize=2.0, center_intonation=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intonations = normalize_intonation(intonations, range_normalize=0.8, center_intonation=1)\n",
    "\n",
    "print(intonations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c596f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = W2V_weighted_DataSet(sentences=texts, intonations=intonations)\n",
    "freq_weighted = test.word_importance\n",
    "\n",
    "rows = []\n",
    "for token, imp in freq_weighted.items():\n",
    "    word = test.decode(token)\n",
    "    freq = float(word_counter.get(word, 0))\n",
    "    parasite = float(score_series.get(word, 0.0))\n",
    "    dist = test.unigram_dist[token].item()\n",
    "    rows.append((int(token), word, float(imp), freq, parasite, dist))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=['token', 'word', 'importance_score', 'frequency', 'parasite_score', 'unigram_dist']).set_index('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1, w2, intonation in test.pairs:\n",
    "    center = test.decode(w1)\n",
    "    context = test.decode(w2)\n",
    "    print(f\"{center:<20}{context:<20}{str(intonation):<6}\")\n",
    "    \n",
    "print(test.unigram_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b303bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_without_0intonation = []\n",
    "intonation_without_0intonation = []\n",
    "\n",
    "for sentence_t, sentence_i in zip(texts, intonations):\n",
    "    text_without_0intonation.append([])\n",
    "    intonation_without_0intonation.append([])\n",
    "    for t, i in zip(sentence_t, sentence_i):\n",
    "        if int(i) != 0:\n",
    "            text_without_0intonation[-1].append(t)\n",
    "            intonation_without_0intonation[-1].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_m, word_list, word_to_index = tool.compute_co_occurrence_matrix(text_without_0intonation, window_size=2)\n",
    "ooc_df = pd.DataFrame(\n",
    "    data=occ_m,\n",
    "    index=word_list,\n",
    "    columns=word_list\n",
    ")\n",
    "\n",
    "test2 = W2V_weighted_DataSet(sentences=text_without_0intonation, intonations=intonation_without_0intonation)\n",
    "freq_weighted2 = test2.word_importance\n",
    "\n",
    "rows = []\n",
    "for token, imp in freq_weighted2.items():\n",
    "    word = test.decode(token)\n",
    "    parasite = float(score_series.get(word, 0.0))\n",
    "    rows.append((int(token), word, float(imp), parasite))\n",
    "\n",
    "df2 = pd.DataFrame(rows, columns=['token', 'word', 'importance_score', 'parasite_score']).set_index('token')\n",
    "\n",
    "for w1, w2, intonation in test2.pairs:\n",
    "    center = test2.decode(w1)\n",
    "    context = test2.decode(w2)\n",
    "    print(f\"{center:<20}{context:<20}{str(intonation):<6}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0914dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "data = test.word_importance\n",
    "words = list(data.keys())\n",
    "weights = list(data.values())\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) # Subtract max for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "words = list(data.keys())\n",
    "scores = np.array(list(data.values()))\n",
    "\n",
    "probabilities = softmax(scores)\n",
    "\n",
    "# Simulate 1,000 selections\n",
    "trials = 10000\n",
    "results = random.choices(words, weights=weights, k=trials)\n",
    "counts = Counter(results)\n",
    "\n",
    "print(f\"{'Word':<12} | {'Score':<8} | {'Frequency (out of 1000)':<25}\")\n",
    "print(\"-\" * 50)\n",
    "for word in words:\n",
    "    print(f\"{word:<12} | {data[word]:<8} | {counts[word]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10798d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = random.choices(words, weights=probabilities, k=trials)\n",
    "counts = Counter(results)\n",
    "\n",
    "print(f\"{'Word':<12} | {'Score':<8} | {'Frequency (out of 1000)':<25}\")\n",
    "print(\"-\" * 50)\n",
    "for word in words:\n",
    "    print(f\"{word:<12} | {data[word]:<8} | {counts[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = W2V_weighted_DataSet(sentences=texts, intonations=intonations,window_size=4 ,nb_neg=5)\n",
    "loader = DataLoader(data_set, batch_size=1, shuffle=False)\n",
    "for center, pos, neg, intonation in loader:\n",
    "    center = data_set.decode(center.tolist())\n",
    "    pos = data_set.decode(pos.tolist())\n",
    "    neg = data_set.decode(neg[0].tolist())\n",
    "    print(f\"Center: {center}, Positive: {pos}, Negatives: {neg}, Intonation: {intonation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlyOneEmb(nn.Module):\n",
    "    def __init__(self, emb_size:int, embedding_dimension:int=15, init_range:float|None=None, sparse:bool=True, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.emb_size:int = emb_size\n",
    "        self.emb_dim:int = embedding_dimension\n",
    "        self.word_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.emb_dim, device=device, sparse=sparse)\n",
    "\n",
    "        if init_range is None:\n",
    "            init_range = 0.5 / self.emb_dim\n",
    "        self.word_emb.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, centrals_words:list|torch.Tensor, pos_context:list|torch.Tensor, neg_context:list|torch.Tensor, weights:List|torch.Tensor):\n",
    "        words_emb:torch.Tensor = self.word_emb(centrals_words)\n",
    "        context_emb:torch.Tensor = self.word_emb(pos_context) # [B, D]\n",
    "        neg_emb:torch.Tensor = self.word_emb(neg_context) # [B, K, D]\n",
    "\n",
    "        pos_score = torch.sum(words_emb * context_emb, dim=1)\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        neg_score = torch.bmm(neg_emb, words_emb.unsqueeze(-1)).squeeze(2)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "        loss = -((pos_loss + neg_loss) * weights).mean()\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = W2V_weighted_DataSet(sentences=text_without_0intonation, intonations=intonation_without_0intonation, window_size=4, nb_neg=5)\n",
    "loader = DataLoader(data_set, batch_size=1, shuffle=False)\n",
    "# for center, pos, neg, intonation in loader:\n",
    "#     center = data_set.decode(center.tolist())\n",
    "#     pos = data_set.decode(pos.tolist())\n",
    "#     neg = data_set.decode(neg[0].tolist())\n",
    "#     print(f\"Center: {center}, Positive: {pos}, Negatives: {neg}, Intonation: {intonation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4968c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V:OnlyOneEmb = OnlyOneEmb(len(data_set.encoder.values()), embedding_dimension=3)\n",
    "optimizer = torch.optim.SparseAdam(modelW2V.parameters(), lr=0.005)\n",
    "\n",
    "nb_epoch = 5\n",
    "\n",
    "for _ in range(nb_epoch):\n",
    "\tfor sentence_nb, (centers, pos, negs, intonation) in enumerate(loader):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss = modelW2V(centers, pos, negs, intonation)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    " \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(vector_word:torch.Tensor, tensor:torch.Tensor, top_n:int=5):\n",
    "    all_scores = cosine_similarity(tensor, vector_word.reshape(1, -1))\n",
    "    score_series = pd.Series(all_scores.flatten())\n",
    "    top_words = score_series.sort_values(ascending=False).head(top_n)\n",
    "    return top_words\n",
    "\n",
    "def cosine_similarity_matrix(embeddings:nn.Embedding) -> torch.Tensor:\n",
    "    emb = embeddings.weight.detach()\n",
    "    emb_norm = F.normalize(emb, p=2, dim=1)\n",
    "    similarity_matrix = emb_norm @ emb_norm.t()\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77573abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"banana\"\n",
    "matrix_of_similarity = cosine_similarity_matrix(modelW2V.word_emb)\n",
    "nearest_neighbors = find_nearest_neighbors(matrix_of_similarity[data_set.encode(word_a)], matrix_of_similarity,\n",
    "                                            top_n=20)\n",
    "nearest_neighbors = nearest_neighbors.rename(index=lambda x: data_set.decoder[x])\n",
    "print(f\"Nearest Neighbors to '{word_a}':\")\n",
    "print(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8af7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
