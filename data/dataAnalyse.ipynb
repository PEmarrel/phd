{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6956251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "from typing import List\n",
    "\n",
    "import math\n",
    "import networkx as nx\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from pipData import *\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_to_remove = {w:[] for w in [1, 2, 3]}\n",
    "print(dct_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_cooccurrences(corpus:List[List[str]], target_words:List[str], window_size=2):\n",
    "    \"\"\"\n",
    "    Get real concurrence present in corpus.\n",
    "    Returns:\n",
    "        list: Frequencies co-occurring words, for each target_word\n",
    "    \"\"\"\n",
    "    co_occurring_tmp = {w:[] for w in target_words}\n",
    "    co_occurring = {w:[] for w in target_words}\n",
    "\n",
    "    for sentence in corpus:\n",
    "        for i, token in enumerate(sentence):\n",
    "            if token in target_words:\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(sentence), i + window_size + 1)\n",
    "                context = sentence[start:i] + sentence[i+1:end]\n",
    "                co_occurring_tmp[token].extend(context)\n",
    "\n",
    "    for w in target_words:\n",
    "        co_occurring[w] = Counter(co_occurring_tmp[w])\n",
    "    return co_occurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ac06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_must_frequent_word(corpus):\n",
    "    \"\"\"\n",
    "    Find must frequent word (stop word for a corpus)\n",
    "    \"\"\"\n",
    "    all_words_iterator = chain.from_iterable(corpus)\n",
    "    word_counts = Counter(all_words_iterator)\n",
    "    total_word_count = sum(word_counts.values())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for rank, (word, count) in enumerate(word_counts.items(), 1):\n",
    "        percentage = (count / total_word_count) * 100\n",
    "        results.append((word, count, round(percentage, 4)))\n",
    "        print(f\"{rank:<5} | {word:<15} | {count:<10} | {percentage:.2f}%\")\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=1):\n",
    "    all_tokens = list(chain(*corpus))\n",
    "    word_list = sorted(list(set(all_tokens)))\n",
    "    word_to_index = {word: i for i, word in enumerate(word_list)}\n",
    "    vocab_size = len(word_list)\n",
    "\n",
    "    co_occurrence_counts = defaultdict(int)\n",
    "\n",
    "    for sentence in corpus:\n",
    "        indices = [word_to_index[word] for word in sentence]\n",
    "        \n",
    "        for i, target_idx in enumerate(indices):\n",
    "            start_index = max(0, i - window_size)\n",
    "            end_index = min(len(indices), i + window_size + 1)\n",
    "\n",
    "            for j in range(start_index, end_index):\n",
    "                if i == j:\n",
    "                    co_occurrence_counts[(target_idx, target_idx)] += 1\n",
    "                    continue\n",
    "                \n",
    "                context_idx = indices[j]\n",
    "                \n",
    "                pair_key = tuple(sorted((target_idx, context_idx)))\n",
    "                co_occurrence_counts[pair_key] += 1\n",
    "\n",
    "    co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int16)\n",
    "\n",
    "    for (idx_a, idx_b), count in co_occurrence_counts.items():\n",
    "        co_occurrence_matrix[idx_a, idx_b] = count\n",
    "        if idx_a != idx_b:\n",
    "            co_occurrence_matrix[idx_b, idx_a] = count\n",
    "        \n",
    "    return co_occurrence_matrix, word_list, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0504a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Graph_from_cooccurrence(cooc_matrix:np.ndarray, word_list:list, threshold:int=5):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i, word_a in enumerate(word_list):\n",
    "        G.add_node(word_a)\n",
    "        for j, word_b in enumerate(word_list):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            weight = cooc_matrix[i, j]\n",
    "            if weight >= threshold:\n",
    "                G.add_edge(word_a, word_b, weight=weight)\n",
    "    return G\n",
    "\n",
    "def get_metrics_from_graph(graph_corpus:nx.Graph):\n",
    "    \"\"\"\n",
    "    Get graph metrics from co-occurrence graph\n",
    "    \"\"\"\n",
    "    # A. Global Connectivity (Largest Connected Component)\n",
    "    if len(graph_corpus) > 0:\n",
    "        largest_cc = max(nx.connected_components(graph_corpus), key=len)\n",
    "        lcc_coverage = len(largest_cc) / len(graph_corpus)\n",
    "    else:\n",
    "        lcc_coverage = 0\n",
    "        largest_cc = []\n",
    "\n",
    "    # B. K-Core Decomposition (Finding the \"Hard Core\" of the language)\n",
    "    try:\n",
    "        core_numbers = nx.core_number(graph_corpus)\n",
    "        max_k_core = max(core_numbers.values()) if core_numbers else 0\n",
    "        average_core = sum(core_numbers.values()) / len(core_numbers) if core_numbers else 0\n",
    "    except:\n",
    "        max_k_core = 0\n",
    "        average_core = 0\n",
    "\n",
    "    return {\n",
    "        \"largest_component_coverage\": f\"{lcc_coverage:.2%}\",\n",
    "        \"max_k_core_value\": max_k_core,\n",
    "        \"average_core_connectivity\": f\"{average_core:.2f}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = prepare_data(\n",
    "    file_path=\"GoodNightGorilla.txt\",\n",
    "    language=\"english\",\n",
    "    remove_accent=True,\n",
    "    remove_punct=True,\n",
    "    keep_apostrophes=False,\n",
    "    contraction_map=None,\n",
    "    stop_words=[]\n",
    ")\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "co_occ_m, word_list, word_index = compute_co_occurrence_matrix(corpus, window_size=5)\n",
    "\n",
    "cooc_df = pd.DataFrame(\n",
    "    data=co_occ_m,\n",
    "    index=word_list,\n",
    "    columns=word_list\n",
    ")\n",
    "\n",
    "graph_corpus = make_Graph_from_cooccurrence(co_occ_m, word_list, threshold=1)\n",
    "print(graph_corpus)\n",
    "\n",
    "get_metrics_from_graph(graph_corpus)\n",
    "\n",
    "# save graph to .gexf file\n",
    "nx.write_gexf(graph_corpus, \"good_night_gorilla_cooccurrence.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f26091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display graph_corpus\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# pos = nx.spring_layout(graph_corpus, k=0.15)\n",
    "# nx.draw_networkx_nodes(graph_corpus, pos, node_size=50)\n",
    "# nx.draw_networkx_edges(graph_corpus, pos, alpha=0.3)\n",
    "# nx.draw_networkx_labels(graph_corpus, pos, font_size=8)\n",
    "# plt.title(\"Co-occurrence Graph from 'Good Night Gorilla'\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(graph_corpus) > 0:\n",
    "#     largest_cc = max(nx.connected_components(graph_corpus), key=len)\n",
    "#     lcc_coverage = len(largest_cc) / len(word_list)\n",
    "# else:\n",
    "#     lcc_coverage = 0\n",
    "#     largest_cc = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parasite_word(co_occurrence_df:pd.DataFrame, percentile_threshold:int=95):\n",
    "    parasitic_scores = {}\n",
    "    for word, row in co_occurrence_df.iterrows():\n",
    "        row_vec = row.values\n",
    "        row_sum = row_vec.sum()\n",
    "        if row_sum == 0:\n",
    "            parasitic_scores[word] = 0\n",
    "            continue\n",
    "        probs = row_vec / row_sum\n",
    "        score = entropy(probs, base=len(probs))\n",
    "        parasitic_scores[word] = score\n",
    "    scores_series = pd.Series(parasitic_scores).sort_values(ascending=False)\n",
    "    cutoff_value = np.percentile(scores_series, percentile_threshold)\n",
    "    bad_words = scores_series[scores_series >= cutoff_value].index.tolist()\n",
    "    return bad_words, scores_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occ_m, word_list, word_index = compute_co_occurrence_matrix(corpus, window_size=4)\n",
    "co_occ_m_5, _, _ = compute_co_occurrence_matrix(corpus, window_size=8)\n",
    "cooc_df_5 = pd.DataFrame(\n",
    "    data=co_occ_m_5,\n",
    "    index=word_list,\n",
    "    columns=word_list\n",
    ")\n",
    "np.fill_diagonal(cooc_df_5.values, 0)\n",
    "_, all_score_5 = get_parasite_word(cooc_df_5)\n",
    "\n",
    "cooc_df = pd.DataFrame(\n",
    "    data=co_occ_m,\n",
    "    index=word_list,\n",
    "    columns=word_list\n",
    ")\n",
    "\n",
    "cooc_df_diag_zero = cooc_df.copy()\n",
    "np.fill_diagonal(cooc_df_diag_zero.values, 0)\n",
    "\n",
    "bad_word, all_score = get_parasite_word(cooc_df_diag_zero)\n",
    "\n",
    "res = [[x for x in sub if x not in bad_word] for sub in corpus]\n",
    "print(res)\n",
    "co_occ_m1, word_list1, word_index1 = compute_co_occurrence_matrix(res, window_size=2)\n",
    "cooc_df1 = pd.DataFrame(\n",
    "    data=co_occ_m1,\n",
    "    index=word_list1,\n",
    "    columns=word_list1\n",
    ")\n",
    "cooc_df_diag_zero1 = cooc_df1.copy()\n",
    "np.fill_diagonal(cooc_df_diag_zero1.values, 0)\n",
    "bad_word1, all_score1 = get_parasite_word(cooc_df_diag_zero1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd624eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_test = \"across\"\n",
    "print(all_score[w_test])\n",
    "print(cooc_df[w_test].values)\n",
    "print(cooc_df[w_test].idxmax(), cooc_df[w_test].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(co_occurrence_matrix:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute Positive Pointwise Mutual Information (PPMI) matrix.\n",
    "    \"\"\"\n",
    "    total = np.sum(co_occurrence_matrix)\n",
    "    row_sums = np.sum(co_occurrence_matrix, axis=1)\n",
    "    col_sums = np.sum(co_occurrence_matrix, axis=0)\n",
    "    \n",
    "    expected = np.outer(row_sums, col_sums) / total\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        pmi = np.log(co_occurrence_matrix / expected)\n",
    "    \n",
    "    pmi[pmi < 0] = 0\n",
    "    pmi = np.nan_to_num(pmi)\n",
    "    \n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_df_wihtout_stop_word = cooc_df_diag_zero.copy()\n",
    "drops = [w for w in bad_word if w in cooc_df_wihtout_stop_word.index]\n",
    "cooc_df_wihtout_stop_word = cooc_df_wihtout_stop_word.drop(index=drops, columns=drops)\n",
    "\n",
    "ppmi = PPMI(cooc_df_wihtout_stop_word.values)\n",
    "\n",
    "ppmi_df = pd.DataFrame(\n",
    "    data=ppmi,\n",
    "    index=cooc_df_wihtout_stop_word.index,\n",
    "    columns=cooc_df_wihtout_stop_word.columns\n",
    ")\n",
    "\n",
    "G = nx.from_pandas_adjacency(ppmi_df)\n",
    "nx.write_gexf(G, \"good_night_gorilla_PPMI.gexf\")\n",
    "\n",
    "nx.community.louvain_communities(G=G)\n",
    "\n",
    "\n",
    "ppmi_without_traitement = PPMI(cooc_df_5.values)\n",
    "ppmi_df_without_traitement = pd.DataFrame(\n",
    "    data=ppmi_without_traitement,\n",
    "    index=cooc_df_5.index,\n",
    "    columns=cooc_df_5.columns\n",
    ")\n",
    "\n",
    "G = nx.from_pandas_adjacency(ppmi_df_without_traitement)\n",
    "nx.write_gexf(G, \"good_night_gorilla_PPMI_without_traitement_size8.gexf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_df_wihtout_stop_word = cooc_df_diag_zero1.copy()\n",
    "drops = [w for w in bad_word if w in cooc_df_wihtout_stop_word.index]\n",
    "cooc_df_wihtout_stop_word = cooc_df_wihtout_stop_word.drop(index=drops, columns=drops)\n",
    "\n",
    "ppmi = PPMI(cooc_df_wihtout_stop_word.values)\n",
    "\n",
    "ppmi_df = pd.DataFrame(\n",
    "    data=ppmi,\n",
    "    index=cooc_df_wihtout_stop_word.index,\n",
    "    columns=cooc_df_wihtout_stop_word.columns\n",
    ")\n",
    "\n",
    "G = nx.from_pandas_adjacency(ppmi_df)\n",
    "nx.write_gexf(G, \"good_night_gorilla_PPMI.gexf\")\n",
    "\n",
    "nx.community.louvain_communities(G=G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e656ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"gorilla\"\n",
    "word_b = \"zookeeper\"\n",
    "ppmi = ppmi_df\n",
    "\n",
    "vec_a = ppmi.loc[word_a].values.reshape(1, -1)\n",
    "vec_b = ppmi.loc[word_b].values.reshape(1, -1)\n",
    "score = cosine_similarity(vec_a, vec_b)[0][0]\n",
    "print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"gorilla\"\n",
    "word_b = \"animal\"\n",
    "ppmi = ppmi_df\n",
    "\n",
    "vec_a = ppmi.loc[word_a].values.reshape(1, -1)\n",
    "vec_b = ppmi.loc[word_b].values.reshape(1, -1)\n",
    "score = cosine_similarity(vec_a, vec_b)[0][0]\n",
    "print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"lion\"\n",
    "word_b = \"hyena\"\n",
    "ppmi = ppmi_df\n",
    "\n",
    "vec_a = ppmi.loc[word_a].values.reshape(1, -1)\n",
    "vec_b = ppmi.loc[word_b].values.reshape(1, -1)\n",
    "score = cosine_similarity(vec_a, vec_b)[0][0]\n",
    "print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a491845",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"lion\"\n",
    "word_b = \"little\"\n",
    "ppmi = ppmi_df\n",
    "\n",
    "vec_a = ppmi.loc[word_a].values.reshape(1, -1)\n",
    "vec_b = ppmi.loc[word_b].values.reshape(1, -1)\n",
    "score = cosine_similarity(vec_a, vec_b)[0][0]\n",
    "print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"lion\"\n",
    "word_b = \"giraffe\"\n",
    "ppmi = ppmi_df\n",
    "\n",
    "vec_a = ppmi.loc[word_a].values.reshape(1, -1)\n",
    "vec_b = ppmi.loc[word_b].values.reshape(1, -1)\n",
    "score = cosine_similarity(vec_a, vec_b)[0][0]\n",
    "print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cd21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(target_word, ppmi_df, top_n=5):\n",
    "    target_vec = ppmi_df.loc[target_word].values.reshape(1, -1)\n",
    "    all_scores = cosine_similarity(ppmi_df.values, target_vec)\n",
    "    score_series = pd.Series(all_scores.flatten(), index=ppmi_df.index)\n",
    "    top_words = score_series.sort_values(ascending=False).drop(target_word).head(top_n)\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"banana\"\n",
    "nearest_neighbors = find_nearest_neighbors(word_a, ppmi_df, top_n=10)\n",
    "print(f\"Nearest Neighbors to '{word_a}':\")\n",
    "print(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = find_nearest_neighbors(\"gorilla\", ppmi_df, top_n=10)\n",
    "print(\"Nearest Neighbors to 'gorilla':\")\n",
    "print(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"little\"\n",
    "nearest_neighbors = find_nearest_neighbors(word_a, ppmi_df, top_n=10)\n",
    "print(f\"Nearest Neighbors to '{word_a}':\")\n",
    "print(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"mouse\"\n",
    "nearest_neighbors = find_nearest_neighbors(word_a, ppmi_df, top_n=10)\n",
    "print(f\"Nearest Neighbors to '{word_a}':\")\n",
    "print(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec98373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text: str) -> str:\n",
    "    \"\"\"Normalizes text to remove accents (e.g., 'café' -> 'cafe').\"\"\"\n",
    "    nk = unicodedata.normalize(\"NFKD\", text)\n",
    "    return \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "\n",
    "def prepare_data_with_intonation(\n",
    "    file_path: str,\n",
    "    language: str,\n",
    "    remove_accent: bool = True,\n",
    "    remove_punct: bool = True,\n",
    "    keep_apostrophes: bool = True,\n",
    "    contraction_map: Optional[Dict[str, str]] = None,\n",
    "    stop_words: Optional[List[str]] = None,\n",
    "    break_line: bool = True,\n",
    "    expand_is_contraction: bool = True\n",
    "    ) -> List[List[str]]:\n",
    "\n",
    "    sentence_split_re = re.compile(r'[\\.!\\?]+')\n",
    "    \n",
    "    contraction_re = None\n",
    "    if contraction_map:\n",
    "        pattern = \"|\".join(re.escape(k) for k in sorted(contraction_map.keys(), reverse=True))\n",
    "        contraction_re = re.compile(f\"({pattern})\")\n",
    "\n",
    "    punctuation_chars = set(string.punctuation)\n",
    "    if keep_apostrophes or expand_is_contraction:\n",
    "        punctuation_chars -= {\"'\", \"’\"}\n",
    "    \n",
    "    punct_trans_table = str.maketrans({c: \" \" for c in punctuation_chars})\n",
    "    stop_words_set: Set[str] = set(stop_words) if stop_words else set()\n",
    "    tokens_by_sentence: List[List[str]] = []\n",
    "    \n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            sub_lines = sentence_split_re.split(line.strip().lower()) if break_line else [line.strip().lower()]\n",
    "            \n",
    "            for s in sub_lines:\n",
    "                if not s: continue\n",
    "                \n",
    "                if contraction_re:\n",
    "                    s = contraction_re.sub(lambda m: contraction_map[m.group(0)], s)\n",
    "                    \n",
    "                s = s.replace(\"-\", \"\")\n",
    "                s = s.replace(\"—\", \" \")\n",
    "                \n",
    "                if remove_accent:\n",
    "                    s = remove_accents(s) \n",
    "\n",
    "                if remove_punct:\n",
    "                    s = s.translate(punct_trans_table)\n",
    "\n",
    "                toks = word_tokenize(s, language=language)\n",
    "\n",
    "                if expand_is_contraction and language == 'english':\n",
    "                    tagged = nltk.pos_tag(toks)\n",
    "                    new_toks = []\n",
    "                    for word, tag in tagged:\n",
    "                        if tag == 'POS': continue # Remove possession\n",
    "                        elif word in [\"'s\", \"’s\"] and tag == 'VBZ':\n",
    "                            new_toks.append(\"is\")\n",
    "                        else:\n",
    "                            new_toks.append(word)\n",
    "                    toks = new_toks\n",
    "\n",
    "                clean_toks = []\n",
    "                for t in toks:\n",
    "                    t_stripped = t.strip(\"'’\")\n",
    "                    if t_stripped and t_stripped not in stop_words_set:\n",
    "                        clean_toks.append(t_stripped)\n",
    "                \n",
    "                if clean_toks:\n",
    "                    tokens_by_sentence.append(clean_toks)\n",
    "\n",
    "    return tokens_by_sentence\n",
    "\n",
    "def separate_text_intonation(data:List[List[str]]):\n",
    "    texts = []\n",
    "    intonations = []\n",
    "    for sentence in data:\n",
    "        intonation = sentence[1::2]\n",
    "        text = sentence[::2]\n",
    "        if all(t.isalpha() for t in text) and all(t.isdigit() for t in intonation):\n",
    "            texts.append(text)\n",
    "            intonations.append(intonation)\n",
    "        else:\n",
    "            print(\"Warning: Mismatched text and intonation in sentence:\", sentence)\n",
    "            print(\"Extracted text:\", text)\n",
    "            print(\"Extracted intonation:\", intonation)\n",
    "            for t in text:\n",
    "                if not t.isalpha():\n",
    "                    print(\" Non-alpha text token:\", t)\n",
    "            for i in intonation:\n",
    "                if not i.isdigit():\n",
    "                    print(\" Non-digit intonation token:\", i)\n",
    "            \n",
    "    return texts, intonations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data_with_intonation(\n",
    "    file_path=\"GoodNightGorilla_Intonation.txt\",\n",
    "    language='english',\n",
    "    remove_accent=True,\n",
    "    remove_punct=True,\n",
    "    keep_apostrophes=False,\n",
    "    contraction_map={\n",
    "        \"that's\" : \"thatis\",\n",
    "        \"it's\" : \"itis\",\n",
    "        \"don't\": \"donot\",\n",
    "        \"doesn't\": \"doesnot\",},\n",
    "    stop_words=[\"s\", \"n't\"],\n",
    "    break_line=False\n",
    ")\n",
    "for s in data:\n",
    "    print(s)\n",
    "texts, intonations = separate_text_intonation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_without_0intonation = []\n",
    "intonation_without_0intonation = []\n",
    "\n",
    "for sentence_t, sentence_i in zip(texts, intonations):\n",
    "    text_without_0intonation.append([])\n",
    "    intonation_without_0intonation.append([])\n",
    "    for t, i in zip(sentence_t, sentence_i):\n",
    "        if int(i) != 0:\n",
    "            text_without_0intonation[-1].append(t)\n",
    "            intonation_without_0intonation[-1].append(i)\n",
    "            \n",
    "occ_m, word_list, word_to_index = compute_co_occurrence_matrix(text_without_0intonation, window_size=4)\n",
    "ooc_df = pd.DataFrame(\n",
    "    data=occ_m,\n",
    "    index=word_list,\n",
    "    columns=word_list\n",
    ")\n",
    "\n",
    "# Put diagonal to zero\n",
    "np.fill_diagonal(ooc_df.values, 0)\n",
    "\n",
    "ppmi_without_0intonation = PPMI(ooc_df.values)\n",
    "\n",
    "# Graph from PPMI without 0 intonation\n",
    "ppmi_df_without_0intonation = pd.DataFrame(\n",
    "    data=ppmi_without_0intonation,\n",
    "    index=ooc_df.index,\n",
    "    columns=ooc_df.columns\n",
    ")\n",
    "\n",
    "filtered_df = ppmi_df_without_0intonation.where(ppmi_df_without_0intonation >= 1, 0)\n",
    "G = nx.from_pandas_adjacency(filtered_df)\n",
    "nx.write_gexf(G, \"good_night_gorilla_PPMI_without_0intonation.gexf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"banana\"\n",
    "df = ppmi_df_without_0intonation\n",
    "nearest_neighbors = find_nearest_neighbors(word_a, df, top_n=10)\n",
    "print(f\"Nearest Neighbors to '{word_a}':\")\n",
    "print(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"gorilla\"\n",
    "df = ppmi_df_without_0intonation\n",
    "nearest_neighbors = find_nearest_neighbors(word_a, df, top_n=10)\n",
    "print(f\"Nearest Neighbors to '{word_a}':\")\n",
    "print(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_a = \"little\"\n",
    "df = ppmi_df_without_0intonation\n",
    "nearest_neighbors = find_nearest_neighbors(word_a, df, top_n=10)\n",
    "print(f\"Nearest Neighbors to '{word_a}':\")\n",
    "print(nearest_neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
