{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61823d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "# and https://github.com/theeluwin/pytorch-sgns/\n",
    "# and https://jalammar.github.io/illustrated-word2vec/\n",
    "# and https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import math, random\n",
    "\n",
    "from typing import Sequence, Optional, Callable, List\n",
    "\n",
    "from visuEmbedding import interactive_embedding_plot_2D, interactive_embedding_plot_3D, plot_similarity_heatmap, cluster_words, components_to_fig_3D\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2192e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert True\n",
    "# assert False"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "185373b5",
   "metadata": {},
   "source": [
    "# DataSet (statique)\n",
    "\n",
    "Pour entraîner notre Word2Vec, il nous faut un jeu de donné particulier. Nous voulons \"reprocher\" certains mots et en \"éloigner\" d'autre. Il faut créer des pairs de similarité et de différence. Ce sont des pairs positif et négatif. Nous pouvons \"forcer\" ses paires, ou, de manière non supervisé, prendre un texte et créer automatiquement les paires. \n",
    "\n",
    "## Statique ?\n",
    "\n",
    "Dans une première version nous créons/chargeons toutes les paires en amont. Si nous passons toutes ces pairs à un data loader de pytorch, toutes ces pairs serons chargé en mémoire. Ce qui peut être trop gros.\n",
    "\n",
    "## Créations automatique de pair (mots positif)\n",
    "\n",
    "Pour créer automatiquement des pairs positif et négatifs, nous allons devoir déterminer des mots \"centraux\". Ces mots serons rattaché à tout les mots présent dans une certaine fenêtre de celui ci.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Par exemple dans cette phrase, si nous prenons \"on\" comme mot central, alors [\"cat\", \"sits\", \"the\", \"mat\"] serons des mots de contexte. Nous pouvons formé les pairs positifs : [(\"on\", \"cat\"), (\"on\", \"sits\"), (\"on\", \"the\"), (\"on\", \"mat\")].\n",
    "\n",
    "### Subsampling\n",
    "\n",
    "Nous ne voulons pas forcement prendre tout les mots comme mots central. Dans la phrase d'exemple nous aurions deux fois le mots \"the\" comme mot central, cela induirais un biais lors de l’entraînement dans lequel les mots trop fréquents serais sur représenté. Dans l'idéal, avec la phrase d'exemple nous voudrions qu'un seul \"the\" soit sélectionné. Pour ce faire nous calculons la fréquence de chaque mots (The : 2; cat : 1, sits : 1; on : 1; \"mat\": 1) puis, nous calculons une probabilité de le sélectionné (dans l'idéal 50% pour le mot \"the\" et 100% pour les autres). Cette idée s'appelle le **Subsampling** et nous vient du papier original du Word2Vec : \"Distributed Representations of Words and Phrases and their Compositionality\". La formule utilisé pour calculer la probabilité est :\n",
    "\n",
    "![image-2.png](attachment:image-2.png) \n",
    "\n",
    "Où f(wi) est la fréquence d’apparition du mot wi et t est un seuil qui permet de plus ou moins diminuer la probabilité des mots fréquents.\n",
    "\n",
    "Voici une implémentation possible de la formule en python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9361068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La fréquence des mots du texte :\n",
      "Counter({'b': 3, 'd': 3, 'c': 2, 'a': 1})\n",
      "le mots b apparaît 3 fois\n",
      "Le mots b a 84.7722557505166 % avec t 0.9 d'être un mots central a noter que la formule peut donner un nombre > à 100 (dans se cas on borne a 100)\n",
      "Avec un seuil plus petit (0.09) le mots b a 20.320508075688775 % d'être un mots central\n"
     ]
    }
   ],
   "source": [
    "data = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"d\",\"d\",\"d\"] \n",
    "freq = Counter(data) # Calcul de la fréquence des mots\n",
    "print(\"La fréquence des mots du texte :\")\n",
    "print(freq)\n",
    "\n",
    "# On prend par exemple le mots \"b\"\n",
    "f = freq[\"b\"] # Le nombre de fois que le mots apparaît dans le texte\n",
    "print(f\"le mots b apparaît {f} fois\")\n",
    "\n",
    "t = 1e-1 * len(data)# Le seuil qui permet de savoir a partir de quand on diminue la probabilité du mots d'apparaître\n",
    "\n",
    "print(f\"Le mots b a {((f / t)**0.5 + 1) * (t / f) * 100 } % avec t {t} d'être un mots central a noter que la formule peut donner un nombre > à 100 (dans se cas on borne a 100)\")\n",
    "\n",
    "t = 1e-2 * len(data)\n",
    "\n",
    "print(f\"Avec un seuil plus petit ({t}) le mots b a {((f / t)**0.5 + 1) * (t / f) * 100 } % d'être un mots central\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5501900",
   "metadata": {},
   "source": [
    "# Mots négatifs\n",
    "Pour déterminer les mots négatifs des mots centraux, on veut connaître la proportion de chaque mots dans tout le corpus. On utilise ce code :\n",
    "```py\n",
    "freq_list = [self.freq.get(self.decoder[i], 0) for i in range(self.vocab_size)]\n",
    "unigram = torch.tensor([f**power for f in freq_list], dtype=torch.float)\n",
    "unigram / unigram.sum()\n",
    "```\n",
    "\n",
    "En plus d'un \"simple\" calcul de proportions, nous avons un atténuateur power. Si il est < 1 alors les mots plus fréquents serons atténué. Si il est > 1 alors c'est l'inverse. L'idée nous vient aussi de \"Distributed Representations of Words and Phrases and their Compositionality\" qui montre que pour leur jeu de données, une puissance de 3/4 est l'idéal.\n",
    "\n",
    "Voici une implémentation possible de cette idée :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53cbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec un power = 1 proportion réel\n",
      "tensor([0.3333, 0.2222, 0.3333, 0.1111])\n",
      "{'d', 'c', 'b', 'a'}\n",
      "avec un power = 0.01 proportion atténué\n",
      "tensor([0.2593, 0.2490, 0.2593, 0.2323])\n",
      "{'d', 'c', 'b', 'a'}\n"
     ]
    }
   ],
   "source": [
    "freq = Counter(data)\n",
    "set_vocab = set(data)\n",
    "power = 1\n",
    "\n",
    "freq_list = [freq.get(i, 0) for i in set_vocab]\n",
    "unigram = torch.tensor([f**power for f in freq_list], dtype=torch.float)\n",
    "print(\"Avec un power = 1 proportion réel\")\n",
    "print(unigram / unigram.sum())\n",
    "print(set_vocab)\n",
    "\n",
    "power = 0.1\n",
    "freq_list = [freq.get(i, 0) for i in set_vocab]\n",
    "unigram = torch.tensor([f**power for f in freq_list], dtype=torch.float)\n",
    "print(\"avec un power = 0.01 proportion atténué\")\n",
    "print(unigram / unigram.sum())\n",
    "print(set_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8982c7",
   "metadata": {},
   "source": [
    "Maintenant pour chaque mot centraux, nous pouvons prendre des mots négatif suivant la distribution calculé. A noté que des mots trop fréquent risque de se retrouver à la fois positifs et négatifs. C'est quelque chose qui ne parait pas abérant, car des mots trop fréquent ne sont pas des mots que l'on veut rapproché. Par exemple le mot \"le\" peut être lié a beaucoup de mot. Donc on ne veut pas le rapproché d'un mot en particulier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c957f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS_store_DataSet(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset pour word2vec avec les négatives sampling.\n",
    "    \"\"\"\n",
    "    def _dist_unigram(self, power:float):\n",
    "        # La distribution unigram ^.75\n",
    "        freq_list = [self.freq.get(self.decoder[i], 0) for i in range(self.vocab_size)]\n",
    "        unigram = torch.tensor([f**power for f in freq_list], dtype=torch.float)\n",
    "        return unigram / unigram.sum()\n",
    "    \n",
    "    def _proba_words(self):\n",
    "        # Calcul des probabilité de garder un mot.\n",
    "        if self.subsample_thresh > 0.0:\n",
    "            # probabilité de garder un mot: p = (sqrt(f_t / t) + 1) * (t / f_t)\n",
    "            total_tokens = sum(self.freq.values())\n",
    "            word_probs = {}\n",
    "            t = self.subsample_thresh * total_tokens\n",
    "            for w, c in self.freq.items():\n",
    "                f = c\n",
    "                prob_keep = ((f / t)**0.5 + 1) * (t / f)\n",
    "                # clamp between 0 and 1\n",
    "                word_probs[w] = min(1.0, prob_keep)\n",
    "        else:\n",
    "            word_probs = None\n",
    "        return word_probs\n",
    "\n",
    "    def _make_pairs(self):\n",
    "        self.pairs = []\n",
    "        for sent in self.sentences:\n",
    "            # option subsampling pendant pré-construction si activé\n",
    "            if self.word_probs is not None:\n",
    "                filtered = [w for w in sent if random.random() < self.word_probs.get(w, 1.0)]\n",
    "            else:\n",
    "                filtered = sent\n",
    "            ids = [self.encoder[w] for w in filtered if w in self.encoder]\n",
    "            L = len(ids)\n",
    "            for i, center in enumerate(ids):\n",
    "                # fenêtre aléatoire entre 1 et window_size (pratique commune)\n",
    "                # Évite le biais d'un apprentissage sur un contexte fixe, ajoute de la dynamique dans l'apprentissage \n",
    "                # cur_window = random.randint(1, self.context_size)\n",
    "                cur_window = self.context_size\n",
    "                start = max(0, i - cur_window)\n",
    "                end = min(L, i + cur_window + 1)\n",
    "                for j in range(start, end):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    context = ids[j]\n",
    "                    self.pairs.append((center, context))\n",
    "\n",
    "    def __init__(self, sentences:list[list[str]], window_size:int=2, nb_neg:int=5, power=0.75,\n",
    "                 subsample_thresh:float=1e-5 , vocab_freq:None|dict|Counter=None, vocab_size_limit:None|int=None):\n",
    "        \"\"\"Initialise le dataset pour du Word2Vec avec des pairs négative. (Warning Méthode qui stocke en mémoire)\n",
    "        \n",
    "        Args:\n",
    "            sentences: Liste des phrases du corpus de texte (une phrases doit être une liste de str)\n",
    "            window_size: La taille de fenêtre pour créer les pairs positif.\n",
    "            nb_neg: Nombre de pair négatif pour chaque mots. (K)\n",
    "            subsample_thresh: Pour réduire la fréquence des mots trop fréquent (Ex : de, le, la, ...) dans le choix des mots centraux\n",
    "            power: Pour réduire la fréquence des mots trop fréquent dans les négatifs\n",
    "            vocab_freq: Dictionnaire ou counter (https://docs.python.org/3/library/collections.html#counter-objects) pour chaque mots indique la fréquence de se mot dans tout le corpus\n",
    "            vocab_size_limit: Pour ne garder que les top-N mots par fréquence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        subsample_thresh = float(subsample_thresh)\n",
    "        # assert isinstance(sentences, list[list[str]]), \"sentences should be a list[list[str]]\"\n",
    "        assert isinstance(window_size, int), \"sentences should be a int\"\n",
    "        assert isinstance(nb_neg, int), \"sentences should be a int\"\n",
    "        assert isinstance(subsample_thresh, float), \"sentences should be a float\"\n",
    "\n",
    "        self.sentences:list[list[str]] = sentences\n",
    "        self.context_size:int = window_size\n",
    "        self.K:int = nb_neg\n",
    "        self.power = power\n",
    "\n",
    "        if vocab_freq is not None:\n",
    "            full_freq:Counter = Counter(vocab_freq)\n",
    "        else:\n",
    "            all_tokens = [t for s in sentences for t in s]\n",
    "            full_freq:Counter = Counter(all_tokens)\n",
    "\n",
    "        if vocab_size_limit is not None:\n",
    "            most_common = full_freq.most_common(vocab_size_limit)\n",
    "            kept_words = [w for w, _ in most_common]\n",
    "            # On recalcul la fréquence des mots\n",
    "            self.freq = Counter({w: full_freq[w] for w in kept_words})\n",
    "        else:\n",
    "            self.freq = full_freq\n",
    "\n",
    "        self.vocab = list(self.freq.keys())\n",
    "        self.vocab_size:int = len(self.vocab)\n",
    "        self.encoder:dict = {w:i for i,w in enumerate(self.vocab)}\n",
    "        self.decoder = {i:w for w,i in self.encoder.items()}\n",
    "\n",
    "        # Calcul de la distibution ungigram permettant de créer les pairs négatifs\n",
    "        self.unigram_dist = self._dist_unigram(self.power)\n",
    "\n",
    "        # Calcul de la probabilité de garder les mots\n",
    "        self.subsample_thresh:float = subsample_thresh\n",
    "        self.word_probs:dict = self._proba_words()\n",
    "\n",
    "        # Create pairs  \n",
    "        self.pairs = None\n",
    "        self._make_pairs()\n",
    "        assert len(self.pairs) != 0 , \"Error to make positif pairs\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def _sample_negatives(self, batch_size):\n",
    "        \"\"\"\n",
    "        Échantillonne (batch_size, K) négatifs selon self.unigram_dist.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: La taille de batch [B]\n",
    "\n",
    "        Return:\n",
    "           torch.LongTensor\n",
    "        \"\"\"\n",
    "        # torch.multinomial attend vecteur de probabilités ; on échantillonne batch_size*K et reshape\n",
    "        neg = torch.multinomial(self.unigram_dist, batch_size * self.K, replacement=True)\n",
    "        return neg.view(batch_size, self.K)    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Prends la pairs positif idx (idx >= 0 and idx < len(self.pairs) et les négatifs qui sont calculé avec la distribution unigram\n",
    "\n",
    "        Args:\n",
    "            idx: Index de la pairs positifs.\n",
    "\n",
    "        Return :\n",
    "            Tuple(center_id:Long, pos_id:long, negatives_ids: torch.LongTensor de taille [K])\n",
    "        - center_id: long\n",
    "        - pos_id: long\n",
    "        - negatives: torch.LongTensor shape [K]\n",
    "        \"\"\"\n",
    "        center, pos = self.pairs[idx]\n",
    "        # neg = torch.multinomial(self.unigram_dist, self.K, replacement=True)\n",
    "        neg = torch.multinomial(self.unigram_dist, self.K, replacement=True)\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(pos, dtype=torch.long), neg\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        \"\"\"Fonction de collate pour DataLoader.\n",
    "        Args:\n",
    "            batch: list of tuples (center, pos, neg) où neg is tensor [K]\n",
    "        Return:\n",
    "            Tuple(centers:Torch.Tensor [B], pos:Torch.Tensor [B], neg:Torch.Tensor [B, K])\n",
    "        \"\"\"\n",
    "        centers = torch.stack([item[0] for item in batch], dim=0)\n",
    "        pos = torch.stack([item[1] for item in batch], dim=0)\n",
    "        negs = torch.stack([item[2] for item in batch], dim=0)\n",
    "        return centers, pos, negs\n",
    "\n",
    "    def sample_batch_negatives(self, centers, K=None):\n",
    "        \"\"\"Méthode pour voir un échantillons de pairs négatifs sur un batch de mots centraux\n",
    "        Args:\n",
    "            centers: Tensor d'idx des mots centraux\n",
    "        Return:\n",
    "            Torch.Tensor() [B K]\n",
    "        \"\"\"\n",
    "        B = centers.size(0)\n",
    "        K = self.K if K is None else K\n",
    "        return torch.multinomial(self.unigram_dist, B * K, replacement=True).view(B, K)\n",
    "    \n",
    "    def encode(self, words:list|str) -> list|int:\n",
    "        if isinstance(words, str) : return self.encoder[words]\n",
    "        ids = []\n",
    "        for w in words :\n",
    "            ids.append(self.encoder[w])\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids:list|int) -> list|int:\n",
    "        if isinstance(ids, int) : return self.decoder[ids]\n",
    "        words = []\n",
    "        for i in ids :\n",
    "            words.append(self.decoder[i])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3dccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le modèle SkipGramModel\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, emb_size:int, embedding_dimension:int=15, sparse:bool=False, context_dimension:int|None=None):\n",
    "        \"\"\"Initialisation du modèle SkipGram\n",
    "        Args:\n",
    "            emb_size: La taille de l'embedding, ce nombre devrais être déterminé après le process sur les data, et dépend de la taille de la fenêtre glissante.\n",
    "            embedding_dimension: La taille souhaité de l'embedding. Pour notre cas d'utilisation nous préférons une taille très petit\n",
    "            sparse: Bool permettant l'optimisation des calcul par pytorch si la fonction d'optimisation SGD est utilisé.\n",
    "            context_dimension: Il n'est pas recommandé de mettre un entier mais de laisser a None.\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emb_size:int = emb_size\n",
    "        self.emb_dim:int = embedding_dimension\n",
    "        # On définit pour chaque mots un embedding (soit un vecteur qui représente le mots)\n",
    "        self.word_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.emb_dim, device=device)\n",
    "\n",
    "        # Ce deuxième embedding correspond au mots utilisé dans un contexte (!= d'être utiliser comme mot centrale)\n",
    "        self.con_size = embedding_dimension if context_dimension is None else context_dimension\n",
    "        self.con_emb:nn.Embedding = nn.Embedding(num_embeddings=self.emb_size, embedding_dim=self.con_size, device=device)\n",
    "\n",
    "        init_range = 0.5 / self.emb_dim\n",
    "        self.word_emb.weight.data.uniform_(-init_range, init_range)\n",
    "        self.con_emb.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, centrals_words:list|torch.Tensor, pos_context:list|torch.Tensor, neg_context:list|torch.Tensor):\n",
    "        \"\"\"Fonction du forward pour le modèle SkipGramModel\n",
    "        Args:\n",
    "            centrals_words: Liste des ids des tokens des mots centraux [B]\n",
    "            pos_context: Liste des ids des tokens des mots dans le contexte [B]\n",
    "            neg_context: Liste des ids des tokens des mots non présent dans le contexte [B, K]\n",
    "        \"\"\"\n",
    "        # B : batch size\n",
    "        # D : dimension de l'embedding\n",
    "        # K : Nombre de mots négatifs\n",
    "\n",
    "        # Pour chaque pair positif, on récupère :\n",
    "        # Le vecteur du mots centrale (les valeurs de l'embeddding pour le token)\n",
    "        words_emb:torch.Tensor = self.word_emb(centrals_words) # [B, D]\n",
    "        # Le vecteur du mots contexte\n",
    "        context_emb:torch.Tensor = self.con_emb(pos_context) # [B, D]\n",
    "        # Et les vecteurs des mots négatifs\n",
    "        neg_emb:torch.Tensor = self.con_emb(neg_context) # [B, K, D]\n",
    "\n",
    "        # Pour chaque pair on calcul le score de similarité (mots central et contexte positif)\n",
    "        # positive score: log sigma(u . v_pos)\n",
    "        pos_score = torch.sum(words_emb * context_emb, dim=1)\n",
    "        # See https://docs.pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#logsigmoid pour la LogSigmoid \n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "\n",
    "        # On calcul aussi le score de dissimilarité (mots centrals et les mots non présent dans le context)\n",
    "        # negative score: sum log sigma(-u . v_neg)\n",
    "        # neg_emb : [B, K, D], on veut multiplier les vecteurs pour chaque mots\n",
    "        # Il faut donc ajouter une dimension à words_emb (voir https://docs.pytorch.org/docs/stable/generated/torch.bmm.html#torch-bmm)\n",
    "        # words_emb.unsqueeze(2) : [B, D, 1]\n",
    "        # See https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch-unsqueeze pour l'ajout de dimension\n",
    "        neg_score = torch.bmm(neg_emb, words_emb.unsqueeze(-1)).squeeze(2)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "    \n",
    "    def save_weight(self, path:str=\"SGNS_weights/\"):\n",
    "        \"\"\"Sauvegarde des poids des deux embeddings (word embedding et context embedding) dans le dossier path\n",
    "        Args :\n",
    "            path: Le dossier dans lequel sauvegarder les poids des deux embeddings\n",
    "        \"\"\"\n",
    "        word_weights = self.word_emb.weight.detach().cpu()\n",
    "        con_weight = self.con_emb.weight.detach().cpu()\n",
    "        torch.save(word_weights, path+'word_embedding.pt')\n",
    "        torch.save(con_weight, path+'con_embedding.pt')\n",
    "\n",
    "    def load_weight(self, path:str=\"SGNS_weights/\", name_word_weights:str=\"word_embedding.pt\", name_con_weights:str=\"con_embedding.pt\"):\n",
    "        \"\"\"Charge les poids depuis un fichier de sauvegarde de pytorch\n",
    "        Args :\n",
    "            path: Le dossier où se trouve les deux fichiers des poids\n",
    "            name_word_weights: Le nom du fichier contenant les poids du word embedding\n",
    "            name_con_weights: Le nom du fichier contenant les poids du contexte embedding\n",
    "        \"\"\"\n",
    "        word_weights = torch.load(path + name_word_weights)\n",
    "        con_weight = torch.load(path + name_con_weights)\n",
    "\n",
    "        self.word_emb:nn.Embedding = nn.Embedding.from_pretrained(word_weights)\n",
    "        self.con_emb:nn.Embedding = nn.Embedding.from_pretrained(con_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e8b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "# class SGNS_Streaming_Dataset(Dataset):\n",
    "#     \"\"\"Dataset pour le SGNS fait de manière dynamique, pour éviter de devoir stoker de grand corpus.\n",
    "#     À utiliser pour de grand corpus\n",
    "\n",
    "#     100% chat GPT :\n",
    "#     \"\"\"\n",
    "\n",
    "#     \"\"\"\n",
    "#     Dataset de type map-style mais sans pré-construction des paires.\n",
    "#     Il estime un nombre total d'exemples via le nombre total de tokens et window_size.\n",
    "#     __getitem__(i) génère une paire aléatoire en parcourant le corpus de façon pseudo-aléatoire.\n",
    "#     Utile si on veut compatibilité DataLoader (shuffle géré par DataLoader) mais sans stocker pairs.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, sentences, word_to_idx, vocab_freq=None, window_size=5, K=5, subsample_thresh=0.0, approx_multiplier=2):\n",
    "#         super().__init__()\n",
    "#         self.sentences = sentences                # list[list[str]]\n",
    "#         self.word_to_idx = word_to_idx\n",
    "#         self.idx_to_word = {i:w for w,i in word_to_idx.items()}\n",
    "#         self.vocab_size = len(self.idx_to_word)\n",
    "#         self.window_size = window_size\n",
    "#         self.K = K\n",
    "\n",
    "#         # fréquences\n",
    "#         if vocab_freq is None:\n",
    "#             all_tokens = [t for s in sentences for t in s]\n",
    "#             self.freq = Counter(all_tokens)\n",
    "#         else:\n",
    "#             self.freq = Counter(vocab_freq)\n",
    "\n",
    "#         # unigram^0.75 distribution pour neg sampling\n",
    "#         power = 0.75\n",
    "#         freq_list = [self.freq.get(self.idx_to_word[i], 0) for i in range(self.vocab_size)]\n",
    "#         unigram = torch.tensor([f**power for f in freq_list], dtype=torch.float)\n",
    "#         self.unigram_dist = unigram / unigram.sum()\n",
    "\n",
    "#         # subsampling\n",
    "#         self.subsample_thresh = float(subsample_thresh)\n",
    "#         if self.subsample_thresh > 0.0:\n",
    "#             total = sum(self.freq.values())\n",
    "#             t = self.subsample_thresh * total\n",
    "#             self.word_keep_prob = {}\n",
    "#             for w, c in self.freq.items():\n",
    "#                 f = c / total\n",
    "#                 prob = ( (f / self.subsample_thresh)**0.5 + 1) * (self.subsample_thresh / f)\n",
    "#                 self.word_keep_prob[w] = min(1.0, prob)\n",
    "#         else:\n",
    "#             self.word_keep_prob = None\n",
    "\n",
    "#         # construire indexation \"plat\" des (sentence_idx, token_idx) pour accès rapide aux tokens\n",
    "#         # pour gros corpus on peut éviter ceci et itérer fichiers; ici on construit une table d'offsets (faible mémoire)\n",
    "#         self.sentence_lengths = [len(s) for s in self.sentences]\n",
    "#         self.cum_lengths = []\n",
    "#         c = 0\n",
    "#         for L in self.sentence_lengths:\n",
    "#             self.cum_lengths.append(c)\n",
    "#             c += L\n",
    "#         self.total_tokens = c\n",
    "\n",
    "#         # approx length pour DataLoader (nombre d'exemples de paires estimé)\n",
    "#         # Chaque token génère en moyenne ~2*window_size contextes (approx); multiplier pour marge\n",
    "#         self.approx_len = max(1, int(self.total_tokens * min(self.window_size, 5) * approx_multiplier))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.approx_len\n",
    "\n",
    "#     def _get_token_at(self, flat_idx):\n",
    "#         \"\"\"Retourne (sent_idx, tok_idx, token_str) pour un index plat dans [0, total_tokens).\"\"\"\n",
    "#         # bsearch cum_lengths pour trouver sent\n",
    "#         # simple linéar scan possible, mais on utilise bisect pour efficience\n",
    "#         import bisect\n",
    "#         sent_idx = bisect.bisect_right(self.cum_lengths, flat_idx) - 1\n",
    "#         if sent_idx < 0:\n",
    "#             sent_idx = 0\n",
    "#         tok_idx = flat_idx - self.cum_lengths[sent_idx]\n",
    "#         sent = self.sentences[sent_idx]\n",
    "#         if tok_idx >= len(sent):\n",
    "#             # cas improbable dû aux limites; normaliser\n",
    "#             tok_idx = len(sent) - 1\n",
    "#         return sent_idx, tok_idx, sent[tok_idx]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Génère une paire (center, context, negatives) pseudo-aléatoire à partir d'un index.\n",
    "#         - idx est utilisé pour varier la position, puis on choisit une fenêtre aléatoire autour du token.\n",
    "#         \"\"\"\n",
    "#         # map idx -> flat token index (cycle)\n",
    "#         flat_idx = idx % max(1, self.total_tokens)\n",
    "#         sent_idx, tok_idx, token = self._get_token_at(flat_idx)\n",
    "\n",
    "#         # si subsampling activé, rejeter token avec proba 1 - keep_prob et chercher un autre\n",
    "#         if self.word_keep_prob is not None:\n",
    "#             attempts = 0\n",
    "#             while random.random() > self.word_keep_prob.get(token, 1.0):\n",
    "#                 flat_idx = (flat_idx + 1) % max(1, self.total_tokens)\n",
    "#                 sent_idx, tok_idx, token = self._get_token_at(flat_idx)\n",
    "#                 attempts += 1\n",
    "#                 if attempts > 10_000:\n",
    "#                     break\n",
    "\n",
    "#         # construire la liste des indices valides de contexte dans la phrase\n",
    "#         sent = self.sentences[sent_idx]\n",
    "#         L = len(sent)\n",
    "#         if L <= 1:\n",
    "#             # phrase triviale -> retourner token négatifs aléatoires\n",
    "#             center = self.word_to_idx.get(token, None)\n",
    "#             if center is None:\n",
    "#                 # choisir un centre aléatoire si token OOV\n",
    "#                 center = random.randrange(self.vocab_size)\n",
    "#             pos = center\n",
    "#         else:\n",
    "#             cur_window = random.randint(1, self.window_size)\n",
    "#             start = max(0, tok_idx - cur_window)\n",
    "#             end = min(L, tok_idx + cur_window + 1)\n",
    "#             # lister les context positions possibles et choisir un contexte aléatoire\n",
    "#             context_positions = [j for j in range(start, end) if j != tok_idx]\n",
    "#             if not context_positions:\n",
    "#                 pos_token = token\n",
    "#             else:\n",
    "#                 pos_tok_idx = random.choice(context_positions)\n",
    "#                 pos_token = sent[pos_tok_idx]\n",
    "#             # convertir en ids (gestion OOV)\n",
    "#             center = self.word_to_idx.get(token, None)\n",
    "#             pos = self.word_to_idx.get(pos_token, None)\n",
    "#             if center is None:\n",
    "#                 center = random.randrange(self.vocab_size)\n",
    "#             if pos is None:\n",
    "#                 pos = random.randrange(self.vocab_size)\n",
    "\n",
    "#         # échantillonnage négatifs (K)\n",
    "#         neg = torch.multinomial(self.unigram_dist, self.K, replacement=True)\n",
    "\n",
    "#         return torch.tensor(center, dtype=torch.long), torch.tensor(pos, dtype=torch.long), neg\n",
    "\n",
    "\n",
    "# class IterableSkipGramDataset(IterableDataset):\n",
    "#     \"\"\"\n",
    "#     Version streamable recommandée: IterableDataset qui parcourt les phrases et génère paires à la volée.\n",
    "#     Idéal pour très grands corpus lus depuis disque (fichiers, lignes, gzip).\n",
    "#     \"\"\"\n",
    "#     def __init__(self, sentences_iterable, word_to_idx, vocab_freq=None, window_size=5, K=5, subsample_thresh=0.0):\n",
    "#         \"\"\"\n",
    "#         sentences_iterable: callable ou itérable qui renvoie un itérateur de phrases tokenisées.\n",
    "#           - Peut être une fonction generator() pour redémarrer l'itération à chaque epoch,\n",
    "#             ou un itérable unique (non-recommencé) si training en un seul passage.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.sentences_iterable = sentences_iterable\n",
    "#         self.word_to_idx = word_to_idx\n",
    "#         self.idx_to_word = {i:w for w,i in word_to_idx.items()}\n",
    "#         self.vocab_size = len(self.idx_to_word)\n",
    "#         self.window_size = window_size\n",
    "#         self.K = K\n",
    "\n",
    "#         # fréquences et unigram pour neg sampling (si non fournies, construire approximation légère)\n",
    "#         if vocab_freq is None:\n",
    "#             # si vocab_freq absent, construire à partir d'un premier passage (coûteux)\n",
    "#             raise ValueError(\"Pour IterableSkipGramDataset, fournis vocab_freq ou un precomputed unigram distribution.\")\n",
    "#         self.freq = Counter(vocab_freq)\n",
    "#         power = 0.75\n",
    "#         freq_list = [self.freq.get(self.idx_to_word[i], 0) for i in range(self.vocab_size)]\n",
    "#         unigram = torch.tensor([f**power for f in freq_list], dtype=torch.float)\n",
    "#         self.unigram_dist = unigram / unigram.sum()\n",
    "\n",
    "#         # subsampling\n",
    "#         self.subsample_thresh = float(subsample_thresh)\n",
    "#         if self.subsample_thresh > 0.0:\n",
    "#             total = sum(self.freq.values())\n",
    "#             t = self.subsample_thresh * total\n",
    "#             self.word_keep_prob = {}\n",
    "#             for w, c in self.freq.items():\n",
    "#                 f = c / total\n",
    "#                 prob = ( (f / self.subsample_thresh)**0.5 + 1) * (self.subsample_thresh / f)\n",
    "#                 self.word_keep_prob[w] = min(1.0, prob)\n",
    "#         else:\n",
    "#             self.word_keep_prob = None\n",
    "\n",
    "#     def get_sent_iterator(self):\n",
    "#         # si sentences_iterable est callable, appelez-le pour obtenir un nouvel itérateur par epoch\n",
    "#         if callable(self.sentences_iterable):\n",
    "#             return self.sentences_iterable()\n",
    "#         else:\n",
    "#             return iter(self.sentences_iterable)\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         sent_iter = self.get_sent_iterator()\n",
    "#         for sent in sent_iter:\n",
    "#             # sent : liste de tokens\n",
    "#             # appliquer subsampling à la volée\n",
    "#             if self.word_keep_prob is not None:\n",
    "#                 filtered = [w for w in sent if random.random() < self.word_keep_prob.get(w, 1.0)]\n",
    "#             else:\n",
    "#                 filtered = sent\n",
    "#             ids = [self.word_to_idx.get(w, None) for w in filtered]\n",
    "#             # filtrer OOV\n",
    "#             ids = [i for i in ids if i is not None]\n",
    "#             L = len(ids)\n",
    "#             if L <= 1:\n",
    "#                 continue\n",
    "#             for i, center in enumerate(ids):\n",
    "#                 cur_window = random.randint(1, self.window_size)\n",
    "#                 start = max(0, i - cur_window)\n",
    "#                 end = min(L, i + cur_window + 1)\n",
    "#                 for j in range(start, end):\n",
    "#                     if j == i:\n",
    "#                         continue\n",
    "#                     pos = ids[j]\n",
    "#                     neg = torch.multinomial(self.unigram_dist, self.K, replacement=True)\n",
    "#                     yield torch.tensor(center, dtype=torch.long), torch.tensor(pos, dtype=torch.long), neg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3785b6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1158029115.py, line 38)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn {\"loss_history\": loss_history, \"final_epoch_loss\": avg_epoch_loss}\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def train_Word2Vec(modelW2V:nn.Module, dataLoader:Dataset, optimizer:optim.Optimizer, epochs:int, verbal:bool=True, log_interval = 100):\n",
    "    \"\"\"Fonction d’entraînement pour un modèle Word2Vec\n",
    "\n",
    "    \"\"\"\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        batches = 0\n",
    "        loss_history = []\n",
    "        global_step = 0\n",
    "        \n",
    "        modelW2V.train()\n",
    "\n",
    "        for batch in dataLoader:\n",
    "            # centers: [B], pos: [B], negs: [B, K]\n",
    "            centers, pos, negs = batch\n",
    "            centers = centers.to(device)\n",
    "            pos = pos.to(device)\n",
    "            negs = negs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = modelW2V(centers, pos, negs)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            loss_history.append(batch_loss)\n",
    "            batches += 1\n",
    "            global_step += 1\n",
    "\n",
    "            if verbal and log_interval and (global_step % log_interval == 0):\n",
    "                print(f\"Epoch {epoch} Step {global_step} AvgLoss {epoch_loss / batches:.6f}\")\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / max(1, batches)\n",
    "        if verbal : print(f\"Epoch {epoch} finished. Avg loss: {avg_epoch_loss:.6f}\")\n",
    "    return {\"loss_history\": loss_history, \"final_epoch_loss\": avg_epoch_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63904068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import csv\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "    \"n't\": \" n't\", \"'re\": \" 're\", \"'ve\": \" 've\", \"'ll\": \" 'll\",\n",
    "    \"'d\": \" 'd\", \"'s\": \" 's\", \"'m\": \" 'm\"\n",
    "}\n",
    "\n",
    "def remove_accents(text: str) -> str:\n",
    "    nk = unicodedata.normalize(\"NFKD\", text)\n",
    "    return \"\".join([c for c in nk if not unicodedata.combining(c)])\n",
    "\n",
    "keep = {\"'\", \"’\"}\n",
    "punct_to_remove = ''.join(ch for ch in string.punctuation if ch not in keep)\n",
    "\n",
    "extra_punct = '“”‘’—–…«»'\n",
    "punct_to_remove += ''.join(ch for ch in extra_punct if ch not in keep)\n",
    "TRANSL_TABLE = str.maketrans('', '', punct_to_remove)\n",
    "\n",
    "new_txt = []\n",
    "tokens_by_sentence = []\n",
    "\n",
    "# with open(\"chat_chien.txt\", encoding='utf-8') as file:\n",
    "#     for sentence in file:\n",
    "#         # print(\"original sentence :\", sentence.rstrip('\\n'))\n",
    "#         sentence = sentence.lower()\n",
    "#         sentence = remove_accents(sentence)\n",
    "#         for k, v in CONTRACTION_MAP.items():\n",
    "#             sentence = sentence.replace(k, v)\n",
    "#         sentence = sentence.translate(TRANSL_TABLE)\n",
    "#         sentence = ' '.join(sentence.split())\n",
    "#         # print(\"sentence keep : \", sentence)\n",
    "#         new_txt.append(sentence + \"\\n\")\n",
    "#         tokens_by_sentence.append(word_tokenize(sentence, language=\"french\"))\n",
    "\n",
    "# with open(\"toy.txt\", encoding='utf-8') as file:\n",
    "#     for sentence in file:\n",
    "#         # print(\"original sentence :\", sentence.rstrip('\\n'))\n",
    "#         sentence = sentence.lower()\n",
    "#         sentence = remove_accents(sentence)\n",
    "#         for k, v in CONTRACTION_MAP.items():\n",
    "#             sentence = sentence.replace(k, v)\n",
    "#         sentence = sentence.translate(TRANSL_TABLE)\n",
    "#         sentence = ' '.join(sentence.split())\n",
    "\n",
    "#         new_txt.append(sentence + \"\\n\")\n",
    "#         tokens_by_sentence.append(word_tokenize(sentence, language=\"french\"))\n",
    "\n",
    "with open(\"data/test.txt\", encoding=\"utf-8\") as file:\n",
    "    for sentence in file:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = remove_accents(sentence)\n",
    "        for k, v in CONTRACTION_MAP.items():\n",
    "            sentence = sentence.replace(k, v)\n",
    "        sentence = sentence.translate(TRANSL_TABLE)\n",
    "        sentence = ' '.join(sentence.split())\n",
    "\n",
    "        new_txt.append(sentence + \"\\n\")\n",
    "        tokens_by_sentence.append(word_tokenize(sentence, language=\"english\"))\n",
    "\n",
    "print(tokens_by_sentence)\n",
    "\n",
    "tokens_by_sentence = [\n",
    "    [\"chat\", \"chien\", \"souris\"],\n",
    "    [\"souris\", \"chien\", \"chat\"],\n",
    "    [\"souris\", \"chat\", \"chat\"],\n",
    "    [\"chat\", \"chien\", \"chaton\"],\n",
    "    [\"souris\", \"chien\", \"chaton\"],\n",
    "    [\"souris\", \"chat\", \"chaton\"],\n",
    "    [\"balle\", \"train\", \"peluche\"],\n",
    "    [\"balle\", \"peluche\", \"train\"],\n",
    "    [\"peluche\", \"train\", \"balle\"]]\n",
    "\n",
    "all_tokens = [t for s in tokens_by_sentence for t in s]\n",
    "min_count = 1\n",
    "freq = Counter(all_tokens)\n",
    "vocab = [w for w, c in freq.items() if c >= min_count]\n",
    "token_to_idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx_to_token = {i:w for w,i in token_to_idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# with open('visu_voca.csv', 'w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     for item in token_to_idx.items():\n",
    "#         writer.writerow([item])\n",
    "\n",
    "# with open('visu_post_traitement.txt', 'w', newline='') as f:\n",
    "#     for sentence in new_txt:\n",
    "#         f.write(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SGNS_store_DataSet(sentences=tokens_by_sentence,\n",
    "                            window_size=3, nb_neg=3, subsample_thresh=1, vocab_size_limit=None)\n",
    "# dataset = SGNS_store_DataSet(sentences=tokens_by_sentence,\n",
    "#                            window_size=2, nb_neg=5, subsample_thresh= 1e-5, vocab_size_limit=5000)\n",
    "\n",
    "# subsample_thresh=1e-5\n",
    "print(dataset.pairs)\n",
    "print(dataset.unigram_dist)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=dataset.collate_batch)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print('all vocab :', vocab_size)\n",
    "print('vocab keep by dataset', dataset.vocab_size)\n",
    "print(\"nombre de pair créer :\", len(dataset.pairs))\n",
    "\n",
    "for i, (centers, pos, negs) in enumerate(loader):\n",
    "    print(f'Voici les mots centraux du batch {i} : ')\n",
    "    # print(centers)\n",
    "    print([idx_to_token[int(id)] for id in centers])\n",
    "    print(f'Voici les mots positifs du batch {i} : ')\n",
    "    print(dataset.decode(pos.tolist()))\n",
    "    # print([idx_to_token[int(id)] for id in pos])\n",
    "    print(f'Voici les mots négatifs du batch {i} : ')\n",
    "    print([[idx_to_token[int(id)] for id in ids] for ids in negs])\n",
    "\n",
    "print(dataset.word_probs)\n",
    "print(dataset.subsample_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SkipGramModel(emb_size=dataset.vocab_size, embedding_dimension=3, sparse=False)\n",
    "\n",
    "# opti = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "opti = torch.optim.Adam(model.parameters(), lr=0.01) # 1e-1)\n",
    "logs = train_Word2Vec(modelW2V=model, dataLoader=loader, epochs=100, optimizer=opti, verbal=False)\n",
    "print(logs['final_epoch_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff761393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# interactive_embedding_plot(model.word_emb, dataset.encoder, dataset.decoder, method=\"pca\", title=\"word_emb PCA\")\n",
    "\n",
    "interactive_embedding_plot_2D(model.word_emb, dataset.encoder, dataset.decoder, method=\"pca\", title=\"word_emb PCA\", query_word=\"chat\", n_components=2)\n",
    "\n",
    "interactive_embedding_plot_2D(model.word_emb, dataset.encoder, dataset.decoder, query_word=\"chat\", top_k_neighbors=3, method=\"tsne\", tsne_params={\"perplexity\":2, \"max_iter\":2000})\n",
    "\n",
    "# interactive_embedding_plot(model.word_emb, dataset.encoder, method=\"tsne\", tsne_params={\"perplexity\":40, \"n_iter\":2000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49978f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3690eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive_embedding_plot(model.word_emb, dataset.encoder, dataset.decoder, method=\"pca\", title=\"word_emb PCA\")\n",
    "\n",
    "f = interactive_embedding_plot_3D(model.word_emb, dataset.encoder, dataset.decoder, method=\"pca\", title=\"word_emb PCA\", query_word=\"chat\", n_components=3)\n",
    "f.show()\n",
    "\n",
    "f = interactive_embedding_plot_3D(model.word_emb, dataset.encoder, dataset.decoder, query_word=\"chat\", top_k_neighbors=3, method=None)\n",
    "f.show()\n",
    "print('oui')\n",
    "\n",
    "f = components_to_fig_3D(model.word_emb.weight.detach().numpy(), dataset.encoder, highlight_words=[\"chat\", \"peluche\"], nb_neighbors=2)\n",
    "f.show()\n",
    "\n",
    "# interactive_embedding_plot(model.word_emb, dataset.encoder, method=\"tsne\", tsne_params={\"perplexity\":40, \"n_iter\":2000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(embedding: torch.nn.Embedding, encoder: dict, w1: str, w2: str, device=None):\n",
    "    \"\"\"\n",
    "    Retourne la similarité cosinus entre w1 et w2 (float).\n",
    "    \"\"\"\n",
    "    if w1 not in encoder or w2 not in encoder:\n",
    "        raise KeyError(\"One or both words not in encoder\")\n",
    "    device = device or next(embedding.parameters()).device\n",
    "    idx1 = torch.tensor([encoder[w1]], dtype=torch.long, device=device)\n",
    "    idx2 = torch.tensor([encoder[w2]], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        v1 = embedding(idx1).squeeze(0)  # [D]\n",
    "        v2 = embedding(idx2).squeeze(0)  # [D]\n",
    "        cos_sim = F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n",
    "        euclidian = torch.dist(v1, v2).item()\n",
    "    return {\"cosine\": cos_sim, \"euclidean\": euclidian}\n",
    "\n",
    "\n",
    "# print(word_similarity(embedding=model.word_emb, encoder=dataset.encoder, w1=\"cat\", w2=\"cats\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_plot = dataset.vocab\n",
    "# words_to_plot = [\"cat\", \"dog\", \"cars\", \"students\", \"king\", \"planes\", \"queen\", \"mat\", \"puppy\", \"sits\", \"boys\", \"girls\"]\n",
    "\n",
    "plot_similarity_heatmap(model.word_emb, dataset.encoder, words_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a62a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_cluster = dataset.vocab\n",
    "clusters = cluster_words(model.word_emb, dataset.encoder, words_to_cluster, n_clusters=4)\n",
    "\n",
    "print(dataset.vocab_size)\n",
    "\n",
    "mots_find = 'chien'\n",
    "for id_c, words in clusters.items():\n",
    "    if mots_find in words:\n",
    "        print(f'le mot {mots_find} est dans le cluster : {id_c}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67569036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "def pipe_data(\n",
    "    language: str,\n",
    "    dataseteur: Callable[..., object],\n",
    "    window_size: int = 3,\n",
    "    nb_neg: int = 3,\n",
    "    subsample_thresh: float = 1.0,\n",
    "    vocab_size_limit: Optional[int] = None,\n",
    "    power:float =0.75,\n",
    "    file: Optional[str] = None,\n",
    "    files: Optional[list[str]] = None,\n",
    "    sentences: Optional[List[List[str]]] = None,\n",
    "    remove_accent: bool = True,\n",
    "    remove_ponct: bool = True,\n",
    "    keep_accent: bool = True,\n",
    "    contraction_map: Optional[dict] = None,\n",
    ") -> object:\n",
    "    \"\"\"Prepare des phrases tokenisées et retourne un dataset construit par `dataseteur`.\n",
    "    \n",
    "    Args:\n",
    "        language: Langue utilisée pour la tokenisation (ex. ``\"english\"``, ``\"french\"``). Affecte le comportement du tokenizer.\n",
    "        dataseteur: Callable qui construit et retourne l'objet Dataset. Doit accepter au moins l'argument ``sentences`` (list[list[str]]) et les paramètres suivants : ``window_size``, ``nb_neg``, ``subsample_thresh``, ``vocab_size_limit``, ``power``.\n",
    "        window_size: Taille de la fenêtre contextuelle (nombre de mots à gauche/droite du mot central) utilisée pour générer paires positives.\n",
    "        nb_neg: Nombre d'exemples négatifs par paire positive à générer.\n",
    "        subsample_thresh: Seuil de sous-échantillonnage des mots fréquents ; plus petit => plus d'échantillonnage (valeurs typiques entre 1e-5 et 1e-3 sur big data).\n",
    "        vocab_size_limit: Limite supérieure facultative pour la taille du vocabulaire (``None`` = pas de limite). Si défini, tronque le vocabulaire aux N mots les plus fréquents.\n",
    "        power: Exposant utilisé pour le calcul de la distribution négative (ex. ``0.75`` pour la distribution de Mikolov). Utilisé lors de l'échantillonnage négatif.\n",
    "        file: Chemin vers un fichier texte unique contenant des phrases. Mutuellement exclusif avec ``files`` et ``sentences``.\n",
    "        files: Liste de chemins vers plusieurs fichiers textes à lire séquentiellement. Mutuellement exclusif avec ``file`` et ``sentences``.\n",
    "        sentences: Pré-collection de phrases déjà tokenisées (list[list[str]]). Mutuellement exclusif avec ``file``/``files``. Chaque élément doit être une liste de tokens (str).\n",
    "        remove_accent: Si ``True``, supprime les accents/diacritiques avant tokenisation (normalisation NFKD).\n",
    "        remove_ponct: Si ``True``, retire les signes de ponctuation définis (sauf ceux indiqués par ``keep_accent``) avant tokenisation.\n",
    "        keep_accent: Si ``True``, conserve les apostrophes typographiques/apostrophes droites dans la ponctuation à garder (utile pour contractions).\n",
    "        contraction_map: Dictionnaire optionnel de substitutions pour séparer ou normaliser les contractions (ex. ``{\"n't\": \" n't\"}``). Si ``None``, un map par défaut sera utilisé.\n",
    "        Instance du dataset construite par ``dataseteur``. Le dataset reçoit la liste tokenisée de phrases via l'argument ``sentences`` et les paramètres de génération (``window_size``, ``nb_neg``, ``subsample_thresh``, ``vocab_size_limit``, ``power``).\n",
    "    Return:\n",
    "        Le dataset du corpus donné\n",
    "    \"\"\"\n",
    "    if contraction_map is None:\n",
    "        contraction_map = {\n",
    "            \"n't\": \" n't\", \"'re\": \" 're\", \"'ve\": \" 've\", \"'ll\": \" 'll\",\n",
    "            \"'d\": \" 'd\", \"'s\": \" 's\", \"'m\": \" 'm\"\n",
    "        }\n",
    "\n",
    "    if all([(files is None), (file is None), (sentences is None)],):\n",
    "        raise AssertionError(\"One of files, file or sentence must not be None\")\n",
    "\n",
    "    if language not in {\"english\", \"french\", None}:\n",
    "        raise ValueError(\"language must be 'english' or 'french' or None\")\n",
    "\n",
    "    def remove_accents(text: str) -> str:\n",
    "        nk = unicodedata.normalize(\"NFKD\", text)\n",
    "        return \"\".join(ch for ch in nk if not unicodedata.combining(ch))\n",
    "\n",
    "    keep = {\"'\", \"’\"} if keep_accent else set()\n",
    "    base_punct = set(string.punctuation)\n",
    "    extra_punct = set('“”‘’—–…«»')\n",
    "    punct_to_remove = (base_punct | extra_punct) - keep\n",
    "    TRANSL_TABLE = str.maketrans('', '', ''.join(sorted(punct_to_remove)))\n",
    "\n",
    "    tokens_by_sentence: List[List[str]] = []\n",
    "\n",
    "    if files is not None:\n",
    "        for name_file in files:\n",
    "            with open(name_file, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    s = line.strip().lower()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    if remove_accent:\n",
    "                        s = remove_accents(s)\n",
    "                    for k, v in contraction_map.items():\n",
    "                        s = s.replace(k, v)\n",
    "                    if remove_ponct:\n",
    "                        s = s.translate(TRANSL_TABLE)\n",
    "                    s = \" \".join(s.split())\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    toks = word_tokenize(s, language=language)\n",
    "                    if toks:\n",
    "                        tokens_by_sentence.append(toks)\n",
    "\n",
    "    elif file is not None:\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip().lower()\n",
    "                if not s:\n",
    "                    continue\n",
    "                if remove_accent:\n",
    "                    s = remove_accents(s)\n",
    "                for k, v in contraction_map.items():\n",
    "                    s = s.replace(k, v)\n",
    "                if remove_ponct:\n",
    "                    s = s.translate(TRANSL_TABLE)\n",
    "                s = \" \".join(s.split())\n",
    "                if not s:\n",
    "                    continue\n",
    "                toks = word_tokenize(s, language=language)\n",
    "                if toks:\n",
    "                    tokens_by_sentence.append(toks)\n",
    "    else:\n",
    "        tokens_by_sentence = [list(s) for s in sentences if s]\n",
    "\n",
    "    return dataseteur(\n",
    "        sentences=tokens_by_sentence,\n",
    "        window_size=window_size,\n",
    "        nb_neg=nb_neg,\n",
    "        subsample_thresh=subsample_thresh,\n",
    "        power=power,\n",
    "        vocab_size_limit=vocab_size_limit\n",
    "    )\n",
    "\n",
    "def pipe(\n",
    "    dataseter:SGNS_store_DataSet,\n",
    "    model:SkipGramModel,\n",
    "    opti:torch.optim.Optimizer,\n",
    "    grid_search:dict=None,\n",
    "    epochs:int=100,\n",
    "    words_highlight:Optional[List[str]|str]=None,\n",
    "    top_k_neighbors:int=5,\n",
    "    most_similar:bool=True,\n",
    "    most_similar_words:Optional[List[str]]=None,\n",
    "    embedding_plot:bool=True,\n",
    "    embedding_plot_method:str|List[str]=\"pca\",\n",
    "    three_dimension:bool=True,\n",
    "    tsne_params:dict={\"perplexity\":10, \"max_iter\":100},\n",
    "    nb_clusters:int=5,\n",
    "    words_to_cluster:Optional[List[str]]=None,\n",
    "    heat_map:Optional[List[str]]=None,\n",
    "):\n",
    "    data = DataLoader(dataseter, shuffle=True)\n",
    "    if grid_search is not None:\n",
    "        raise NotImplemented\n",
    "\n",
    "    logs_loss = train_Word2Vec(modelW2V=model, dataLoader=data, epochs=epochs, optimizer=opti, verbal=False)\n",
    "    print(\"train done\")\n",
    "\n",
    "    if most_similar:\n",
    "        if words_highlight is None:\n",
    "            print(\"words_highlight est à None, on ne peut pas trouver de mots similaires à None\")\n",
    "        else:            \n",
    "            if most_similar_words is None:\n",
    "                W = model.word_emb.weight.detach().numpy()\n",
    "            else:\n",
    "                idxs:List[int] = dataseter.encode(most_similar_words)\n",
    "                with torch.no_grad():\n",
    "                    W = model.word_emb.weight.detach().numpy()[idxs]\n",
    "            words_most_similar = {}\n",
    "            if isinstance(words_highlight, list):\n",
    "                for wh in words_highlight:\n",
    "                    idx = dataseter.encode(wh)\n",
    "                    print(idx)\n",
    "                    query_vec = model.word_emb.weight.detach().numpy()[idx:idx+1]\n",
    "                    sims = cosine_similarity(query_vec, W).flatten()\n",
    "                    order = np.argsort(-sims)\n",
    "                    topk = order[:top_k_neighbors]\n",
    "                    words_most_similar[wh] = dataseter.decode(topk)\n",
    "\n",
    "            elif isinstance(words_highlight, str):\n",
    "                idx = dataseter.encode(words_highlight)\n",
    "                query_vec = model.word_emb.weight.detach().numpy()[idx:idx+1] \n",
    "                sims = cosine_similarity(query_vec, W).flatten()\n",
    "                order = np.argsort(-sims)\n",
    "                topk = order[:top_k_neighbors]\n",
    "                words_most_similar[words_highlight] = dataseter.decode(topk)\n",
    "        print(\"most similar done\")\n",
    "\n",
    "    def multi_query_words_plot(m, words):\n",
    "        if isinstance(words, list):\n",
    "            for w in words:\n",
    "                if three_dimension:\n",
    "                    interactive_embedding_plot_3D(embedding=model.word_emb, encoder=dataseter.encoder, decoder=dataseter.decoder,\n",
    "                        words=None, method=m, n_components=3, tsne_params=tsne_params, query_word=w, top_k_neighbors=top_k_neighbors)\n",
    "                else:\n",
    "                    interactive_embedding_plot_2D(embedding=model.word_emb, encoder=dataseter.encoder, decoder=dataseter.decoder,\n",
    "                        words=None, method=m, n_components=2, tsne_params=tsne_params, query_word=w, top_k_neighbors=top_k_neighbors)\n",
    "        else:\n",
    "            if three_dimension:\n",
    "                interactive_embedding_plot_3D(embedding=model.word_emb, encoder=dataseter.encoder, decoder=dataseter.decoder,\n",
    "                    words=None, method=m, n_components=3, tsne_params=tsne_params, query_word=words, top_k_neighbors=top_k_neighbors)\n",
    "            else:\n",
    "                interactive_embedding_plot_2D(embedding=model.word_emb, encoder=dataseter.encoder, decoder=dataseter.decoder,\n",
    "                    words=None, method=m, n_components=2, tsne_params=tsne_params, query_word=words, top_k_neighbors=top_k_neighbors)\n",
    "\n",
    "\n",
    "    if embedding_plot:\n",
    "        if isinstance(embedding_plot_method, list):\n",
    "            for m in embedding_plot_method:\n",
    "                multi_query_words_plot(m, words_highlight)\n",
    "        elif embedding_plot_method == \"pca\" or embedding_plot_method == \"tsne\" or embedding_plot_method is None:\n",
    "            multi_query_words_plot(embedding_plot_method, words_highlight)\n",
    "        else:\n",
    "            raise TypeError(f\"Can't use {embedding_plot_method} to plot embedding\")\n",
    "        \n",
    "\n",
    "    \n",
    "    if nb_clusters is not None:\n",
    "        if words_to_cluster is None:\n",
    "            words_to_cluster = dataseter.vocab\n",
    "        print(words_to_cluster)\n",
    "        clusters = cluster_words(embedding=model.word_emb, encoder=dataseter.encoder, words=words_to_cluster, n_clusters=nb_clusters)\n",
    "\n",
    "    if heat_map is not None:\n",
    "        plot_similarity_heatmap(model.word_emb, dataseter.encoder, words_highlight)\n",
    "\n",
    "    return {\n",
    "        \"loss_history\" : logs_loss,\n",
    "        \"most similar\" : words_most_similar,\n",
    "        \"clusters\" : clusters\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_simple:SGNS_store_DataSet = pipe_data(\n",
    "    language=\"french\",\n",
    "    dataseteur=SGNS_store_DataSet,\n",
    "    window_size = 3,\n",
    "    nb_neg=3,\n",
    "    power=0.75,\n",
    "    subsample_thresh= 1.0,\n",
    "    vocab_size_limit=None,\n",
    "    file=None,\n",
    "    sentences= [\n",
    "    [\"chat\", \"chien\", \"souris\"],\n",
    "    [\"souris\", \"chien\", \"chat\"],\n",
    "    [\"souris\", \"chat\", \"chat\"],\n",
    "    [\"chat\", \"chien\", \"chaton\"],\n",
    "    [\"souris\", \"chien\", \"chaton\"],\n",
    "    [\"souris\", \"chat\", \"chaton\"],\n",
    "    [\"balle\", \"train\", \"peluche\"],\n",
    "    [\"balle\", \"peluche\", \"train\"],\n",
    "    [\"peluche\", \"train\", \"balle\"]\n",
    "    ],\n",
    "    remove_accent=True,\n",
    "    remove_ponct=True,\n",
    "    keep_accent= True,\n",
    "    contraction_map=None\n",
    ")\n",
    "\n",
    "print(dataset.pairs)\n",
    "center, pos, neg = dataset[1]\n",
    "print(f'Mot central : {dataset.decoder[int(center)]}; Mot positif :  {dataset.decoder[int(pos)]} \\nMots négatifs : {dataset.decode(neg.tolist())}')\n",
    "\n",
    "\n",
    "dataset:SGNS_store_DataSet = pipe_data(\n",
    "    language=\"french\",\n",
    "    dataseteur=SGNS_store_DataSet,\n",
    "    window_size = 3,\n",
    "    nb_neg=10,\n",
    "    subsample_thresh= 1.0,\n",
    "    vocab_size_limit=None,\n",
    "    file=\"data/GPT5v2.txt\",\n",
    "    # files=[\"data/chat_chien.txt\", \"data/toy.txt\"],\n",
    "    remove_accent=True,\n",
    "    remove_ponct=True,\n",
    "    keep_accent= False,\n",
    "    contraction_map=None\n",
    ")\n",
    "\n",
    "print(dataset.pairs)\n",
    "center, pos, neg = dataset[1]\n",
    "print(f'Mot central : {dataset.decoder[int(center)]}; Mot positif :  {dataset.decoder[int(pos)]} \\nMots négatifs : {dataset.decode(neg.tolist())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec71328",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_simple.encode('balle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ddf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGramModel(emb_size=dataset.vocab_size, embedding_dimension=3, sparse=False)\n",
    "info = pipe(\n",
    "    dataseter= dataset,\n",
    "    model=model,\n",
    "    opti=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "    grid_search=None, # Non fonctionnel\n",
    "    epochs=30,\n",
    "    words_highlight=['chat', 'chien', 'train', 'balle', 'animal'],\n",
    "    top_k_neighbors=15,\n",
    "    most_similar=True,\n",
    "    most_similar_words=None, # Prendre tout le vocab\n",
    "    embedding_plot=True,\n",
    "    three_dimension= True,\n",
    "    embedding_plot_method=None, # Pas de réduction si embedding_dimension = 3\n",
    "    tsne_params= None,\n",
    "    nb_clusters=2,\n",
    "    words_to_cluster=None, # Prendre tout le vocab\n",
    "    heat_map=['chat', 'chien', 'train', 'balle', 'animal']   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, v in info[\"clusters\"].items():\n",
    "    print(v)\n",
    "print(info[\"loss_history\"])\n",
    "\n",
    "print(info[\"most similar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9441188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbf103",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGramModel(emb_size=dataset.vocab_size, embedding_dimension=3, sparse=False)\n",
    "info = pipe(\n",
    "    dataseter= dataset,\n",
    "    model=model,\n",
    "    opti=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "    grid_search=None, # Non fonctionnel\n",
    "    epochs=30,\n",
    "    words_highlight=['chat', 'chien', 'train', 'balle', 'animal'],\n",
    "    top_k_neighbors=10,\n",
    "    most_similar=True,\n",
    "    most_similar_words=None, # Prendre tout le vocab\n",
    "    embedding_plot=False,\n",
    "    three_dimension= True,\n",
    "    embedding_plot_method=None, # Pas de réduction si embedding_dimension = 3\n",
    "    tsne_params= None,\n",
    "    nb_clusters=2,\n",
    "    words_to_cluster=None, # Prendre tout le vocab\n",
    "    heat_map=['chat', 'chien', 'train', 'balle', 'animal']   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weight(\"SGNS_weights/\")\n",
    "w = model.word_emb.weight.detach().numpy()\n",
    "f = components_to_fig_3D(w, dataset.encoder)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c652cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(dataset.encoder)\n",
    "\n",
    "with open(\"data/encoder\", 'w') as f:\n",
    "    f.write(json.dumps(dataset.encoder))\n",
    "\n",
    "\n",
    "with open(\"data/encoder\") as f:\n",
    "    pet = json.loads(f.read())\n",
    "\n",
    "print(pet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
